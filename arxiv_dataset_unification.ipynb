{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04dcb448-a981-4ab5-81eb-3405aa80a2e9",
   "metadata": {},
   "source": [
    "# Loading the raw dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9671c1d-5dff-46e1-972c-15d02a31e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "raw_arxiv_json_path=\"your-project-path/arxiv-metadata-oai-snapshot.json\"\n",
    "dataset_dir = 'your-project-path/datasets'\n",
    "raw_pyalex_df_path = os.path.join(dataset_dir, 'pyalex_df.csv') #the base openalex dataset to unify arxiv records with\n",
    "df = pd.read_csv(raw_pyalex_df_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a466d09-9bb7-42e9-852a-b74158067db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Keywords to search for\n",
    "keywords = ['antisemitism', 'antisemitic','anti-semitism', 'antizionist', 'antizionism', 'anti-zionism', 'jew', 'jews','jewish','holocaust', 'nazi', 'nazism']\n",
    "# Fields to check for keywords\n",
    "fields_to_check = ['title', 'abstract']\n",
    "# Fields to keep in the output\n",
    "fields_to_keep = ['id', 'title', 'abstract', 'authors', 'authors_parsed', 'doi']\n",
    "\n",
    "\n",
    "def contains_keywords(text, keywords_list):\n",
    "    \"\"\"Check if any keyword appears as full words in the text\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return False\n",
    "    \n",
    "    text = text.lower()\n",
    "    # Create a pattern that matches whole words only\n",
    "    for keyword in keywords_list:\n",
    "        # Create a regex pattern that ensures the keyword is surrounded by word boundaries\n",
    "        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n",
    "        if re.search(pattern, text):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "    \n",
    "def extract_year_from_date(date_str):\n",
    "    \"\"\"Extract year from date string in format yyyy-mm-dd\"\"\"\n",
    "    if not date_str or not isinstance(date_str, str):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Simple split to get the year part (first 4 characters)\n",
    "        return date_str.split('-')[0]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def process_json_file(json_path, output_path, chunk_size=1000):\n",
    "    \"\"\"Process JSON file line by line and filter for keywords\"\"\"\n",
    "    \n",
    "    # Count lines for progress bar\n",
    "    print(\"Counting lines in file...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "    \n",
    "    filtered_records = []\n",
    "    records_processed = 0\n",
    "    matches_found = 0\n",
    "    \n",
    "    print(f\"Processing and filtering JSON file...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        for line in tqdm(f, total=total_lines):\n",
    "            try:\n",
    "                # Parse JSON record\n",
    "                record = json.loads(line.strip())\n",
    "                records_processed += 1\n",
    "                \n",
    "                # Check if any field contains keywords\n",
    "                match_found = False\n",
    "                for field in fields_to_check:\n",
    "                    if field in record and contains_keywords(record.get(field), keywords):\n",
    "                        match_found = True\n",
    "                        break\n",
    "                \n",
    "                # If match found, keep only the specified fields\n",
    "                if match_found:\n",
    "                    matches_found += 1\n",
    "                    filtered_record = {field: record.get(field, '') for field in fields_to_keep}\n",
    "                    \n",
    "                    # Extract year from update_date and add as publication_year\n",
    "                    update_date = record.get('update_date', '')\n",
    "                    publication_year = extract_year_from_date(update_date)\n",
    "                    if publication_year:\n",
    "                        filtered_record['publication_year'] = publication_year\n",
    "                    \n",
    "                    filtered_records.append(filtered_record)\n",
    "                \n",
    "                # Write chunks to disk to save memory\n",
    "                if len(filtered_records) >= chunk_size:\n",
    "                    write_chunk_to_file(filtered_records, output_path, \n",
    "                                        append=(matches_found > chunk_size))\n",
    "                    filtered_records = []\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Error parsing JSON at line {records_processed+1}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing record: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Write any remaining records\n",
    "    if filtered_records:\n",
    "        write_chunk_to_file(filtered_records, output_path, \n",
    "                           append=(matches_found > chunk_size))\n",
    "    \n",
    "    print(f\"Processing complete. {records_processed} records processed, {matches_found} matches found.\")\n",
    "    print(f\"Filtered data saved to {output_path}\")\n",
    "    return matches_found\n",
    "\n",
    "def write_chunk_to_file(records, output_path, append=False):\n",
    "    \"\"\"Write a chunk of records to the output file\"\"\"\n",
    "    mode = 'a' if append else 'w'\n",
    "    with open(output_path, mode) as f:\n",
    "        for record in records:\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "# Execute the processing\n",
    "output_file = 'filtered_arxiv_data.jsonl'\n",
    "process_json_file(raw_arxiv_json_path, output_file)\n",
    "\n",
    "# Sample code to read the filtered data (for verification)\n",
    "def peek_at_results(output_path, num_samples=3):\n",
    "    \"\"\"Show a few sample records from the output file\"\"\"\n",
    "    print(f\"\\nSample records from {output_path}:\")\n",
    "    with open(output_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            record = json.loads(line)\n",
    "            print(f\"Record {i+1}:\")\n",
    "            print(f\"  Title: {record.get('title', '')[:100]}...\")\n",
    "            print(f\"  Authors: {record.get('authors', '')[:100]}...\")\n",
    "            print(f\"  DOI: {record.get('doi', 'N/A')}\")\n",
    "            print(f\"  Publication Year: {record.get('publication_year', 'N/A')}\")\n",
    "            print()\n",
    "\n",
    "# Peek at the results\n",
    "peek_at_results(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19671627-aac3-4221-88ae-f71cee93c6d6",
   "metadata": {},
   "source": [
    "# Unify datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1490f8-b61a-45b1-96a7-aa50e1c78d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def unify_datasets(json_path, df, output_path='unified_data.csv'):\n",
    "    \"\"\"\n",
    "    Unify the arXiv JSON dataset with the PyAlex dataframe\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to the filtered arXiv JSON file\n",
    "        df: PyAlex dataframe\n",
    "        output_path: Path to save the unified dataframe\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    unified_df = df.copy()\n",
    "    \n",
    "    # Convert DOI column to lowercase for case-insensitive comparison\n",
    "    # and create a title lookup dictionary for faster searches\n",
    "    doi_lookup = {}\n",
    "    title_lookup = {}\n",
    "    \n",
    "    print(\"Creating lookup dictionaries...\")\n",
    "    for index, row in unified_df.iterrows():\n",
    "        if pd.notna(row['DOI']) and row['DOI'] != '':\n",
    "            doi_lookup[row['DOI'].lower()] = index\n",
    "        \n",
    "        if pd.notna(row['Title']):\n",
    "            # Use lowercase title for matching\n",
    "            title_lookup[row['Title'].lower()] = index\n",
    "    \n",
    "    # Track changes\n",
    "    updated_records = 0\n",
    "    new_records = 0\n",
    "    mismatched_titles = 0\n",
    "    \n",
    "    # Process the JSON file line by line\n",
    "    print(f\"Processing JSON file and unifying with dataframe...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        for line_num, line in enumerate(tqdm(f), 1):\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                \n",
    "                # Extract needed fields\n",
    "                arxiv_doi = record.get('doi', '').lower() if record.get('doi') else ''\n",
    "                arxiv_title = record.get('title', '')\n",
    "                arxiv_abstract = record.get('abstract', '')\n",
    "                arxiv_year = record.get('publication_year', '')\n",
    "                arxiv_authors_raw = record.get('authors', '')\n",
    "                \n",
    "                # Process authors - convert to list of strings\n",
    "                if isinstance(arxiv_authors_raw, list):\n",
    "                    # If already a list\n",
    "                    arxiv_authors = arxiv_authors_raw\n",
    "                elif isinstance(arxiv_authors_raw, str):\n",
    "                    # Split by commas if it's a string\n",
    "                    arxiv_authors = [a.strip() for a in arxiv_authors_raw.split(',')]\n",
    "                else:\n",
    "                    # Default empty list\n",
    "                    arxiv_authors = []\n",
    "                \n",
    "                # Join with comma+space for standard format\n",
    "                arxiv_authors_str = \", \".join(arxiv_authors)\n",
    "                \n",
    "                # Case 1: Check if DOI exists and is in the dataframe\n",
    "                if arxiv_doi and arxiv_doi in doi_lookup:\n",
    "                    index = doi_lookup[arxiv_doi]\n",
    "                    existing_title = unified_df.at[index, 'Title']\n",
    "                    \n",
    "                    # Compare titles\n",
    "                    if existing_title.lower() != arxiv_title.lower():\n",
    "                        print(f\"Line {line_num} - Title mismatch for DOI {arxiv_doi}:\")\n",
    "                        print(f\"  PyAlex: {existing_title}\")\n",
    "                        print(f\"  arXiv: {arxiv_title}\")\n",
    "                        mismatched_titles += 1\n",
    "                    else:\n",
    "                        # Titles match, update record if needed\n",
    "                        # Update abstract if empty\n",
    "                        if (pd.isna(unified_df.at[index, 'Abstract']) or \n",
    "                            unified_df.at[index, 'Abstract'] == '' or \n",
    "                            unified_df.at[index, 'Abstract'] == 'No abstract available'):\n",
    "                            if arxiv_abstract:\n",
    "                                unified_df.at[index, 'Abstract'] = arxiv_abstract\n",
    "                        \n",
    "                        # Update authors if new ones found\n",
    "                        if arxiv_authors_str:\n",
    "                            existing_authors = unified_df.at[index, 'Authors']\n",
    "                            # Add new authors not in the existing list\n",
    "                            if existing_authors:\n",
    "                                existing_author_list = [a.strip() for a in existing_authors.split(',')]\n",
    "                                new_authors = [a for a in arxiv_authors if a not in existing_author_list]\n",
    "                                if new_authors:\n",
    "                                    unified_df.at[index, 'Authors'] = existing_authors + \", \" + \", \".join(new_authors)\n",
    "                            else:\n",
    "                                unified_df.at[index, 'Authors'] = arxiv_authors_str\n",
    "                        \n",
    "                        updated_records += 1\n",
    "                \n",
    "                # Case 2: Check if title exists in the dataframe\n",
    "                elif arxiv_title and arxiv_title.lower() in title_lookup:\n",
    "                    index = title_lookup[arxiv_title.lower()]\n",
    "                    \n",
    "                    # Update abstract if empty\n",
    "                    if (pd.isna(unified_df.at[index, 'Abstract']) or \n",
    "                        unified_df.at[index, 'Abstract'] == '' or \n",
    "                        unified_df.at[index, 'Abstract'] == 'No abstract available'):\n",
    "                        if arxiv_abstract:\n",
    "                            unified_df.at[index, 'Abstract'] = arxiv_abstract\n",
    "                    \n",
    "                    # Update authors if new ones found\n",
    "                    if arxiv_authors_str:\n",
    "                        existing_authors = unified_df.at[index, 'Authors']\n",
    "                        # Add new authors not in the existing list\n",
    "                        if existing_authors:\n",
    "                            existing_author_list = [a.strip() for a in existing_authors.split(',')]\n",
    "                            new_authors = [a for a in arxiv_authors if a not in existing_author_list]\n",
    "                            if new_authors:\n",
    "                                unified_df.at[index, 'Authors'] = existing_authors + \", \" + \", \".join(new_authors)\n",
    "                        else:\n",
    "                            unified_df.at[index, 'Authors'] = arxiv_authors_str\n",
    "                    \n",
    "                    updated_records += 1\n",
    "                \n",
    "                # Case 3: New record - doesn't exist in dataframe\n",
    "                else:\n",
    "                    # Create a new record with arxiv data\n",
    "                    new_record = {\n",
    "                        'Title': arxiv_title,\n",
    "                        'DOI': arxiv_doi,\n",
    "                        'OpenAlex ID': '',\n",
    "                        'Publication Year': int(arxiv_year) if arxiv_year and arxiv_year.isdigit() else None,\n",
    "                        'Type': 'preprint',  # Default type for arXiv papers\n",
    "                        'Citation Count': 0,  # Default citation count\n",
    "                        'Abstract': arxiv_abstract,\n",
    "                        'Journal': '',\n",
    "                        'Publisher': 'arXiv',\n",
    "                        'Authors': arxiv_authors_str,\n",
    "                        'Institutions': [],\n",
    "                        'Countries': [],\n",
    "                        'Concepts': [],\n",
    "                        'Sub-fields': [],\n",
    "                        'Topics': [],\n",
    "                        'Domains': [],\n",
    "                        'Fields': [],\n",
    "                        'concept_dict': {},\n",
    "                    }\n",
    "                    \n",
    "                    # Calculate decade from publication year\n",
    "                    if arxiv_year and arxiv_year.isdigit():\n",
    "                        new_record['decade'] = (int(arxiv_year) // 10) * 10\n",
    "                    else:\n",
    "                        new_record['decade'] = None\n",
    "                    \n",
    "                    # Add new record to dataframe\n",
    "                    unified_df = pd.concat([unified_df, pd.DataFrame([new_record])], ignore_index=True)\n",
    "                    \n",
    "                    # Update lookup dictionaries\n",
    "                    if arxiv_doi:\n",
    "                        doi_lookup[arxiv_doi] = len(unified_df) - 1\n",
    "                    title_lookup[arxiv_title.lower()] = len(unified_df) - 1\n",
    "                    \n",
    "                    new_records += 1\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Error parsing JSON at line {line_num}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line {line_num}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Save the unified dataframe\n",
    "    print(f\"Saving unified dataset to {output_path}...\")\n",
    "    unified_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Unification complete:\")\n",
    "    print(f\"  - Updated records: {updated_records}\")\n",
    "    print(f\"  - New records added: {new_records}\")\n",
    "    print(f\"  - Title mismatches found: {mismatched_titles}\")\n",
    "    print(f\"  - Final dataframe size: {len(unified_df)}\")\n",
    "    \n",
    "    return unified_df\n",
    "\n",
    "# Example usage:\n",
    "unified_df = unify_datasets('filtered_arxiv_data.jsonl', df, 'unified_dataset_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73e056-c51b-4753-bbc6-588a505ddde2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b90d68f-df65-4c0b-9c5a-96b61aa46a00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miki_project",
   "language": "python",
   "name": "miki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
