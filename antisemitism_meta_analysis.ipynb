{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1866314b-792e-4568-95fe-d42e103f2e5f",
   "metadata": {},
   "source": [
    "# SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaca33c-268a-4c3b-a8ef-9a2aaaaaed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from ast import literal_eval\n",
    "from typing import List, Union, Optional, Dict, Tuple\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import random\n",
    "import warnings\n",
    "import networkx as nx\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf56f432-05e0-4889-be84-f47fb26e9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'your-project-path/datasets'\n",
    "subject_main_dir = 'your-project-path/analyze_dataset'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "os.makedirs(subject_main_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8546d25-7f96-4bbf-bb9f-9b409796ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pyalex_df_path = os.path.join(dataset_dir, 'pyalex_df.csv')\n",
    "clean_pyalex_df_path = os.path.join(dataset_dir, 'cleaned_pyalex_df.csv')\n",
    "unified_df_path=os.path.join(dataset_dir, 'unified_dataset.csv')\n",
    "unified_cleaned_df_path=os.path.join(dataset_dir, 'cleaned_unified_dataset.csv')\n",
    "filtered_titles_df_path=os.path.join(dataset_dir, 'filtered_titles_dataset.csv')\n",
    "filtered_abstracts_df_path=os.path.join(dataset_dir, 'filtered_abstracts_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa2942-4d0b-410b-828f-dc3c2578dcf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(unified_cleaned_df_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c008f4bc-603d-48c7-a366-f8ba666e9d98",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31557bd2-fc69-4f84-aeb0-c0e2df8d2486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detect the language of a given text\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The text to analyze\n",
    "    \n",
    "    Returns:\n",
    "    str: The detected language code\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return langdetect.detect(text)\n",
    "    except (langdetect.lang_detect_exception.LangDetectException, ValueError, TypeError):\n",
    "        # Return None if language detection fails\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892815d-2f7a-4384-92f5-5c799274913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "def save_plot(plt, filename, subject_main_dir, subject_type):\n",
    "    \"\"\"\n",
    "    Save the current matplotlib plot to a file in the appropriate subject subdirectory\n",
    "    \n",
    "    Parameters:\n",
    "    plt: matplotlib.pyplot instance\n",
    "    filename (str): Name of the file to save\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis (e.g., 'domains', 'fields', 'topics')\n",
    "    \"\"\"\n",
    "    # Create subject-specific subdirectory\n",
    "    subject_dir = Path(subject_main_dir) / subject_type\n",
    "    subject_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save the plot\n",
    "    save_path = subject_dir / filename\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e57e99f-b07d-4619-a97c-8a90e35fe545",
   "metadata": {},
   "source": [
    "# Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c38aa-59ec-4c28-b9b7-13dfe95201cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_columns(df):\n",
    "    \"\"\"\n",
    "    Drops columns that have 100% missing values.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with empty columns removed\n",
    "    tuple: (processed DataFrame, list of dropped columns)\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    # Calculate percentage of missing values for each column\n",
    "    missing_percentage = df_cleaned.isna().mean() * 100\n",
    "    \n",
    "    # Find columns with 100% missing values\n",
    "    completely_empty_cols = missing_percentage[missing_percentage == 100].index.tolist()\n",
    "    \n",
    "    # Print information about dropped columns\n",
    "    if completely_empty_cols:\n",
    "        print(f\"Dropping {len(completely_empty_cols)} completely empty columns: {', '.join(completely_empty_cols)}\")\n",
    "        \n",
    "        # Drop the completely empty columns\n",
    "        df_cleaned = df_cleaned.drop(columns=completely_empty_cols)\n",
    "    else:\n",
    "        print(\"No completely empty columns found.\")\n",
    "    \n",
    "    return df_cleaned, completely_empty_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9266fd-76a4-4769-a579-f3a54cb85f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import langdetect\n",
    "from langdetect import DetectorFactory\n",
    "import re\n",
    "\n",
    "# For reproducible language detection results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def clean_author_name(author_name):\n",
    "    \"\"\"\n",
    "    Remove text within parentheses from author names\n",
    "    \n",
    "    Parameters:\n",
    "    author_name (str): Author name that may contain text in parentheses\n",
    "    \n",
    "    Returns:\n",
    "    str: Cleaned author name\n",
    "    \"\"\"\n",
    "    if not isinstance(author_name, str):\n",
    "        return author_name\n",
    "    \n",
    "    # Remove text within parentheses and any extra whitespace\n",
    "    cleaned_name = re.sub(r'\\s*\\([^)]*\\)\\s*', ' ', author_name)\n",
    "    # Remove any extra spaces that might have been created\n",
    "    cleaned_name = re.sub(r'\\s+', ' ', cleaned_name).strip()\n",
    "    \n",
    "    return cleaned_name\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame to convert string representations back to their original types\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Processed DataFrame with correct data types\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Identify all columns that might contain lists\n",
    "    potential_list_columns = []\n",
    "    for col in df.columns:\n",
    "        # Sample some non-null values to check if they look like lists\n",
    "        sample = df[col].dropna().head(10)\n",
    "        for val in sample:\n",
    "            if isinstance(val, str) and val.strip().startswith('[') and val.strip().endswith(']'):\n",
    "                potential_list_columns.append(col)\n",
    "                break\n",
    "    \n",
    "    print(f\"Detected potential list columns: {potential_list_columns}\")\n",
    "    \n",
    "    # Convert string representations of lists back to actual lists for all detected columns\n",
    "    for col in potential_list_columns:\n",
    "        print(f\"Converting column: {col}\")\n",
    "        df[col] = df[col].apply(\n",
    "            lambda x: ast.literal_eval(x) if isinstance(x, str) and x.strip().startswith('[') and x.strip().endswith(']') else x\n",
    "        )\n",
    "    \n",
    "    # Convert concept_dict from string to dictionary (special handling)\n",
    "    if 'concept_dict' in df.columns:\n",
    "        print(\"Converting concept_dict column\")\n",
    "        df['concept_dict'] = df['concept_dict'].apply(\n",
    "            lambda x: ast.literal_eval(x) if isinstance(x, str) and x.strip().startswith('{') and x.strip().endswith('}') else x\n",
    "        )\n",
    "    \n",
    "    # Clean author names by removing text in parentheses\n",
    "    if 'Authors' in df.columns:\n",
    "        print(\"Cleaning author names\")\n",
    "        # Handle both string and list representations\n",
    "        df['Authors'] = df['Authors'].apply(\n",
    "            lambda x: [clean_author_name(author) for author in x] if isinstance(x, list) \n",
    "                     else clean_author_name(x) if isinstance(x, str)\n",
    "                     else x\n",
    "        )\n",
    "    \n",
    "    # Ensure numeric columns are correct type\n",
    "    numeric_columns = ['Publication Year', 'Citation Count']\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            print(f\"Converting numeric column: {col}\")\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "def clean_titles(df, remove_missing=True, english_only=True):\n",
    "    \"\"\"\n",
    "    Clean the dataset by removing rows with missing titles \n",
    "    and optionally filtering to keep only English titles\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame\n",
    "    remove_missing (bool): Whether to remove rows with missing titles\n",
    "    english_only (bool): Whether to keep only English titles\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Cleaned DataFrame\n",
    "    tuple: (cleaned DataFrame, stats dictionary with cleaning information)\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Initialize stats dictionary\n",
    "    stats = {\n",
    "        'original_row_count': len(df),\n",
    "        'missing_titles': 0,\n",
    "        'non_english_titles': 0,\n",
    "        'remaining_row_count': 0\n",
    "    }\n",
    "    \n",
    "    # Remove rows with missing titles\n",
    "    if remove_missing and 'Title' in df.columns:\n",
    "        missing_mask = df_clean['Title'].isna() | (df_clean['Title'] == '')\n",
    "        stats['missing_titles'] = missing_mask.sum()\n",
    "        df_clean = df_clean[~missing_mask]\n",
    "    \n",
    "    # Filter to keep only English titles\n",
    "    if english_only and 'Title' in df.columns:\n",
    "        # Apply language detection to titles\n",
    "        print(\"Detecting languages for titles... (this might take a while)\")\n",
    "        df_clean['detected_language'] = df_clean['Title'].apply(detect_language)\n",
    "        \n",
    "        # Filter out non-English titles\n",
    "        non_english_mask = (df_clean['detected_language'] != 'en') & (df_clean['detected_language'].notna())\n",
    "        stats['non_english_titles'] = non_english_mask.sum()\n",
    "        df_clean = df_clean[~non_english_mask]\n",
    "        \n",
    "        # Drop the temporary language column\n",
    "        df_clean = df_clean.drop('detected_language', axis=1)\n",
    "    \n",
    "    stats['remaining_row_count'] = len(df_clean)\n",
    "    \n",
    "    return df_clean, stats\n",
    "\n",
    "def print_cleaning_stats(stats):\n",
    "    \"\"\"\n",
    "    Print statistics about the cleaning process\n",
    "    \n",
    "    Parameters:\n",
    "    stats (dict): Dictionary containing cleaning statistics\n",
    "    \"\"\"\n",
    "    print(f\"Original number of rows: {stats['original_row_count']}\")\n",
    "    print(f\"Rows with missing titles: {stats['missing_titles']}\")\n",
    "    print(f\"Rows with non-English titles: {stats['non_english_titles']}\")\n",
    "    print(f\"Remaining rows after cleaning: {stats['remaining_row_count']}\")\n",
    "    print(f\"Percentage of data retained: {stats['remaining_row_count'] / stats['original_row_count'] * 100:.2f}%\")\n",
    "    \n",
    "def analyze_dataset(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform analysis on the dataset to understand its structure and quality\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "    verbose (bool): Whether to print detailed information\n",
    "    \n",
    "    Returns:\n",
    "    dict: Analysis results\n",
    "    \"\"\"\n",
    "    analysis = {}\n",
    "    \n",
    "    # Basic information\n",
    "    analysis['row_count'] = len(df)\n",
    "    analysis['column_count'] = len(df.columns)\n",
    "    \n",
    "    # Missing values\n",
    "    missing_values = df.isna().sum()\n",
    "    missing_percentage = (missing_values / len(df) * 100).round(2)\n",
    "    analysis['missing_values'] = dict(zip(df.columns, missing_values))\n",
    "    analysis['missing_percentage'] = dict(zip(df.columns, missing_percentage))\n",
    "    \n",
    "    # Data types\n",
    "    analysis['dtypes'] = dict(zip(df.columns, df.dtypes.astype(str)))\n",
    "    \n",
    "    # Identify columns with list values\n",
    "    list_columns = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dropna().head(1).map(lambda x: isinstance(x, list)).any():\n",
    "            list_columns.append(col)\n",
    "    analysis['list_columns'] = list_columns\n",
    "    \n",
    "    # Publication years distribution\n",
    "    if 'Publication Year' in df.columns:\n",
    "        year_counts = df['Publication Year'].value_counts().sort_index()\n",
    "        analysis['publication_years'] = dict(zip(year_counts.index, year_counts.values))\n",
    "        analysis['min_year'] = df['Publication Year'].min()\n",
    "        analysis['max_year'] = df['Publication Year'].max()\n",
    "        \n",
    "        print(\"Calculating Decade column\")\n",
    "        # Floor division by 10 and then multiply by 10 to get the decade\n",
    "        df['decade'] = (df['Publication Year'] // 10) * 10\n",
    "        # Handle any NaN values\n",
    "        df['decade'] = df['decade'].astype('Int64')  # pandas nullable integer type to handle NAs\n",
    "    \n",
    "    return df\n",
    "    # Document types distribution\n",
    "    if 'Type' in df.columns:\n",
    "        type_counts = df['Type'].value_counts()\n",
    "        analysis['document_types'] = dict(zip(type_counts.index, type_counts.values))\n",
    "    \n",
    "    # Citation count statistics\n",
    "    if 'Citation Count' in df.columns:\n",
    "        analysis['citation_stats'] = {\n",
    "            'min': df['Citation Count'].min(),\n",
    "            'max': df['Citation Count'].max(),\n",
    "            'mean': df['Citation Count'].mean(),\n",
    "            'median': df['Citation Count'].median(),\n",
    "            'std': df['Citation Count'].std()\n",
    "        }\n",
    "        \n",
    "    # Language detection summary (if available)\n",
    "    if 'detected_language' in df.columns:\n",
    "        lang_counts = df['detected_language'].value_counts()\n",
    "        analysis['language_counts'] = dict(zip(lang_counts.index, lang_counts.values))\n",
    "    \n",
    "    # Print detailed analysis if requested\n",
    "    if verbose:\n",
    "        print(\"\\n=== Dataset Analysis ===\")\n",
    "        print(f\"Number of rows: {analysis['row_count']}\")\n",
    "        print(f\"Number of columns: {analysis['column_count']}\")\n",
    "        \n",
    "        print(\"\\n--- Missing Values ---\")\n",
    "        for col, count in sorted(analysis['missing_values'].items(), key=lambda x: x[1], reverse=True):\n",
    "            if count > 0:\n",
    "                print(f\"{col}: {count} missing values ({analysis['missing_percentage'][col]}%)\")\n",
    "        \n",
    "        print(\"\\n--- Data Types ---\")\n",
    "        for col, dtype in analysis['dtypes'].items():\n",
    "            print(f\"{col}: {dtype}\")\n",
    "        \n",
    "        print(\"\\n--- List Columns ---\")\n",
    "        for col in analysis['list_columns']:\n",
    "            # Get sample lengths of lists in this column\n",
    "            sample_lengths = df[col].dropna().head(5).map(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "            print(f\"{col}: (sample lengths: {list(sample_lengths)})\")\n",
    "        \n",
    "        if 'publication_years' in analysis:\n",
    "            print(\"\\n--- Publication Years ---\")\n",
    "            print(f\"Range: {analysis['min_year']} to {analysis['max_year']}\")\n",
    "            \n",
    "        if 'document_types' in analysis:\n",
    "            print(\"\\n--- Document Types ---\")\n",
    "            for doc_type, count in sorted(analysis['document_types'].items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"{doc_type}: {count} ({count/analysis['row_count']*100:.2f}%)\")\n",
    "        \n",
    "        if 'citation_stats' in analysis:\n",
    "            print(\"\\n--- Citation Statistics ---\")\n",
    "            for stat, value in analysis['citation_stats'].items():\n",
    "                print(f\"{stat.capitalize()}: {value:.2f}\")\n",
    "                \n",
    "        if 'language_counts' in analysis:\n",
    "            print(\"\\n--- Detected Languages ---\")\n",
    "            total = sum(analysis['language_counts'].values())\n",
    "            for lang, count in sorted(analysis['language_counts'].items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "                print(f\"{lang}: {count} ({count/total*100:.2f}%)\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Analyze the raw dataset\n",
    "    print(\"\\n==== Raw Dataset Analysis ====\")\n",
    "    raw_analysis = analyze_dataset(df)\n",
    "    \n",
    "    # Preprocess the dataframe to ensure correct data types\n",
    "    print(\"\\nPreprocessing data types...\")\n",
    "    df = preprocess_dataframe(df)\n",
    "    \n",
    "    # Analyze after type conversion\n",
    "    print(\"\\n==== Dataset Analysis After Type Conversion ====\")\n",
    "    preprocessed_analysis = analyze_dataset(df)\n",
    "    \n",
    "    # Clean titles (remove missing and non-English)\n",
    "    print(\"\\nCleaning titles...\")\n",
    "    df_clean, stats = clean_titles(df, remove_missing=True, english_only=False)\n",
    "\n",
    "\n",
    "    \n",
    "    # Print cleaning statistics\n",
    "    print(\"\\nCleaning statistics:\")\n",
    "    print_cleaning_stats(stats)\n",
    "\n",
    "    # After loading the dataframe and before preprocessing\n",
    "    print(\"\\nChecking for empty columns...\")\n",
    "    df_clean, dropped_cols = drop_empty_columns(df_clean)\n",
    "    print(f\"DataFrame shape after dropping empty columns: {df_clean.shape}\")\n",
    "    \n",
    "    # Analyze the cleaned dataset\n",
    "    print(\"\\n==== Cleaned Dataset Analysis ====\")\n",
    "    cleaned_analysis = analyze_dataset(df_clean)\n",
    "\n",
    "\n",
    "    \n",
    "    # Show some sample data after cleaning\n",
    "    print(\"\\nSample of cleaned data:\")\n",
    "    print(df_clean.head())\n",
    "    \n",
    "    # Save the cleaned dataset\n",
    "    df=df_clean\n",
    "    df_clean.to_csv(unified_cleaned_df_path, index=False)\n",
    "\n",
    "    print(f\"\\nCleaned dataset saved to: {unified_cleaned_df_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725fb595-8d81-4864-8e05-ba0898573aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c59f6365-1cc2-4d56-b8c3-3f35291c05e0",
   "metadata": {},
   "source": [
    "# General analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d563c1-e6b3-4911-aa9f-4b7b026b4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def check_decade_column(df):\n",
    "    \"\"\"\n",
    "    Check if the DataFrame has a decade column, raise error if not\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame to check\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if 'decade' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'decade' column\")\n",
    "\n",
    "def count_papers_by_decade(df):\n",
    "    \"\"\"\n",
    "    Count the number of papers published in each decade\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame with 'decade' column\n",
    "    \n",
    "    Returns:\n",
    "    pandas.Series: Series with decade as index and paper count as value\n",
    "    \"\"\"\n",
    "    check_decade_column(df)\n",
    "    \n",
    "    # Count papers by decade and sort by decade\n",
    "    decade_counts = df['decade'].value_counts().sort_index()\n",
    "    \n",
    "    return decade_counts\n",
    "\n",
    "def plot_papers_per_decade_bar(df, subject_main_dir):\n",
    "    \"\"\"\n",
    "    Create a bar chart of papers published per decade with vertical count labels\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame with publication data\n",
    "    subject_main_dir (str/Path): Main directory for saving plots\n",
    "    \"\"\"\n",
    "    # Count papers by decade\n",
    "    decade_counts = count_papers_by_decade(df)\n",
    "    \n",
    "    # Set up the figure with a clean, professional style\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Sort the decade_counts for color mapping\n",
    "    sorted_indices = np.argsort(decade_counts.values)\n",
    "    color_indices = np.zeros(len(decade_counts), dtype=int)\n",
    "    \n",
    "    # Assign color indices based on rank (higher count = darker color)\n",
    "    for i, idx in enumerate(sorted_indices):\n",
    "        color_indices[idx] = i\n",
    "    \n",
    "    # Create a light blue palette\n",
    "    blue_palette = sns.color_palette(\"Blues\", len(decade_counts))\n",
    "    \n",
    "    # Map colors to bars based on their count value\n",
    "    colors = [blue_palette[idx] for idx in color_indices]\n",
    "    \n",
    "    # Create the bar plot\n",
    "    ax = sns.barplot(x=decade_counts.index, y=decade_counts.values, \n",
    "                     palette=colors)\n",
    "    \n",
    "    # Add count labels on the bars\n",
    "    for i, count in enumerate(decade_counts.values):\n",
    "        ax.text(i, count + (max(decade_counts.values) * 0.01), f'n={count:,}', \n",
    "                ha='center', va='bottom', color='black', fontsize=9)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel('Number of Papers', fontsize=12)\n",
    "    plt.title('Number of Papers Published by Decade', fontsize=14)\n",
    "    \n",
    "    # Format x-axis ticks to show years with no rotation\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, 'papers_per_decade_bar.png', subject_main_dir, 'general_analysis')\n",
    "    \n",
    "    return decade_counts\n",
    "    \n",
    "def plot_papers_per_decade_line(df, subject_main_dir):\n",
    "    \"\"\"\n",
    "    Create a line graph of papers published per decade with trend line\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame with publication data\n",
    "    subject_main_dir (str/Path): Main directory for saving plots\n",
    "    \"\"\"\n",
    "    # Count papers by decade\n",
    "    decade_counts = count_papers_by_decade(df)\n",
    "    \n",
    "    # Set up the figure with a clean, professional style\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Create line plot\n",
    "    decades = decade_counts.index.astype(int)\n",
    "    counts = decade_counts.values\n",
    "    \n",
    "    plt.plot(decades, counts, 'o-', linewidth=2, markersize=10, color='#1f77b4')\n",
    "    \n",
    "    # Add trend line (linear regression)\n",
    "    z = np.polyfit(decades, counts, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(decades, p(decades), \"r--\", linewidth=2, \n",
    "             label=f'Trend line (y = {z[0]:.2f}x + {z[1]:.2f})')\n",
    "    \n",
    "    # Calculate R-squared value\n",
    "    correlation_matrix = np.corrcoef(decades, counts)\n",
    "    correlation_xy = correlation_matrix[0,1]\n",
    "    r_squared = correlation_xy**2\n",
    "    \n",
    "    # Add R-squared annotation\n",
    "    plt.annotate(f'R² = {r_squared:.4f}', \n",
    "                xy=(0.05, 0.95), \n",
    "                xycoords='axes fraction', \n",
    "                fontsize=10, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel('Number of Papers', fontsize=12)\n",
    "    plt.title('Growth of Publication Volume by Decade', fontsize=14)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Format x-axis to show decades properly\n",
    "    plt.xticks(decades)\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, 'papers_per_decade_line.png', subject_main_dir, 'general_analysis')\n",
    "    \n",
    "    return decade_counts\n",
    "\n",
    "def analyze_decades(df, subject_main_dir):\n",
    "    \"\"\"\n",
    "    Perform decade analysis with multiple visualizations\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame with publication data\n",
    "    subject_main_dir (str/Path): Main directory for saving plots\n",
    "    \n",
    "    Returns:\n",
    "    pandas.Series: Series with decade counts\n",
    "    \"\"\"\n",
    "    # Check that decade column exists\n",
    "    check_decade_column(df)\n",
    "    \n",
    "    # Create both visualizations\n",
    "    decade_counts_bar = plot_papers_per_decade_bar(df, subject_main_dir)\n",
    "    decade_counts_line = plot_papers_per_decade_line(df, subject_main_dir)\n",
    "    \n",
    "    # Print decade statistics\n",
    "    print(\"\\nDecade Publication Statistics:\")\n",
    "    for decade, count in decade_counts_bar.items():\n",
    "        print(f\"{decade}s: {count:,} papers\")\n",
    "    \n",
    "    # Calculate growth rates between decades\n",
    "    decades = decade_counts_bar.index.sort_values()\n",
    "    print(\"\\nDecade-over-Decade Growth Rates:\")\n",
    "    for i in range(1, len(decades)):\n",
    "        current_count = decade_counts_bar[decades[i]]\n",
    "        previous_count = decade_counts_bar[decades[i-1]]\n",
    "        growth_rate = ((current_count - previous_count) / previous_count) * 100\n",
    "        print(f\"{decades[i-1]}s to {decades[i]}s: {growth_rate:.2f}%\")\n",
    "    \n",
    "    return decade_counts_bar\n",
    "\n",
    "# Function call\n",
    "decade_counts = analyze_decades(df, subject_main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a9e3f6-ad01-45bb-b7ec-cd7729296833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import langdetect\n",
    "from collections import Counter\n",
    "\n",
    "# Utility function for horizontal bar charts\n",
    "def plot_horizontal_bar_chart(data, title, xlabel, ylabel, filename, subject_main_dir, subject_type, \n",
    "                             figsize=(10, 8), color='skyblue', edgecolor='navy', min_count=None, \n",
    "                             top_n=None, sort_by_value=True, show_count=True):\n",
    "    \"\"\"\n",
    "    Create and save a horizontal bar chart for distribution analysis\n",
    "    \n",
    "    Parameters:\n",
    "    data (dict or Counter): Dictionary with categories as keys and counts as values\n",
    "    title (str): Chart title\n",
    "    xlabel (str): X-axis label (typically count or frequency)\n",
    "    ylabel (str): Y-axis label (typically category name)\n",
    "    filename (str): Name of the file to save\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    figsize (tuple): Figure size in inches (width, height)\n",
    "    color (str): Bar color\n",
    "    edgecolor (str): Bar edge color\n",
    "    min_count (int, optional): Minimum count to include in visualization\n",
    "    top_n (int, optional): Only show top N categories by count\n",
    "    sort_by_value (bool): Whether to sort by count (True) or alphabetically (False)\n",
    "    show_count (bool): Whether to display count values on bars\n",
    "    \"\"\"\n",
    "    # Filter data if needed\n",
    "    if min_count is not None:\n",
    "        data = {k: v for k, v in data.items() if v >= min_count}\n",
    "    \n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(list(data.items()), columns=['Category', 'Count'])\n",
    "    \n",
    "    # Sort data\n",
    "    if sort_by_value:\n",
    "        df = df.sort_values('Count', ascending=True)\n",
    "    else:\n",
    "        df = df.sort_values('Category', ascending=True)\n",
    "    \n",
    "    # Take only top N if specified\n",
    "    if top_n is not None and len(df) > top_n:\n",
    "        if sort_by_value:\n",
    "            # If sorting by value, take the highest counts\n",
    "            df = df.nlargest(top_n, 'Count').sort_values('Count', ascending=True)\n",
    "        else:\n",
    "            # If sorting alphabetically, take first top_n after sorting\n",
    "            df = df.head(top_n)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    bars = plt.barh(df['Category'], df['Count'], color=color, edgecolor=edgecolor)\n",
    "    \n",
    "    # Add counts on the bars if requested\n",
    "    if show_count:\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + (width * 0.02),  # Slightly offset from the end of the bar\n",
    "                     bar.get_y() + bar.get_height()/2,\n",
    "                     f'{width:,}',  # Format with commas for thousands\n",
    "                     va='center')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Task-specific function for language distribution visualization\n",
    "def visualize_language_distribution(df, text_column, subject_main_dir, \n",
    "                                   top_n=15, min_count=5):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of languages in the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe containing the data\n",
    "    text_column (str): The column name containing text to analyze for language\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    top_n (int): Number of top languages to display\n",
    "    min_count (int): Minimum count to include in visualization\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing language distribution in {len(df)} texts...\")\n",
    "    \n",
    "    # Check if language column already exists\n",
    "    lang_column = 'detected_language'\n",
    "    if lang_column not in df.columns:\n",
    "        # Apply language detection\n",
    "        print(\"Detecting languages...\")\n",
    "        df[lang_column] = df[text_column].apply(detect_language)\n",
    "        print(\"Language detection complete.\")\n",
    "    \n",
    "    # Count languages\n",
    "    language_counts = Counter(df[lang_column].dropna())\n",
    "    \n",
    "    # Map language codes to full names for better readability\n",
    "    language_names = {\n",
    "        'en': 'English',\n",
    "        'es': 'Spanish',\n",
    "        'fr': 'French',\n",
    "        'de': 'German',\n",
    "        'it': 'Italian',\n",
    "        'pt': 'Portuguese',\n",
    "        'nl': 'Dutch',\n",
    "        'ru': 'Russian',\n",
    "        'zh': 'Chinese',\n",
    "        'ja': 'Japanese',\n",
    "        'ko': 'Korean',\n",
    "        'ar': 'Arabic',\n",
    "        'hi': 'Hindi',\n",
    "        'sv': 'Swedish',\n",
    "        'fi': 'Finnish',\n",
    "        'no': 'Norwegian',\n",
    "        'da': 'Danish',\n",
    "        'cs': 'Czech',\n",
    "        'pl': 'Polish',\n",
    "        'tr': 'Turkish',\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "    \n",
    "    # Replace codes with full names where possible\n",
    "    named_counts = {}\n",
    "    for code, count in language_counts.items():\n",
    "        name = language_names.get(code, code)  # Use code as fallback if not in mapping\n",
    "        named_counts[name] = count\n",
    "    \n",
    "    # Create visualization\n",
    "    plot_horizontal_bar_chart(\n",
    "        data=named_counts,\n",
    "        title='Distribution of Languages',\n",
    "        xlabel='Count',\n",
    "        ylabel='Language',\n",
    "        filename='language_distribution.png',\n",
    "        subject_main_dir=subject_main_dir,\n",
    "        subject_type='languages',\n",
    "        figsize=(12, 10),\n",
    "        color='lightseagreen',\n",
    "        edgecolor='teal',\n",
    "        min_count=min_count,\n",
    "        top_n=top_n,\n",
    "        sort_by_value=True,\n",
    "        show_count=True\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    total_documents = sum(language_counts.values())\n",
    "    print(f\"Total documents with detected language: {total_documents}\")\n",
    "    print(f\"Number of unique languages detected: {len(language_counts)}\")\n",
    "    \n",
    "    # Return the counts for further analysis if needed\n",
    "    return named_counts\n",
    "\n",
    "# Example usage:\n",
    "visualize_language_distribution(df, 'Title', subject_main_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5207612-b6f8-4633-97c9-cb144fae2494",
   "metadata": {},
   "source": [
    "## Data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff22f5-9fef-42e8-960e-d8040fecb522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "\n",
    "def analyze_academic_dataset(df):\n",
    "    \"\"\"\n",
    "    Analyzes academic dataset where list fields are stored as string representations of lists\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Title and Abstract Analysis\n",
    "    results['title_na_percentage'] = (df['Title'].isna().sum() / len(df)) * 100\n",
    "    \n",
    "    # Consider \"No abstract available\" as missing for Abstract\n",
    "    abstract_na = df['Abstract'].isna() | df['Abstract'].eq('No abstract available')\n",
    "    results['abstract_na_percentage'] = (abstract_na.sum() / len(df)) * 100\n",
    "    \n",
    "    # 2. Citation Count and Publication Year Statistics\n",
    "    numeric_cols = ['Citation Count', 'Publication Year']\n",
    "    numeric_stats = {}\n",
    "    for col in numeric_cols:\n",
    "        numeric_stats[col] = {\n",
    "            'median': df[col].median(),\n",
    "            'std_dev': df[col].std(),\n",
    "            'min': df[col].min(),\n",
    "            'max': df[col].max()\n",
    "        }\n",
    "    results['numeric_stats'] = numeric_stats\n",
    "    \n",
    "    # 3. DOI Analysis\n",
    "    results['doi_na_percentage'] = (df['DOI'].isna().sum() / len(df)) * 100\n",
    "    \n",
    "    # 4. AUTHORS ANALYSIS - Handle string representation of lists\n",
    "    # Convert string representations to actual lists\n",
    "    parsed_authors = df['Authors'].apply(lambda x: parse_string_list(x) if isinstance(x, str) else [])\n",
    "    \n",
    "    # Count NA values (empty lists or NaN)\n",
    "    authors_na = df['Authors'].isna() | parsed_authors.apply(lambda x: len(x) == 0)\n",
    "    authors_na_percentage = (authors_na.sum() / len(df)) * 100\n",
    "    \n",
    "    # Extract unique authors (removing URLs)\n",
    "    unique_authors = set()\n",
    "    for authors_list in parsed_authors:\n",
    "        for author in authors_list:\n",
    "            # Remove the URL part if present\n",
    "            clean_author = re.sub(r'\\s*\\(https://.*?\\)', '', author).strip()\n",
    "            if clean_author:\n",
    "                unique_authors.add(clean_author)\n",
    "    \n",
    "    results['authors_stats'] = {\n",
    "        'unique_authors': len(unique_authors),\n",
    "        'na_percentage': authors_na_percentage\n",
    "    }\n",
    "    \n",
    "    # 5. Categorical fields\n",
    "    categorical_cols = ['Journal', 'Publisher', 'Type']\n",
    "    categorical_stats = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        unique_count = df[col].dropna().nunique()\n",
    "        na_percentage = (df[col].isna().sum() / len(df)) * 100\n",
    "        \n",
    "        categorical_stats[col] = {\n",
    "            'unique_values': unique_count,\n",
    "            'na_percentage': na_percentage\n",
    "        }\n",
    "    \n",
    "    # 6. Special case: INSTITUTIONS and COUNTRIES\n",
    "    for col in ['Institutions', 'Countries']:\n",
    "        # Parse string representations to actual nested lists\n",
    "        parsed_col = df[col].apply(lambda x: parse_string_list(x) if isinstance(x, str) else [])\n",
    "        \n",
    "        # Count NA/empty\n",
    "        is_empty = df[col].isna() | parsed_col.apply(lambda x: x == [] or x == [[]])\n",
    "        na_percentage = (is_empty.sum() / len(df)) * 100\n",
    "        \n",
    "        # For these columns, we need to flatten nested lists\n",
    "        unique_values = set()\n",
    "        for item in parsed_col:\n",
    "            if isinstance(item, list):\n",
    "                # Flatten one level of nesting\n",
    "                flattened = []\n",
    "                for subitem in item:\n",
    "                    if isinstance(subitem, list):\n",
    "                        flattened.extend(subitem)\n",
    "                    else:\n",
    "                        flattened.append(subitem)\n",
    "                \n",
    "                # Add non-empty values\n",
    "                for value in flattened:\n",
    "                    if isinstance(value, str) and value.strip():\n",
    "                        normalized = re.sub(r'[^\\w\\s]', '', value.lower()).strip()\n",
    "                        if normalized:\n",
    "                            unique_values.add(normalized)\n",
    "        \n",
    "        categorical_stats[col] = {\n",
    "            'unique_values': len(unique_values),\n",
    "            'na_percentage': na_percentage\n",
    "        }\n",
    "    \n",
    "    results['categorical_stats'] = categorical_stats\n",
    "    \n",
    "    # 7. List columns analysis (Concepts, Sub-fields, Topics, Domains, Fields)\n",
    "    list_cols = ['Concepts', 'Sub-fields', 'Topics', 'Domains', 'Fields']\n",
    "    list_stats = {}\n",
    "    \n",
    "    for col in list_cols:\n",
    "        # Parse string representations to actual lists\n",
    "        parsed_col = df[col].apply(lambda x: parse_string_list(x) if isinstance(x, str) else [])\n",
    "        \n",
    "        # Count NA/empty\n",
    "        is_empty = df[col].isna() | parsed_col.apply(lambda x: len(x) == 0)\n",
    "        na_percentage = (is_empty.sum() / len(df)) * 100\n",
    "        \n",
    "        # Get unique values\n",
    "        unique_values = set()\n",
    "        for item_list in parsed_col:\n",
    "            for item in item_list:\n",
    "                if isinstance(item, str) and item.strip():\n",
    "                    # For concepts, remove score part\n",
    "                    if col == 'Concepts' and '(score:' in item:\n",
    "                        item = re.sub(r'\\s*\\(score:.*?\\)', '', item)\n",
    "                    \n",
    "                    # Normalize and add\n",
    "                    normalized = re.sub(r'[^\\w\\s]', '', item.lower()).strip()\n",
    "                    if normalized:\n",
    "                        unique_values.add(normalized)\n",
    "        \n",
    "        list_stats[col] = {\n",
    "            'unique_values': len(unique_values),\n",
    "            'na_percentage': na_percentage\n",
    "        }\n",
    "    \n",
    "    results['list_stats'] = list_stats\n",
    "    \n",
    "    return results\n",
    "\n",
    "def parse_string_list(s):\n",
    "    \"\"\"\n",
    "    Safely convert a string representation of a list to an actual list\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Use ast.literal_eval for safer parsing\n",
    "        return ast.literal_eval(s)\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If parsing fails, return empty list\n",
    "        return []\n",
    "\n",
    "def generate_latex_table(analysis_results):\n",
    "    \"\"\"Generate LaTeX table from analysis results\"\"\"\n",
    "    latex_code = r\"\"\"\\begin{table}[t]\n",
    "  \\caption{Dataset Features Description}\n",
    "  \\label{tab:dataset_description}\n",
    "  \\begin{tabular}{llcl}\\toprule\n",
    "    \\textbf{Feature} & \\textbf{Type} & \\textbf{Missing (\\%)} & \\textbf{Statistics} \\\\ \\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    # Title and Abstract\n",
    "    latex_code += f\"    Title & Text & {analysis_results['title_na_percentage']:.1f}\\\\% & - \\\\\\\\\\n\"\n",
    "    latex_code += f\"    Abstract & Text & {analysis_results['abstract_na_percentage']:.1f}\\\\% & Includes 'No abstract available' as missing \\\\\\\\\\n\"\n",
    "    \n",
    "    # Citation Count and Publication Year\n",
    "    for col in ['Citation Count', 'Publication Year']:\n",
    "        stats = analysis_results['numeric_stats'][col]\n",
    "        latex_code += f\"    {col} & Numeric & - & Median: {stats['median']:.1f}, Std: {stats['std_dev']:.1f}, \"\n",
    "        latex_code += f\"Min: {stats['min']}, Max: {stats['max']} \\\\\\\\\\n\"\n",
    "    \n",
    "    # DOI\n",
    "    latex_code += f\"    DOI & String & {analysis_results['doi_na_percentage']:.1f}\\\\% & - \\\\\\\\\\n\"\n",
    "    \n",
    "    # Authors\n",
    "    authors_stats = analysis_results['authors_stats']\n",
    "    latex_code += f\"    Authors & List & {authors_stats['na_percentage']:.1f}\\\\% & Unique authors: {authors_stats['unique_authors']} \\\\\\\\\\n\"\n",
    "    \n",
    "    # Categorical fields\n",
    "    for col, stats in analysis_results['categorical_stats'].items():\n",
    "        latex_code += f\"    {col} & Categorical & {stats['na_percentage']:.1f}\\\\% & Unique values: {stats['unique_values']} \\\\\\\\\\n\"\n",
    "    \n",
    "    # List fields\n",
    "    for col, stats in analysis_results['list_stats'].items():\n",
    "        latex_code += f\"    {col} & List & {stats['na_percentage']:.1f}\\\\% & Unique values: {stats['unique_values']} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_code += r\"  \\bottomrule\" + \"\\n\"\n",
    "    latex_code += r\"  \\end{tabular}\" + \"\\n\"\n",
    "    latex_code += r\"\\end{table}\"\n",
    "    \n",
    "    return latex_code\n",
    "\n",
    "# Usage:\n",
    "results = analyze_academic_dataset(df)\n",
    "latex = generate_latex_table(results)\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e8037-9f2e-4a8a-99ac-55dbaa3569fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_decade_column(df):\n",
    "    \"\"\"\n",
    "    Analyze the 'decade' column in the academic papers dataset.\n",
    "    Returns a dictionary with key statistics.\n",
    "    \"\"\"\n",
    "    # Make sure we're working with a clean numeric series\n",
    "    decade_series = pd.to_numeric(df['decade'], errors='coerce')\n",
    "    \n",
    "    # Basic statistics\n",
    "    decade_stats = {\n",
    "        'min': decade_series.min(),\n",
    "        'max': decade_series.max(),\n",
    "        'mode': decade_series.mode().iloc[0],  # Most common decade\n",
    "        'count': decade_series.count(),  # Number of non-NA values\n",
    "        'missing': decade_series.isna().sum(),\n",
    "        'missing_percentage': (decade_series.isna().sum() / len(df)) * 100\n",
    "    }\n",
    "    \n",
    "    # Papers per decade (for distribution analysis)\n",
    "    decade_counts = decade_series.value_counts().sort_index()\n",
    "    decade_stats['counts_by_decade'] = decade_counts.to_dict()\n",
    "    \n",
    "    # Calculate decade with highest number of papers\n",
    "    max_decade = decade_counts.idxmax()\n",
    "    decade_stats['most_papers_decade'] = max_decade\n",
    "    decade_stats['most_papers_count'] = decade_counts[max_decade]\n",
    "    \n",
    "    # Calculate growth rates between decades\n",
    "    decade_growth = {}\n",
    "    previous_count = None\n",
    "    for decade, count in decade_counts.items():\n",
    "        if previous_count is not None and previous_count > 0:\n",
    "            growth_rate = ((count - previous_count) / previous_count) * 100\n",
    "            decade_growth[decade] = growth_rate\n",
    "        previous_count = count\n",
    "    \n",
    "    decade_stats['growth_rates'] = decade_growth\n",
    "    \n",
    "    # Calculate the average growth rate\n",
    "    if decade_growth:\n",
    "        decade_stats['avg_growth_rate'] = sum(decade_growth.values()) / len(decade_growth)\n",
    "    \n",
    "    return decade_stats\n",
    "\n",
    "def generate_decade_table_row(decade_stats):\n",
    "    \"\"\"\n",
    "    Generate a LaTeX table row for the decade column.\n",
    "    \"\"\"\n",
    "    min_decade = decade_stats['min']\n",
    "    max_decade = decade_stats['max']\n",
    "    mode_decade = decade_stats['mode']\n",
    "    most_papers_decade = decade_stats['most_papers_decade']\n",
    "    most_papers_count = decade_stats['most_papers_count']\n",
    "    \n",
    "    latex_row = (\n",
    "        f\"    Decade & Numeric & {decade_stats['missing_percentage']:.1f}\\\\% & \"\n",
    "        f\"Range: {min_decade}-{max_decade}, \"\n",
    "        f\"Most papers in {most_papers_decade}s ({most_papers_count})\"\n",
    "        \" \\\\\\\\\\n\"\n",
    "    )\n",
    "    \n",
    "    return latex_row\n",
    "\n",
    "# Example usage:\n",
    "# decade_stats = analyze_decade_column(raw_df)\n",
    "# decade_row = generate_decade_table_row(decade_stats)\n",
    "# print(decade_row)\n",
    "\n",
    "# Optional: Create a visualization of papers per decade\n",
    "def plot_papers_per_decade(decade_stats):\n",
    "    \"\"\"\n",
    "    Create a bar chart of papers per decade.\n",
    "    \"\"\"\n",
    "    decades = decade_stats['counts_by_decade']\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(decades.keys(), decades.values())\n",
    "    plt.xlabel('Decade')\n",
    "    plt.ylabel('Number of Papers')\n",
    "    plt.title('Distribution of Papers by Decade')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('analyze_dataset/decade/papers_per_decade.png', dpi=300)\n",
    "    plt.close()\n",
    "# raw_df=pd.read_csv(\n",
    "decade_stats = analyze_decade_column(df)\n",
    "decade_row = generate_decade_table_row(decade_stats)\n",
    "print(decade_row)\n",
    "plot_papers_per_decade(decade_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ab651-843e-4a8b-af64-4e380ffaa6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "oa_na=(df['OpenAlex ID'].isna().sum() / len(df)) * 100\n",
    "print(oa_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9045ac-304d-4b9c-85e9-4a5463aeba87",
   "metadata": {},
   "source": [
    "# Text column analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c887aae-959d-4017-99ab-33df3475b2e0",
   "metadata": {},
   "source": [
    "## Text column utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834deafc-d993-4b0d-91f3-2f07d7882455",
   "metadata": {},
   "source": [
    "### Preprocess utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f602c68-3e6f-41e6-a404-9385884b2c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import langdetect\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=True, language='english', custom_stopwords=None):\n",
    "    \"\"\"Text preprocessing\"\"\"\n",
    "    if not isinstance(text, str) or pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        custom_academic_stopwords = ['pp','Pp.', 'vol', 'eds', 'ed', 'new', 'review', 'content', 'access', 'image', 'sizeclick', 'decrease', 'click', 'size', 'increase', 'informationnotes', 'additional']\n",
    "        try:\n",
    "            # Get standard stopwords\n",
    "            stop_words = set(stopwords.words(language))\n",
    "            stop_words.update(custom_academic_stopwords)\n",
    "            if 'Pp.' not in stop_words:\n",
    "                raise\n",
    "            # Add custom stopwords\n",
    "            if custom_stopwords:\n",
    "                stop_words.update(custom_stopwords)\n",
    "                \n",
    "            tokens = [word for word in tokens if word not in stop_words]\n",
    "        except:\n",
    "            # If language is not supported, skip stopword removal\n",
    "            pass\n",
    "    \n",
    "    # Rejoin tokens\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def filter_by_language(df, column_name, target_language='en', drop_other_languages=True,\n",
    "                       filtered_titles_path=filtered_titles_df_path, filtered_abstracts_path=filtered_abstracts_df_path):\n",
    "    \"\"\"\n",
    "    Filter a dataframe based on the detected language of a text column\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the text column\n",
    "    column_name (str): Name of the column to analyze\n",
    "    target_language (str): Target language code to keep\n",
    "    drop_other_languages (bool): Whether to drop rows with non-target languages\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (Filtered DataFrame, language statistics)\n",
    "    \"\"\"\n",
    "    print(\"filtering languages...\")\n",
    "\n",
    "    if column_name == \"Title\":\n",
    "        try:\n",
    "            filtered_df = pd.read_csv(filtered_titles_path)\n",
    "            print(\"Loaded filtered dataset...\")\n",
    "            return filtered_df, {}\n",
    "        except:\n",
    "            pass\n",
    "    elif column_name == \"Abstract\":\n",
    "        try:\n",
    "            filtered_df = pd.read_csv(filtered_abstracts_path)\n",
    "            print(\"Loaded filtered dataset...\")\n",
    "            return filtered_df, {}\n",
    "        except:\n",
    "            pass\n",
    "    # Copy dataframe to avoid modifying original\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    # Add language column\n",
    "    filtered_df['detected_language'] = filtered_df[column_name].apply(detect_language)\n",
    "    \n",
    "    # Get language statistics before filtering\n",
    "    language_counts = filtered_df['detected_language'].value_counts().to_dict()\n",
    "    \n",
    "    # Count rows before filtering\n",
    "    total_rows_before = len(filtered_df)\n",
    "    total_valid_texts = len(filtered_df[filtered_df[column_name].apply(lambda x: isinstance(x, str) and not pd.isna(x))])\n",
    "    \n",
    "    # Language statistics before filtering\n",
    "    language_stats = {\n",
    "        'language_counts': language_counts,\n",
    "        'total_texts': total_valid_texts,\n",
    "        'language_detected': len(filtered_df[filtered_df['detected_language'].notna()]),\n",
    "        'language_not_detected': len(filtered_df[filtered_df['detected_language'].isna()])\n",
    "    }\n",
    "    \n",
    "    # Filter by language if requested\n",
    "    if drop_other_languages:\n",
    "        filtered_df = filtered_df[filtered_df['detected_language'] == target_language]\n",
    "        \n",
    "        # Count rows after filtering\n",
    "        total_rows_after = len(filtered_df)\n",
    "        rows_removed = total_rows_before - total_rows_after\n",
    "        \n",
    "        # Add filtering stats to language stats\n",
    "        language_stats.update({\n",
    "            'rows_before_filtering': total_rows_before,\n",
    "            'rows_after_filtering': total_rows_after,\n",
    "            'rows_removed': rows_removed,\n",
    "            'percentage_removed': (rows_removed / total_rows_before) * 100 if total_rows_before > 0 else 0\n",
    "        })\n",
    "        \n",
    "        # Print summary of language filtering\n",
    "        print(f\"\\nLanguage Filtering Summary for column '{column_name}':\")\n",
    "        print(f\"Total records analyzed: {total_valid_texts}\")\n",
    "        print(f\"Languages detected:\")\n",
    "        \n",
    "        for lang, count in language_counts.items():\n",
    "            lang_name = lang if lang is not None else \"Unknown\"\n",
    "            print(f\"  - {lang_name}: {count} records\")\n",
    "        \n",
    "        print(f\"Filtering applied for language: {target_language}\")\n",
    "        print(f\"Records before filtering: {total_rows_before}\")\n",
    "        print(f\"Records after filtering: {total_rows_after}\")\n",
    "        print(f\"Records removed: {rows_removed} ({language_stats['percentage_removed']:.2f}%)\")\n",
    "\n",
    "    # if column_name == \"Title\":\n",
    "    #     filtered_df.to_csv(filtered_titles_df_path)\n",
    "    #     print(f\"Filtered dataset was saved into {filtered_titles_df_path}\")\n",
    "    # elif column_name == \"Abstract\":\n",
    "    #     filtered_df.to_csv(filtered_abstracts_path)\n",
    "    #     print(f\"Filtered dataset was saved into {filtered_titles_df_path}\")\n",
    "    return filtered_df, language_stats\n",
    "\n",
    "def preprocess_dataframe_text_col(df, text_column, remove_stopwords=True, language='english', \n",
    "                      filter_language=True, target_language='en', drop_other_languages=True,\n",
    "                                 output_df_path=None):\n",
    "    \"\"\"\n",
    "    Preprocess a dataframe with text column - includes language filtering, text preprocessing,\n",
    "    and special handling for terms 'anti-semitism' and 'anti-zionism'\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the text column\n",
    "    text_column (str): Name of the column to preprocess\n",
    "    remove_stopwords (bool): Whether to remove stopwords\n",
    "    language (str): Language for stopwords removal (e.g., 'english')\n",
    "    filter_language (bool): Whether to filter by language\n",
    "    target_language (str): Target language code to keep\n",
    "    drop_other_languages (bool): Whether to drop rows with non-target languages\n",
    "    output_df_path (str, optional): Path to save processed DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (Processed DataFrame, language statistics)\n",
    "    \"\"\"\n",
    "    processed_df = df.copy()\n",
    "    language_stats = None\n",
    "    \n",
    "    # First apply language filtering if requested\n",
    "    if filter_language:\n",
    "        processed_df, language_stats = filter_by_language(\n",
    "            processed_df, \n",
    "            text_column, \n",
    "            target_language=target_language, \n",
    "            drop_other_languages=drop_other_languages\n",
    "        )\n",
    "    \n",
    "    if text_column == \"Abstract\":\n",
    "        processed_df = processed_df[processed_df[\"Abstract\"] != \"No abstract available\"]\n",
    "    \n",
    "    # Replace hyphens in specific terms before general preprocessing\n",
    "    processed_df[text_column] = processed_df[text_column].str.replace(\n",
    "        r'anti-semitism', 'antisemitism', case=False, regex=True\n",
    "    )\n",
    "    processed_df[text_column] = processed_df[text_column].str.replace(\n",
    "        r'anti-zionism', 'antizionism', case=False, regex=True\n",
    "    )\n",
    "    \n",
    "    # Remove bibliography and academic paper-related terms\n",
    "    bibliography_patterns = [\n",
    "        r'\\b[Pp][Pp]\\.\\s*\\d+(-\\d+)?',  # pp. 123 or pp. 123-145\n",
    "        r'\\b[Pp][Pp]\\b',               # pp or Pp\n",
    "        r'\\bvol\\.\\s*\\d+',              # vol. 123\n",
    "        r'\\bVol\\.\\s*\\d+',              # Vol. 123\n",
    "        r'\\bissue\\s*\\d+',              # issue 123\n",
    "        r'\\bIssue\\s*\\d+',              # Issue 123\n",
    "        r'\\beditor[s]?\\b',             # editor or editors\n",
    "        r'\\bet\\s+al\\.',                # et al.\n",
    "        r'\\bibid\\.',                   # ibid.\n",
    "        r'\\bop\\.\\s*cit\\.',             # op. cit.\n",
    "        r'\\bcf\\.',                     # cf.\n",
    "        r'\\bvide\\b',                   # vide\n",
    "        r'\\b[Ii]n\\s+press\\b',          # in press or In press\n",
    "        r'\\b[Ff]orthcoming\\b',         # forthcoming or Forthcoming\n",
    "        r'\\bDOI:.*?\\b',                # DOI: followed by the identifier\n",
    "        r'\\bISBN:.*?\\b',               # ISBN: followed by the number\n",
    "        r'\\bISSN:.*?\\b',               # ISSN: followed by the number\n",
    "        r'\\bAccessed\\s+on\\b.*?\\d{4}',  # Accessed on date\n",
    "        r'\\bretrieved\\s+from\\b'        # retrieved from\n",
    "    ]\n",
    "    \n",
    "    for pattern in bibliography_patterns:\n",
    "        processed_df[text_column] = processed_df[text_column].str.replace(\n",
    "            pattern, '', regex=True\n",
    "        )\n",
    "        \n",
    "    # Apply text preprocessing to the column\n",
    "    processed_df[f'{text_column}_processed'] = processed_df[text_column].apply(\n",
    "        lambda x: preprocess_text(x, remove_stopwords=remove_stopwords, language=language)\n",
    "    )\n",
    "    \n",
    "    if output_df_path:\n",
    "        processed_df.to_csv(output_df_path)\n",
    "    \n",
    "    return processed_df, language_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8312269-dd12-4845-872f-04ab8c438aff",
   "metadata": {},
   "source": [
    "### Analysis utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb9af2-be83-45a8-9456-b523fc2b5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import langdetect\n",
    "from pathlib import Path\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"\n",
    "    Count number of words in text\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): Text to analyze\n",
    "    \n",
    "    Returns:\n",
    "    int: Word count\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or pd.isna(text):\n",
    "        return 0\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "def get_most_common_words(texts, n=10, preprocess=True, remove_stopwords=True, language='english'):\n",
    "    \"\"\"\n",
    "    Get most common words across multiple texts\n",
    "    \n",
    "    Parameters:\n",
    "    texts (list/Series): List or Series of texts to analyze\n",
    "    n (int): Number of most common words to return\n",
    "    preprocess (bool): Whether to preprocess texts\n",
    "    remove_stopwords (bool): Whether to remove stopwords\n",
    "    language (str): Language for stopwords removal\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary of word counts\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    \n",
    "    for text in texts:\n",
    "        if preprocess and isinstance(text, str) and not pd.isna(text):\n",
    "            text = preprocess_text(text, remove_stopwords, language)\n",
    "            \n",
    "        if isinstance(text, str) and not pd.isna(text):\n",
    "            all_words.extend(word_tokenize(text))\n",
    "    \n",
    "    word_counts = Counter(all_words)\n",
    "    return dict(word_counts.most_common(n))\n",
    "\n",
    "def create_wordcloud(texts, preprocess=True, remove_stopwords=True, language='english', max_words=100):\n",
    "    \"\"\"\n",
    "    Create a word cloud from a collection of texts\n",
    "    \n",
    "    Parameters:\n",
    "    texts (list/Series): List or Series of texts\n",
    "    preprocess (bool): Whether to preprocess texts\n",
    "    remove_stopwords (bool): Whether to remove stopwords\n",
    "    language (str): Language for stopwords removal\n",
    "    max_words (int): Maximum number of words in wordcloud\n",
    "    \n",
    "    Returns:\n",
    "    WordCloud: WordCloud object\n",
    "    \"\"\"\n",
    "    combined_text = \"\"\n",
    "    \n",
    "    for text in texts:\n",
    "        if isinstance(text, str) and not pd.isna(text):\n",
    "            if preprocess:\n",
    "                text = preprocess_text(text, remove_stopwords, language)\n",
    "            combined_text += \" \" + text\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=400, \n",
    "        max_words=max_words, \n",
    "        background_color='white',\n",
    "        collocations=False\n",
    "    ).generate(combined_text)\n",
    "    \n",
    "    return wordcloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cf0fa-d793-4a95-a89b-58056341a4fc",
   "metadata": {},
   "source": [
    "## General text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbb6fa7-21e5-4f5c-9134-2c00b4dba15b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def text_length_stats(texts):\n",
    "    \"\"\"\n",
    "    Calculate statistics about text lengths\n",
    "    \n",
    "    Parameters:\n",
    "    texts (list/Series): List or Series of texts\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with length statistics\n",
    "    \"\"\"\n",
    "    # Convert to series if list\n",
    "    if isinstance(texts, list):\n",
    "        texts = pd.Series(texts)\n",
    "    \n",
    "    # Filter out non-string and NaN values\n",
    "    valid_texts = texts[texts.apply(lambda x: isinstance(x, str) and not pd.isna(x))]\n",
    "    \n",
    "    # Calculate lengths\n",
    "    lengths = valid_texts.apply(len)\n",
    "    word_counts = valid_texts.apply(count_words)\n",
    "    \n",
    "    return {\n",
    "        'char_mean': lengths.mean(),\n",
    "        'char_median': lengths.median(),\n",
    "        'char_min': lengths.min(),\n",
    "        'char_max': lengths.max(),\n",
    "        'word_mean': word_counts.mean(),\n",
    "        'word_median': word_counts.median(),\n",
    "        'word_min': word_counts.min(),\n",
    "        'word_max': word_counts.max(),\n",
    "        'total_texts': len(valid_texts)\n",
    "    }\n",
    "\n",
    "def visualize_text_length_trends_by_decade(df, text_column, subject_main_dir, subject_type, decade_column=\"decade\"):\n",
    "    \"\"\"\n",
    "    Visualize trends of text lengths (both characters and words) over time by decade\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the text and decade columns\n",
    "    text_column (str): Name of the column with text to analyze\n",
    "    decade_column (str): Name of the column with decade information\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    \n",
    "    Returns:\n",
    "    dict: Analysis results\n",
    "    \"\"\"\n",
    "    # Extract text data, filtering out non-string and NaN values\n",
    "    valid_df = df.dropna(subset=[text_column, decade_column])\n",
    "    valid_df = valid_df[valid_df[text_column].apply(lambda x: isinstance(x, str))]\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        return {\"error\": f\"No valid text data found in column {text_column}\"}\n",
    "    \n",
    "    # Calculate character and word lengths\n",
    "    valid_df['char_length'] = valid_df[text_column].str.len()\n",
    "    valid_df['word_count'] = valid_df[text_column].apply(count_words)\n",
    "    \n",
    "    # Group by decade and calculate average lengths\n",
    "    decade_stats = valid_df.groupby(decade_column).agg({\n",
    "        'char_length': 'mean',\n",
    "        'word_count': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Sort by decade\n",
    "    decade_stats = decade_stats.sort_values(by=decade_column)\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot character length trends\n",
    "    sns.lineplot(data=decade_stats, x=decade_column, y='char_length', marker='o', ax=ax1)\n",
    "    ax1.set_title(f'Average Character Length by Decade for {text_column}')\n",
    "    ax1.set_xlabel('Decade')\n",
    "    ax1.set_ylabel('Average Number of Characters')\n",
    "    \n",
    "    # Plot word count trends\n",
    "    sns.lineplot(data=decade_stats, x=decade_column, y='word_count', marker='o', ax=ax2)\n",
    "    ax2.set_title(f'Average Word Count by Decade for {text_column}')\n",
    "    ax2.set_xlabel('Decade')\n",
    "    ax2.set_ylabel('Average Number of Words')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, f\"{text_column}_length_trends_by_decade.png\", subject_main_dir, subject_type)\n",
    "    \n",
    "    return {\n",
    "        \"column_name\": text_column,\n",
    "        \"decade_stats\": decade_stats.to_dict(),\n",
    "        \"visualizations\": [\n",
    "            f\"{subject_type}/{text_column}_length_trends_by_decade.png\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def visualize_word_frequencies(df, column_name, subject_main_dir, subject_type, n=20, \n",
    "                             preprocess=True, remove_stopwords=True, language='english'):\n",
    "    \"\"\"\n",
    "    Visualize the most frequent words in a text column\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the text column\n",
    "    column_name (str): Name of the column to analyze\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    n (int): Number of most common words to show\n",
    "    preprocess (bool): Whether to preprocess texts\n",
    "    remove_stopwords (bool): Whether to remove stopwords\n",
    "    language (str): Language for stopwords removal\n",
    "    \n",
    "    Returns:\n",
    "    dict: Analysis results\n",
    "    \"\"\"\n",
    "    # Extract text data, filtering out non-string and NaN values\n",
    "    text_data = df[column_name].dropna()\n",
    "    text_data = text_data[text_data.apply(lambda x: isinstance(x, str))]\n",
    "    \n",
    "    if len(text_data) == 0:\n",
    "        return {\"error\": f\"No valid text data found in column {column_name}\"}\n",
    "    \n",
    "    # Get most common words\n",
    "    common_words = get_most_common_words(\n",
    "        text_data, \n",
    "        n=n, \n",
    "        preprocess=preprocess, \n",
    "        remove_stopwords=remove_stopwords, \n",
    "        language=language\n",
    "    )\n",
    "    \n",
    "    # Create bar plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Sort by frequency\n",
    "    words = list(common_words.keys())\n",
    "    counts = list(common_words.values())\n",
    "    \n",
    "    # Create horizontal bar plot\n",
    "    plt.barh(range(len(words)), counts, align='center')\n",
    "    plt.yticks(range(len(words)), words)\n",
    "    \n",
    "    # Add labels and title\n",
    "    stopwords_text = \"without stopwords\" if remove_stopwords else \"with stopwords\"\n",
    "    plt.title(f'Top {n} Most Common Words in {column_name} ({stopwords_text})')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Words')\n",
    "    \n",
    "    # Add count labels to bars\n",
    "    for i, count in enumerate(counts):\n",
    "        plt.text(count + 0.1, i, str(count), va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    stopwords_flag = \"no_stopwords\" if remove_stopwords else \"with_stopwords\"\n",
    "    filename = f\"{column_name}_top{n}_words_{stopwords_flag}.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    # Create word cloud\n",
    "    wordcloud = create_wordcloud(\n",
    "        text_data, \n",
    "        preprocess=preprocess, \n",
    "        remove_stopwords=remove_stopwords, \n",
    "        language=language\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud for {column_name} ({stopwords_text})')\n",
    "    \n",
    "    # Save wordcloud\n",
    "    wc_filename = f\"{column_name}_wordcloud_{stopwords_flag}.png\"\n",
    "    save_plot(plt, wc_filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    return {\n",
    "        \"column_name\": column_name,\n",
    "        \"top_words\": common_words,\n",
    "        \"visualizations\": [\n",
    "            f\"{subject_type}/{filename}\",\n",
    "            f\"{subject_type}/{wc_filename}\"\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8820185-2991-436b-97d0-78fd041c915d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_text_column(df, column_name, subject_main_dir, filter_language=True, \n",
    "                      target_language='en', remove_stopwords=True, language='english', \n",
    "                       proc_df_out_path=None, subject_type=None):\n",
    "    \"\"\"\n",
    "    Run general text analysis on a string column\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the column to analyze\n",
    "    column_name (str): Name of the column to analyze\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    filter_language (bool): Whether to filter by language\n",
    "    target_language (str): Target language code to keep\n",
    "    remove_stopwords (bool): Whether to remove stopwords in preprocessing\n",
    "    language (str): Language for stopwords removal\n",
    "    \n",
    "    Returns:\n",
    "    dict: Analysis results or error message\n",
    "    \"\"\"\n",
    "    # Validate that the column exists\n",
    "    if column_name not in df.columns:\n",
    "        return {\"error\": f\"Column '{column_name}' not found in DataFrame\"}\n",
    "    \n",
    "    # Check if column contains string values\n",
    "    text_data = df[column_name].dropna()\n",
    "    string_values = text_data[text_data.apply(lambda x: isinstance(x, str))]\n",
    "    \n",
    "    if len(string_values) == 0:\n",
    "        return {\"error\": f\"Column '{column_name}' does not contain any string values\"}\n",
    "\n",
    "    if not subject_type:\n",
    "        # Create subject type from column name (lowercase)\n",
    "        subject_type = column_name.lower()\n",
    "    \n",
    "    print(f\"\\nAnalyzing text column: '{column_name}'\")\n",
    "    print(f\"Using subject type: '{subject_type}'\")\n",
    "    \n",
    "    # Step 1: Preprocess the data with optional language filtering\n",
    "    print(\"\\nStep 1: Preprocessing data...\")\n",
    "    processed_df, language_stats = preprocess_dataframe_text_col(\n",
    "        df=df,\n",
    "        text_column=column_name,\n",
    "        remove_stopwords=remove_stopwords,\n",
    "        language=language,\n",
    "        filter_language=filter_language,\n",
    "        target_language=target_language,\n",
    "        drop_other_languages=filter_language,\n",
    "        output_df_path=proc_df_out_path\n",
    "    )\n",
    "    \n",
    "    # Step 2: Analyze text length distribution\n",
    "    print(\"\\nStep 2: Analyzing text length distribution...\")\n",
    "    length_results = visualize_text_length_trends_by_decade(\n",
    "        df=processed_df,\n",
    "        text_column=column_name,\n",
    "        subject_main_dir=subject_main_dir,\n",
    "        subject_type=subject_type\n",
    "    )\n",
    "    \n",
    "    # Step 3: Analyze word frequencies\n",
    "    print(\"\\nStep 3: Analyzing word frequencies...\")\n",
    "    word_freq_results = visualize_word_frequencies(\n",
    "        df=processed_df,\n",
    "        column_name=column_name,\n",
    "        subject_main_dir=subject_main_dir,\n",
    "        subject_type=subject_type,\n",
    "        n=20,\n",
    "        preprocess=True,\n",
    "        remove_stopwords=remove_stopwords,\n",
    "        language=language\n",
    "    )\n",
    "    \n",
    "    # Compile all results\n",
    "    results = {\n",
    "        \"column_name\": column_name,\n",
    "        \"subject_type\": subject_type,\n",
    "        \"language_stats\": language_stats,\n",
    "        \"length_stats\": length_results[\"stats\"] if \"stats\" in length_results else None,\n",
    "        \"top_words\": word_freq_results[\"top_words\"] if \"top_words\" in word_freq_results else None,\n",
    "        \"visualizations\": (\n",
    "            length_results.get(\"visualizations\", []) + \n",
    "            word_freq_results.get(\"visualizations\", [])\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nAnalysis complete for column '{column_name}'\")\n",
    "    print(f\"Generated {len(results['visualizations'])} visualizations in {subject_main_dir}/{subject_type}/\")\n",
    "    \n",
    "    return results, processed_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c68874-86e8-433b-b456-fec4e9eb3399",
   "metadata": {},
   "source": [
    "## Entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b43c1a-25fc-45ca-9688-2c811c3309c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_entities(df, column_name, model='en_core_web_sm', entity_types=None, min_count=1):\n",
    "    \"\"\"\n",
    "    Extract named entities from a text column\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the text column\n",
    "    column_name (str): Name of the column to analyze\n",
    "    model (str): spaCy model to use for NER\n",
    "    entity_types (list): List of entity types to extract (None for all)\n",
    "    min_count (int): Minimum count for an entity to be included\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing entity analysis results\n",
    "    \"\"\"\n",
    "    # Validate that the column exists\n",
    "    if column_name not in df.columns:\n",
    "        return {\"error\": f\"Column '{column_name}' not found in DataFrame\"}\n",
    "    \n",
    "    # Load spaCy model\n",
    "    try:\n",
    "        nlp = spacy.load(model)\n",
    "    except OSError:\n",
    "        # If model not installed, attempt to download it\n",
    "        print(f\"Model {model} not found. Installing...\")\n",
    "        import subprocess\n",
    "        subprocess.call(f\"python -m spacy download {model}\", shell=True)\n",
    "        try:\n",
    "            nlp = spacy.load(model)\n",
    "        except:\n",
    "            return {\"error\": f\"Failed to load spaCy model {model}. Please install it manually.\"}\n",
    "    \n",
    "    # Extract text data, filtering out non-string and NaN values\n",
    "    text_data = df[column_name].dropna()\n",
    "    text_data = text_data[text_data.apply(lambda x: isinstance(x, str))]\n",
    "    \n",
    "    if len(text_data) == 0:\n",
    "        return {\"error\": f\"Column '{column_name}' does not contain any string values\"}\n",
    "    \n",
    "    print(f\"Extracting entities from {len(text_data)} text entries...\")\n",
    "    \n",
    "    # Process texts with spaCy\n",
    "    all_entities = []\n",
    "    entity_texts = []\n",
    "    \n",
    "    for text in tqdm(text_data):\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Extract entities\n",
    "        for ent in doc.ents:\n",
    "            # Filter by entity type if specified\n",
    "            if entity_types is None or ent.label_ in entity_types:\n",
    "                all_entities.append((ent.text, ent.label_))\n",
    "                entity_texts.append(ent.text)\n",
    "    \n",
    "    # Count entities by text\n",
    "    entity_counts = Counter(entity_texts)\n",
    "    \n",
    "    # Filter by minimum count\n",
    "    filtered_entities = {k: v for k, v in entity_counts.items() if v >= min_count}\n",
    "    \n",
    "    # Count by entity type\n",
    "    entity_types_count = Counter([entity[1] for entity in all_entities])\n",
    "    \n",
    "    # Create a list of entities with their type and count\n",
    "    entity_details = []\n",
    "    for entity_text, entity_type in set(all_entities):\n",
    "        count = entity_counts[entity_text]\n",
    "        if count >= min_count:\n",
    "            entity_details.append({\n",
    "                'text': entity_text,\n",
    "                'type': entity_type,\n",
    "                'count': count\n",
    "            })\n",
    "    \n",
    "    # Sort by count\n",
    "    entity_details = sorted(entity_details, key=lambda x: x['count'], reverse=True)\n",
    "    \n",
    "    return {\n",
    "        \"entity_counts\": filtered_entities,\n",
    "        \"entity_types_count\": dict(entity_types_count),\n",
    "        \"entity_details\": entity_details,\n",
    "        \"total_entities\": len(all_entities),\n",
    "        \"unique_entities\": len(filtered_entities)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c3b09f-12ba-45d6-ac50-c54302b1e0ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "def visualize_entity_wordcloud(entity_results, subject_main_dir, subject_type, max_words=100, colormap='viridis'):\n",
    "    \"\"\"\n",
    "    Create a word cloud visualization of entities\n",
    "    \n",
    "    Parameters:\n",
    "    entity_results (dict): Results from extract_entities function\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    max_words (int): Maximum number of words to include in word cloud\n",
    "    colormap (str): Matplotlib colormap for word cloud\n",
    "    \n",
    "    Returns:\n",
    "    str: Path to saved visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    if \"error\" in entity_results:\n",
    "        return {\"error\": entity_results[\"error\"]}\n",
    "    \n",
    "    # Create frequency dictionary for wordcloud\n",
    "    entity_freq = entity_results[\"entity_counts\"]\n",
    "    \n",
    "    # Create wordcloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=500, \n",
    "        max_words=max_words, \n",
    "        background_color='white',\n",
    "        colormap=colormap,\n",
    "        collocations=False\n",
    "    ).generate_from_frequencies(entity_freq)\n",
    "    \n",
    "    # Display wordcloud\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Entity Word Cloud')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    filename = \"entity_wordcloud.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    return f\"{subject_type}/{filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d712fd-2060-4c70-8225-d4ee7b6de577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_entity_trends(entity_results, subject_main_dir, subject_type, max_entities=20):\n",
    "    \"\"\"\n",
    "    Visualize entity trends from extracted entity data\n",
    "    \n",
    "    Parameters:\n",
    "    entity_results (dict): Results from extract_entities function\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    max_entities (int): Maximum number of entities to show in visualizations\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with visualization paths\n",
    "    \"\"\"\n",
    "    if \"error\" in entity_results:\n",
    "        return {\"error\": entity_results[\"error\"]}\n",
    "    \n",
    "    visualizations = []\n",
    "    \n",
    "    # [Previous visualizations code remains the same]\n",
    "    \n",
    "    # Add entity word cloud visualization\n",
    "    wordcloud_viz = visualize_entity_wordcloud(entity_results, subject_main_dir, subject_type)\n",
    "    visualizations.append(wordcloud_viz)\n",
    "    \n",
    "    return {\n",
    "        \"visualizations\": visualizations,\n",
    "        \"total_entities\": entity_results[\"total_entities\"],\n",
    "        \"unique_entities\": entity_results[\"unique_entities\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eda2be9-aa72-4591-bb41-1cbb723bfcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_entities_in_column(df, column_name, subject_main_dir, min_count=2, max_entities=20, model='en_core_web_sm', preprocess=True,\n",
    "                              proc_df_out_path=None, subject_type=None):\n",
    "    \"\"\"\n",
    "    Run complete entity extraction and visualization on a text column\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the text column\n",
    "    column_name (str): Name of the column to analyze\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    min_count (int): Minimum count for an entity to be included\n",
    "    max_entities (int): Maximum number of entities to show in visualizations\n",
    "    model (str): spaCy model to use for NER\n",
    "    \n",
    "    Returns:\n",
    "    dict: Combined results from entity extraction and visualization\n",
    "    \"\"\"\n",
    "    if preprocess:\n",
    "        df, language_stats = preprocess_dataframe_text_col(df=df, text_column=column_name, output_df_path=proc_df_out_path)\n",
    "    if not subject_type:\n",
    "        # Create subject type from column name (lowercase)\n",
    "        subject_type = column_name.lower()\n",
    "    \n",
    "    print(f\"Running entity recognition on '{column_name}' column...\")\n",
    "    \n",
    "    # Extract entities\n",
    "    entity_results = extract_entities(\n",
    "        df=df, \n",
    "        column_name=column_name, \n",
    "        model=model,\n",
    "        entity_types=None,\n",
    "        min_count=min_count\n",
    "    )\n",
    "    \n",
    "    if \"error\" in entity_results:\n",
    "        print(f\"Error: {entity_results['error']}\")\n",
    "        return entity_results\n",
    "    \n",
    "    # Print basic stats\n",
    "    print(f\"Found {entity_results['total_entities']} total entities\")\n",
    "    print(f\"Found {entity_results['unique_entities']} unique entities\")\n",
    "    \n",
    "    print(\"\\nTop entity types:\")\n",
    "    for entity_type, count in sorted(entity_results['entity_types_count'].items(), \n",
    "                                  key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"  - {entity_type}: {count}\")\n",
    "\n",
    "    print(\"\\nTop entities:\")\n",
    "    for entity in entity_results['entity_details'][:10]:\n",
    "        print(f\"  - {entity['text']} ({entity['type']}): {entity['count']} occurrences\")\n",
    "    \n",
    "    # Visualize entity trends\n",
    "    viz_results = visualize_entity_trends(\n",
    "        entity_results=entity_results,\n",
    "        subject_main_dir=subject_main_dir,\n",
    "        subject_type=subject_type,\n",
    "        max_entities=max_entities\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nGenerated {len(viz_results['visualizations'])} visualizations:\")\n",
    "    for viz in viz_results['visualizations']:\n",
    "        print(f\"  - {viz}\")\n",
    "    \n",
    "    # Combine results\n",
    "    combined_results = {\n",
    "        \"column_name\": column_name,\n",
    "        \"subject_type\": subject_type,\n",
    "        \"entity_results\": entity_results,\n",
    "        \"visualization_results\": viz_results\n",
    "    }\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ccb05b-b47e-4318-9438-8fe3ef69b6cb",
   "metadata": {},
   "source": [
    "## Bert-Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de765c34-2ae8-4099-99ea-afbd1bdd5cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_topic_modeling(df, column_name, model=None, language=\"english\", \n",
    "                          nr_topics=\"auto\", min_topic_size=10):\n",
    "    \"\"\"\n",
    "    Perform topic modeling on text data using BERTopic\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the text column\n",
    "    column_name (str): Name of the column to analyze\n",
    "    model (BERTopic, optional): Pre-configured BERTopic model\n",
    "    language (str): Language for stopwords removal and preprocessing\n",
    "    nr_topics (int or str): Number of topics to find ('auto' or specific number)\n",
    "    min_topic_size (int): Minimum size of topics\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (model, topics, probabilities, processed_df)\n",
    "    \"\"\"\n",
    "    # Extract text data, filtering out non-string and NaN values\n",
    "    processed_df = df.copy()\n",
    "    text_data = processed_df[column_name].dropna()\n",
    "    processed_df = processed_df[processed_df[column_name].apply(lambda x: isinstance(x, str) and pd.notna(x))]\n",
    "    \n",
    "    if len(processed_df) == 0:\n",
    "        return {\"error\": f\"No valid text data found in column {column_name}\"}\n",
    "    \n",
    "    # Apply specific filtering if needed\n",
    "    if column_name == \"Abstract\":\n",
    "        processed_df = processed_df[processed_df[\"Abstract\"] != \"No abstract available\"]\n",
    "    \n",
    "    # Get clean text data\n",
    "    documents = processed_df[column_name].tolist()\n",
    "    \n",
    "    # Create model if not provided\n",
    "    if model is None:\n",
    "        model = setup_bertopic(language, nr_topics, min_topic_size)\n",
    "        if model is None:\n",
    "            return {\"error\": \"Failed to initialize BERTopic model\"}\n",
    "    \n",
    "    # Fit the model on our text data\n",
    "    topics, probabilities = model.fit_transform(documents)\n",
    "    \n",
    "    # Add topics to the dataframe\n",
    "    processed_df[\"bertopic_topic\"] = topics\n",
    "    \n",
    "    # Fix for the \"numpy.float64 has no len()\" error\n",
    "    # Check if probabilities is a numpy array and handle accordingly\n",
    "    if hasattr(probabilities, 'ndim') and probabilities.ndim == 2:\n",
    "        # If probabilities is a 2D array, get the max value for each row\n",
    "        processed_df[\"bertopic_probability\"] = [prob.max() for prob in probabilities]\n",
    "    elif hasattr(probabilities, 'ndim') and probabilities.ndim == 1:\n",
    "        # If probabilities is a 1D array, use it directly\n",
    "        processed_df[\"bertopic_probability\"] = probabilities\n",
    "    else:\n",
    "        # Handle case where probabilities might be a list of arrays or other structures\n",
    "        processed_df[\"bertopic_probability\"] = [\n",
    "            prob.max() if hasattr(prob, 'max') else \n",
    "            (max(prob) if hasattr(prob, '__len__') and len(prob) > 0 else 0) \n",
    "            for prob in probabilities\n",
    "        ]\n",
    "    \n",
    "    return model, topics, probabilities, processed_df\n",
    "\n",
    "\n",
    "def setup_bertopic(language=\"english\", nr_topics=\"auto\", min_topic_size=10):\n",
    "    \"\"\"\n",
    "    Set up and configure a BERTopic model\n",
    "    \n",
    "    Parameters:\n",
    "    language (str): Language for stopwords removal and preprocessing\n",
    "    nr_topics (int or str): Number of topics to find ('auto' or specific number)\n",
    "    min_topic_size (int): Minimum size of topics\n",
    "    \n",
    "    Returns:\n",
    "    BERTopic model: Configured model instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from bertopic import BERTopic\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        from nltk.corpus import stopwords\n",
    "        import nltk\n",
    "        \n",
    "        # Download stopwords if needed\n",
    "        try:\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            nltk.download('stopwords')\n",
    "        \n",
    "        # Get stopwords for the specified language\n",
    "        stop_words = stopwords.words(language)\n",
    "        \n",
    "        # Set up the vectorizer with stopwords\n",
    "        vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "        \n",
    "        # Set environment variable to avoid the tokenizers parallelism warning\n",
    "        import os\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "        \n",
    "        # Create and return the BERTopic model\n",
    "        return BERTopic(\n",
    "            language=language,\n",
    "            nr_topics=nr_topics,\n",
    "            min_topic_size=min_topic_size,\n",
    "            vectorizer_model=vectorizer\n",
    "        )\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"Please install required packages with: pip install bertopic nltk\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def visualize_topic_wordclouds(model, subject_main_dir, subject_type, \n",
    "                              column_name, top_n_topics=10):\n",
    "    \"\"\"\n",
    "    Visualize topics as wordclouds\n",
    "    \n",
    "    Parameters:\n",
    "    model (BERTopic): Fitted BERTopic model\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    column_name (str): Name of the column analyzed\n",
    "    top_n_topics (int): Number of top topics to visualize\n",
    "    \n",
    "    Returns:\n",
    "    dict: Visualization details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get topic info dataframe\n",
    "        topic_info = model.get_topic_info()\n",
    "        \n",
    "        # Get topics excluding the -1 outlier topic\n",
    "        if -1 in model.get_topics():\n",
    "            top_topics = [topic for topic in topic_info['Topic'].tolist() \n",
    "                         if topic != -1][:top_n_topics]\n",
    "        else:\n",
    "            top_topics = topic_info['Topic'].tolist()[:top_n_topics]\n",
    "        \n",
    "        # Create figure for word clouds\n",
    "        fig = model.visualize_topics(topics=top_topics)\n",
    "        \n",
    "        # Save the visualization\n",
    "        from pathlib import Path\n",
    "        Path(f\"{subject_main_dir}/{subject_type}\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        filename = f\"{column_name}_topic_wordclouds.html\"\n",
    "        fig.write_html(f\"{subject_main_dir}/{subject_type}/{filename}\")\n",
    "        \n",
    "        return {\n",
    "            \"visualization\": f\"{subject_type}/{filename}\",\n",
    "            \"type\": \"topic_wordclouds\",\n",
    "            \"topics\": top_topics\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\"error\": f\"Failed to create topic wordclouds: {str(e)}\\n{traceback.format_exc()}\"}\n",
    "\n",
    "\n",
    "def visualize_topic_barchart(model, subject_main_dir, subject_type, column_name, n_topics=10):\n",
    "    \"\"\"\n",
    "    Visualize top terms for each topic as bar charts\n",
    "    \n",
    "    Parameters:\n",
    "    model (BERTopic): Fitted BERTopic model\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    column_name (str): Name of the column analyzed\n",
    "    n_topics (int): Number of topics to show\n",
    "    \n",
    "    Returns:\n",
    "    dict: Visualization details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get topic info dataframe\n",
    "        topic_info = model.get_topic_info()\n",
    "        \n",
    "        # Exclude -1 topic (outliers)\n",
    "        topics = [topic for topic in topic_info['Topic'].tolist() \n",
    "                 if topic != -1][:n_topics]\n",
    "        \n",
    "        # Create visualization\n",
    "        fig = model.visualize_barchart(topics=topics, top_n_topics=n_topics)\n",
    "        \n",
    "        # Save the visualization\n",
    "        from pathlib import Path\n",
    "        Path(f\"{subject_main_dir}/{subject_type}\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        filename = f\"{column_name}_topic_barchart.html\"\n",
    "        fig.write_html(f\"{subject_main_dir}/{subject_type}/{filename}\")\n",
    "        \n",
    "        return {\n",
    "            \"visualization\": f\"{subject_type}/{filename}\",\n",
    "            \"type\": \"topic_barchart\",\n",
    "            \"topics\": topics\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\"error\": f\"Failed to create topic barchart: {str(e)}\\n{traceback.format_exc()}\"}\n",
    "\n",
    "\n",
    "def visualize_topics_over_time(model, processed_df, decade_column, subject_main_dir, \n",
    "                              subject_type, column_name, top_n_topics=10):\n",
    "    \"\"\"\n",
    "    Visualize topic trends over decades\n",
    "    \n",
    "    Parameters:\n",
    "    model (BERTopic): Fitted BERTopic model\n",
    "    processed_df (DataFrame): DataFrame with assigned topics\n",
    "    decade_column (str): Name of the column with decade information\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    column_name (str): Name of the column analyzed\n",
    "    top_n_topics (int): Number of top topics to visualize\n",
    "    \n",
    "    Returns:\n",
    "    dict: Visualization details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        from pathlib import Path\n",
    "        \n",
    "        # Ensure decade column exists\n",
    "        if decade_column not in processed_df.columns:\n",
    "            return {\"error\": f\"Decade column '{decade_column}' not found in DataFrame\"}\n",
    "        \n",
    "        # Create copy to avoid modifying original\n",
    "        df_time = processed_df.copy()\n",
    "        \n",
    "        # Convert topics to document-topic matrix\n",
    "        topics = df_time[\"bertopic_topic\"].tolist()\n",
    "        documents = df_time[column_name].tolist()\n",
    "        \n",
    "        # Handle timestamps properly\n",
    "        if pd.api.types.is_datetime64_dtype(df_time[decade_column]):\n",
    "            # Already datetime format\n",
    "            timestamps = df_time[decade_column].tolist()\n",
    "        else:\n",
    "            # Convert to datetime - handle both numeric and string formats\n",
    "            try:\n",
    "                # Try to convert directly\n",
    "                timestamps = pd.to_datetime(df_time[decade_column]).tolist()\n",
    "            except:\n",
    "                # If that fails, try to interpret as year values\n",
    "                try:\n",
    "                    # First convert to string if numeric\n",
    "                    year_strings = df_time[decade_column].astype(str).tolist()\n",
    "                    # Then convert to datetime with January 1st of that year\n",
    "                    timestamps = [pd.to_datetime(f\"{year}-01-01\") for year in year_strings]\n",
    "                except:\n",
    "                    return {\"error\": f\"Could not convert '{decade_column}' to datetime format\"}\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        Path(f\"{subject_main_dir}/{subject_type}\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get basic topic information for labels\n",
    "        topic_info = model.get_topic_info()\n",
    "        \n",
    "        # Set up bins for decades\n",
    "        if isinstance(timestamps[0], pd.Timestamp):\n",
    "            min_year = min(ts.year for ts in timestamps)\n",
    "            max_year = max(ts.year for ts in timestamps)\n",
    "        else:\n",
    "            # Fallback if conversion somehow failed\n",
    "            try:\n",
    "                min_year = int(min(df_time[decade_column]))\n",
    "                max_year = int(max(df_time[decade_column]))\n",
    "            except:\n",
    "                min_year = 1900  # Fallback\n",
    "                max_year = 2020  # Fallback\n",
    "        \n",
    "        # Round to decades\n",
    "        min_decade = (min_year // 10) * 10\n",
    "        max_decade = ((max_year + 9) // 10) * 10\n",
    "        decades = range(min_decade, max_decade + 10, 10)\n",
    "        \n",
    "        # Skip BERTopic's built-in time visualization which is causing errors\n",
    "        # Instead, create our own visualization with Matplotlib\n",
    "        \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Add a decade label column for grouping\n",
    "        if isinstance(timestamps[0], pd.Timestamp):\n",
    "            df_time['decade_label'] = [((ts.year // 10) * 10) for ts in timestamps]\n",
    "        else:\n",
    "            # Fallback\n",
    "            df_time['decade_label'] = df_time[decade_column].astype(int) // 10 * 10\n",
    "        \n",
    "        # Get top topics excluding outliers\n",
    "        top_topics = [topic for topic in topic_info['Topic'].tolist() \n",
    "                     if topic != -1][:top_n_topics]\n",
    "        \n",
    "        # Group by decade and count topics\n",
    "        decade_topic_counts = df_time.groupby(['decade_label', 'bertopic_topic']).size().unstack().fillna(0)\n",
    "        \n",
    "        # Filter to top topics if they exist in the unstack result\n",
    "        available_topics = set(decade_topic_counts.columns).intersection(top_topics)\n",
    "        if available_topics:\n",
    "            decade_topic_counts = decade_topic_counts[[col for col in decade_topic_counts.columns if col in top_topics]]\n",
    "        \n",
    "        # Convert to percentages\n",
    "        totals = decade_topic_counts.sum(axis=1)\n",
    "        decade_topic_pcts = decade_topic_counts.div(totals, axis=0) * 100\n",
    "        \n",
    "        # Get topic names/labels\n",
    "        topic_names = {}\n",
    "        for topic in top_topics:\n",
    "            if topic in model.get_topics():\n",
    "                # Get the top 3 words for this topic\n",
    "                topic_words = model.get_topic(topic)\n",
    "                if topic_words:  # Check if topic_words is not empty\n",
    "                    words = [word for word, _ in topic_words][:3]\n",
    "                    topic_names[topic] = f\"Topic {topic}: {', '.join(words)}\"\n",
    "                else:\n",
    "                    topic_names[topic] = f\"Topic {topic}\"\n",
    "            else:\n",
    "                topic_names[topic] = f\"Topic {topic}\"\n",
    "        \n",
    "        # Plot with proper legend\n",
    "        for topic in decade_topic_counts.columns:\n",
    "            if topic in topic_names:  # Only plot if we have a name for this topic\n",
    "                label = topic_names[topic]\n",
    "                plt.plot(decade_topic_pcts.index, decade_topic_pcts[topic], marker='o', linewidth=2, label=label)\n",
    "        \n",
    "        plt.title(f'Topic Trends by Decade for {column_name}')\n",
    "        plt.xlabel('Decade')\n",
    "        plt.ylabel('Topic Prevalence (%)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the matplotlib plot\n",
    "        plt_filename = f\"{column_name}_decade_topic_trends.png\"\n",
    "        \n",
    "        # Define save_plot function\n",
    "        def save_plot(plt_obj, filename, main_dir, sub_dir):\n",
    "            plt_obj.savefig(f\"{main_dir}/{sub_dir}/{filename}\")\n",
    "            plt_obj.close()\n",
    "        \n",
    "        save_plot(plt, plt_filename, subject_main_dir, subject_type)\n",
    "        \n",
    "        # Create a simple HTML to display\n",
    "        html_content = f\"\"\"\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Topic Trends Over Time - {column_name}</title>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Topic Trends Over Time - {column_name}</h1>\n",
    "            <img src=\"{plt_filename}\" alt=\"Topic Trends\" style=\"width:100%;\">\n",
    "            <h2>Topic Descriptions</h2>\n",
    "            <ul>\n",
    "        \"\"\"\n",
    "        \n",
    "        for topic, name in topic_names.items():\n",
    "            words = model.get_topic(topic)\n",
    "            if words:\n",
    "                word_list = \", \".join([f\"{word} ({weight:.3f})\" for word, weight in words[:10]])\n",
    "                html_content += f\"<li><strong>{name}</strong>: {word_list}</li>\\n\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </ul>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save HTML\n",
    "        html_filename = f\"{column_name}_topics_over_time.html\"\n",
    "        with open(f\"{subject_main_dir}/{subject_type}/{html_filename}\", \"w\") as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        return {\n",
    "            \"visualizations\": [\n",
    "                f\"{subject_type}/{html_filename}\",\n",
    "                f\"{subject_type}/{plt_filename}\"\n",
    "            ],\n",
    "            \"type\": \"topics_over_time\",\n",
    "            \"topics\": top_topics\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\"error\": f\"Failed to create topics over time visualization: {str(e)}\\n{traceback.format_exc()}\"}\n",
    "\n",
    "\n",
    "def run_bertopic_analysis(df, column_name, decade_column=\"decade\", subject_main_dir=\"analyze_dataset\", \n",
    "                        language=\"english\", nr_topics=\"auto\", min_topic_size=10, top_n_topics=10):\n",
    "    \"\"\"\n",
    "    Run comprehensive BERTopic analysis on text data with all visualizations\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the text column\n",
    "    column_name (str): Name of the column to analyze\n",
    "    decade_column (str, optional): Name of the column with decade information for trend analysis\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    language (str): Language for stopwords removal and preprocessing\n",
    "    nr_topics (int or str): Number of topics to find ('auto' or specific number)\n",
    "    min_topic_size (int): Minimum size of topics\n",
    "    top_n_topics (int): Number of top topics to visualize\n",
    "    \n",
    "    Returns:\n",
    "    dict: Analysis results\n",
    "    \"\"\"\n",
    "    # Print status\n",
    "    print(f\"Running BERTopic analysis on '{column_name}'...\")\n",
    "    subject_type=column_name.lower()\n",
    "    \n",
    "    try:\n",
    "        # Import required packages\n",
    "        from bertopic import BERTopic\n",
    "        import nltk\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        from pathlib import Path\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Set environment variable to avoid the tokenizers parallelism warning\n",
    "        import os\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "        \n",
    "        # Add necessary imports for visualization\n",
    "        import plotly.graph_objects as go\n",
    "        \n",
    "        # Create subject directory if it doesn't exist\n",
    "        if subject_main_dir:\n",
    "            subject_dir = Path(subject_main_dir) / subject_type\n",
    "            subject_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Set up model directly here for better control\n",
    "        def setup_bertopic(language=\"english\", nr_topics=\"auto\", min_topic_size=10):\n",
    "            try:\n",
    "                # Download stopwords if needed\n",
    "                try:\n",
    "                    nltk.data.find('corpora/stopwords')\n",
    "                except LookupError:\n",
    "                    nltk.download('stopwords')\n",
    "                \n",
    "                # Get stopwords for the specified language\n",
    "                from nltk.corpus import stopwords\n",
    "                stop_words = stopwords.words(language)\n",
    "                \n",
    "                # Set up the vectorizer with stopwords\n",
    "                vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "                \n",
    "                # Create and return the BERTopic model\n",
    "                model = BERTopic(\n",
    "                    language=language,\n",
    "                    nr_topics=nr_topics,\n",
    "                    min_topic_size=min_topic_size,\n",
    "                    vectorizer_model=vectorizer\n",
    "                )\n",
    "                return model\n",
    "            \n",
    "            except ImportError:\n",
    "                print(\"Please install required packages with: pip install bertopic nltk\")\n",
    "                return None\n",
    "        \n",
    "        # Create the model\n",
    "        model = setup_bertopic(language, nr_topics, min_topic_size)\n",
    "        if model is None:\n",
    "            return {\"error\": \"Failed to initialize BERTopic model\"}\n",
    "        \n",
    "        # Extract text data, filtering out non-string and NaN values\n",
    "        processed_df = df.copy()\n",
    "        processed_df = processed_df[processed_df[column_name].apply(\n",
    "            lambda x: isinstance(x, str) and pd.notna(x))]\n",
    "        \n",
    "        if len(processed_df) == 0:\n",
    "            return {\"error\": f\"No valid text data found in column {column_name}\"}\n",
    "        \n",
    "        # Apply specific filtering if needed\n",
    "        if column_name == \"Abstract\":\n",
    "            processed_df = processed_df[processed_df[\"Abstract\"] != \"No abstract available\"]\n",
    "        \n",
    "        # Get clean text data\n",
    "        documents = processed_df[column_name].tolist()\n",
    "        \n",
    "        # Fit the model on our text data\n",
    "        topics, probabilities = model.fit_transform(documents)\n",
    "        \n",
    "        # Add topics to the dataframe\n",
    "        processed_df[\"bertopic_topic\"] = topics\n",
    "        \n",
    "        # Handle probabilities properly\n",
    "        if hasattr(probabilities, 'ndim') and probabilities.ndim == 2:\n",
    "            # If probabilities is a 2D array, get the max value for each row\n",
    "            processed_df[\"bertopic_probability\"] = [prob.max() for prob in probabilities]\n",
    "        elif hasattr(probabilities, 'ndim') and probabilities.ndim == 1:\n",
    "            # If probabilities is a 1D array, use it directly\n",
    "            processed_df[\"bertopic_probability\"] = probabilities\n",
    "        else:\n",
    "            # Handle case where probabilities might be a list of arrays or other structures\n",
    "            processed_df[\"bertopic_probability\"] = [\n",
    "                prob.max() if hasattr(prob, 'max') else \n",
    "                (max(prob) if hasattr(prob, '__len__') and len(prob) > 0 else 0) \n",
    "                for prob in probabilities\n",
    "            ]\n",
    "        \n",
    "        # Get basic topic information\n",
    "        topic_info = model.get_topic_info()\n",
    "        print(f\"Identified {len(topic_info)-1} topics (excluding outlier topic)\")\n",
    "        \n",
    "        # Generate visualizations\n",
    "        results = {\n",
    "            \"column_analyzed\": column_name,\n",
    "            \"topic_count\": len(model.get_topics()),\n",
    "            \"visualizations\": []\n",
    "        }\n",
    "        \n",
    "        # 1. Topic Word Clouds\n",
    "        print(\"Generating topic word clouds...\")\n",
    "        wc_result = visualize_topic_wordclouds(\n",
    "            model=model,\n",
    "            subject_main_dir=subject_main_dir,\n",
    "            subject_type=subject_type,\n",
    "            column_name=column_name,\n",
    "            top_n_topics=top_n_topics\n",
    "        )\n",
    "        if \"error\" not in wc_result:\n",
    "            results[\"visualizations\"].append(wc_result[\"visualization\"])\n",
    "        else:\n",
    "            print(f\"Error generating word clouds: {wc_result['error']}\")\n",
    "        \n",
    "        # 2. Topic Bar Charts\n",
    "        print(\"Generating topic bar charts...\")\n",
    "        bar_result = visualize_topic_barchart(\n",
    "            model=model,\n",
    "            subject_main_dir=subject_main_dir,\n",
    "            subject_type=subject_type,\n",
    "            column_name=column_name,\n",
    "            n_topics=top_n_topics\n",
    "        )\n",
    "        if \"error\" not in bar_result:\n",
    "            results[\"visualizations\"].append(bar_result[\"visualization\"])\n",
    "        else:\n",
    "            print(f\"Error generating bar charts: {bar_result['error']}\")\n",
    "        \n",
    "        # 3. Topic Similarity\n",
    "        print(\"Generating topic similarity heatmap...\")\n",
    "        try:\n",
    "            fig = model.visualize_heatmap()\n",
    "            filename = f\"{column_name}_topic_similarity.html\"\n",
    "            fig.write_html(f\"{subject_main_dir}/{subject_type}/{filename}\")\n",
    "            results[\"visualizations\"].append(f\"{subject_type}/{filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating similarity heatmap: {str(e)}\")\n",
    "        \n",
    "        # 4. Topic Hierarchy\n",
    "        print(\"Generating topic hierarchy visualization...\")\n",
    "        try:\n",
    "            fig = model.visualize_hierarchy()\n",
    "            filename = f\"{column_name}_topic_hierarchy.html\"\n",
    "            fig.write_html(f\"{subject_main_dir}/{subject_type}/{filename}\")\n",
    "            results[\"visualizations\"].append(f\"{subject_type}/{filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating hierarchy visualization: {str(e)}\")\n",
    "        \n",
    "        # 5. Topics over Time (if decade column exists)\n",
    "        if decade_column and decade_column in df.columns:\n",
    "            print(f\"Generating topic trends over decades using '{decade_column}'...\")\n",
    "            time_result = visualize_topics_over_time(\n",
    "                model=model,\n",
    "                processed_df=processed_df,\n",
    "                decade_column=decade_column,\n",
    "                subject_main_dir=subject_main_dir,\n",
    "                subject_type=subject_type,\n",
    "                column_name=column_name,\n",
    "                top_n_topics=top_n_topics\n",
    "            )\n",
    "            if \"error\" not in time_result:\n",
    "                results[\"visualizations\"].extend(time_result[\"visualizations\"])\n",
    "            else:\n",
    "                print(f\"Error generating topics over time: {time_result['error']}\")\n",
    "        \n",
    "        # Save topic info as CSV\n",
    "        if subject_main_dir:\n",
    "            topic_info_path = f\"{subject_main_dir}/{subject_type}/{column_name}_topic_info.csv\"\n",
    "            topic_info.to_csv(topic_info_path, index=False)\n",
    "            results[\"topic_info_path\"] = topic_info_path\n",
    "            print(f\"Topic information saved to {topic_info_path}\")\n",
    "        \n",
    "        # Add topic assignments to results\n",
    "        results[\"document_topics\"] = topics\n",
    "        results[\"document_topic_probs\"] = probabilities\n",
    "        results[\"processed_df\"] = processed_df\n",
    "        results[\"model\"] = model\n",
    "        \n",
    "        print(\"BERTopic analysis completed successfully!\")\n",
    "        return results\n",
    "        \n",
    "    except ImportError as e:\n",
    "        return {\"error\": f\"Required package missing: {str(e)}. Please install with: pip install bertopic nltk scikit-learn plotly matplotlib pandas\"}\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\"error\": f\"BERTopic analysis failed: {str(e)}\\n{traceback.format_exc()}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06b1ed3-7ef7-462c-84fe-902fd8957edc",
   "metadata": {},
   "source": [
    "### BERTopic utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9dbd4-6f33-40fb-b46a-7b45d4eed891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import plotly.graph_objects as go\n",
    "from typing import List, Dict, Tuple, Union, Optional, Any\n",
    "\n",
    "def save_plot(plt, filename, subject_main_dir, subject_type):\n",
    "    \"\"\"\n",
    "    Save the current matplotlib plot to a file in the appropriate subject subdirectory\n",
    "    \n",
    "    Parameters:\n",
    "    plt: matplotlib.pyplot instance\n",
    "    filename (str): Name of the file to save\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis (e.g., 'domains', 'fields', 'topics')\n",
    "    \"\"\"\n",
    "    # Create subject-specific subdirectory\n",
    "    subject_dir = Path(subject_main_dir) / subject_type\n",
    "    subject_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save the plot\n",
    "    save_path = subject_dir / filename\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def save_plotly_figure(fig, filename, subject_main_dir, subject_type):\n",
    "    \"\"\"\n",
    "    Save a Plotly figure to HTML and optionally as a static image\n",
    "    \n",
    "    Parameters:\n",
    "    fig: Plotly figure object\n",
    "    filename (str): Name of the file to save (without extension)\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Paths to HTML and PNG files\n",
    "    \"\"\"\n",
    "    # Create subject-specific subdirectory\n",
    "    subject_dir = Path(subject_main_dir) / subject_type\n",
    "    subject_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save as HTML (interactive)\n",
    "    html_path = subject_dir / f\"{filename}.html\"\n",
    "    fig.write_html(str(html_path))\n",
    "    \n",
    "    # Also save as PNG for static viewing/embedding\n",
    "    png_path = subject_dir / f\"{filename}.png\"\n",
    "    fig.write_image(str(png_path))\n",
    "    \n",
    "    return str(html_path), str(png_path)\n",
    "\n",
    "def get_topic_labels(model, topics, include_top_n_words=3):\n",
    "    \"\"\"\n",
    "    Generate descriptive labels for topics with top words\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    topics (list): List of topic IDs\n",
    "    include_top_n_words (int): Number of top words to include in label\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary mapping topic IDs to descriptive labels\n",
    "    \"\"\"\n",
    "    topic_labels = {}\n",
    "    \n",
    "    for topic in topics:\n",
    "        words = model.get_topic(topic)\n",
    "        if words:\n",
    "            top_words = \", \".join([word for word, _ in words[:include_top_n_words]])\n",
    "            topic_labels[topic] = f\"Topic {topic}: {top_words}\"\n",
    "        else:\n",
    "            topic_labels[topic] = f\"Topic {topic}\"\n",
    "    \n",
    "    return topic_labels\n",
    "\n",
    "def get_top_topics(model, n=10, exclude_outliers=True):\n",
    "    \"\"\"\n",
    "    Get the top N topics by document count\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    n (int): Number of topics to return\n",
    "    exclude_outliers (bool): Whether to exclude the outlier topic (-1)\n",
    "    \n",
    "    Returns:\n",
    "    list: List of top topic IDs\n",
    "    \"\"\"\n",
    "    topic_info = model.get_topic_info()\n",
    "    \n",
    "    if exclude_outliers:\n",
    "        filtered_topics = topic_info[topic_info['Topic'] != -1]\n",
    "    else:\n",
    "        filtered_topics = topic_info\n",
    "    \n",
    "    return filtered_topics.nlargest(n, 'Count')['Topic'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632d94d2-4663-4ae3-9eb6-2c0727e3cd12",
   "metadata": {},
   "source": [
    "### Basic BERTopic visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763ac814-2ac4-4d39-9f13-4c321be09e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.colors as mcolors\n",
    "from pathlib import Path\n",
    "\n",
    "def create_topic_wordcloud(model, topic_id, topic_label=None, colormap='viridis'):\n",
    "    \"\"\"\n",
    "    Create a wordcloud visualization for a specific topic with topic number in the title\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    topic_id (int): ID of the topic to visualize\n",
    "    topic_label (str, optional): Custom label for the topic\n",
    "    colormap (str): Matplotlib colormap to use for the wordcloud\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: The figure containing the wordcloud\n",
    "    \"\"\"\n",
    "    # Get the words and their weights for the topic\n",
    "    words = model.get_topic(topic_id)\n",
    "    if not words:\n",
    "        return None\n",
    "        \n",
    "    # Create a dictionary of word:weight for the wordcloud\n",
    "    word_dict = {word: weight for word, weight in words}\n",
    "    \n",
    "    # Generate the color function based on weights\n",
    "    def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "        weight = word_dict.get(word, 0.5)  # Default to middle weight if word not found\n",
    "        colors = plt.cm.get_cmap(colormap)(np.linspace(0, 1, 100))\n",
    "        color_idx = int(weight * 99)  # Scale weight to 0-99 range\n",
    "        return tuple(int(255 * c) for c in colors[color_idx][:3])\n",
    "    \n",
    "    # Create the wordcloud\n",
    "    wc = WordCloud(\n",
    "        background_color='white',\n",
    "        max_words=50,\n",
    "        width=800,\n",
    "        height=400,\n",
    "        prefer_horizontal=1.0,\n",
    "        color_func=color_func,\n",
    "        max_font_size=100,\n",
    "        random_state=42\n",
    "    ).generate_from_frequencies(word_dict)\n",
    "    \n",
    "    # Create figure and add the wordcloud\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    \n",
    "    # Add title with topic number\n",
    "    if topic_label:\n",
    "        title = topic_label\n",
    "    else:\n",
    "        title = f\"Topic {topic_id}\"\n",
    "        \n",
    "    # Add top 5 words to the title\n",
    "    top_words = \", \".join([word for word, _ in words[:5]])\n",
    "    title = f\"{title}: {top_words}\"\n",
    "    \n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_topic_wordclouds_grid(model, topics=None, rows=3, cols=3, title=\"Topic Wordclouds\", subject_main_dir=None, subject_type=None):\n",
    "    \"\"\"\n",
    "    Create a grid of wordclouds for multiple topics with topic numbers as labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    topics (list, optional): List of topics to visualize\n",
    "    rows (int): Number of rows in the grid\n",
    "    cols (int): Number of columns in the grid\n",
    "    title (str): Title for the visualization\n",
    "    subject_main_dir (str/Path, optional): Directory to save the plot\n",
    "    subject_type (str, optional): Subdirectory type for saving\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: The figure containing the wordcloud grid\n",
    "    \"\"\"\n",
    "    # If topics not specified, use top N non-outlier topics\n",
    "    if topics is None:\n",
    "        topic_info = model.get_topic_info()\n",
    "        filtered_topics = topic_info[topic_info['Topic'] != -1]\n",
    "        n_topics = rows * cols\n",
    "        topics = filtered_topics.nlargest(n_topics, 'Count')['Topic'].tolist()\n",
    "    else:\n",
    "        # Limit to rows*cols topics\n",
    "        topics = topics[:rows*cols]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*4))\n",
    "    fig.suptitle(title, fontsize=20, y=0.98)\n",
    "    \n",
    "    # Flatten axes array for easier iteration\n",
    "    if rows > 1 or cols > 1:\n",
    "        axes_flat = axes.flatten()\n",
    "    else:\n",
    "        axes_flat = [axes]\n",
    "    \n",
    "    # Generate wordclouds for each topic\n",
    "    for i, (ax, topic) in enumerate(zip(axes_flat, topics)):\n",
    "        # Get the words and their weights for the topic\n",
    "        words = model.get_topic(topic)\n",
    "        if not words:\n",
    "            ax.text(0.5, 0.5, f\"No words for Topic {topic}\", \n",
    "                    horizontalalignment='center', verticalalignment='center')\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "            \n",
    "        # Create a dictionary of word:weight for the wordcloud\n",
    "        word_dict = {word: weight for word, weight in words}\n",
    "        \n",
    "        # Create the wordcloud\n",
    "        wc = WordCloud(\n",
    "            background_color='white',\n",
    "            max_words=50,\n",
    "            width=400,\n",
    "            height=400,\n",
    "            prefer_horizontal=1.0,\n",
    "            colormap='viridis',\n",
    "            max_font_size=100,\n",
    "            random_state=42\n",
    "        ).generate_from_frequencies(word_dict)\n",
    "        \n",
    "        # Display the wordcloud\n",
    "        ax.imshow(wc, interpolation='bilinear')\n",
    "        \n",
    "        # Add title with topic number\n",
    "        top_words = \", \".join([word for word, _ in words[:3]])\n",
    "        ax.set_title(f\"Topic {topic}: {top_words}\", fontsize=12)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(len(topics), len(axes_flat)):\n",
    "        axes_flat[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for the main title\n",
    "    \n",
    "    # Save the plot if directory is provided\n",
    "    if subject_main_dir and subject_type:\n",
    "        save_path = Path(subject_main_dir) / subject_type / f\"{title.replace(' ', '_').lower()}.png\"\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_topic_wordclouds(model, subject_main_dir, subject_type, \n",
    "                             column_name, top_n_topics=10):\n",
    "    \"\"\"\n",
    "    Visualize topics as wordclouds with topic numbers as labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    column_name (str): Name of the column analyzed\n",
    "    top_n_topics (int): Number of top topics to visualize\n",
    "    \n",
    "    Returns:\n",
    "    dict: Visualization details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get topic info dataframe\n",
    "        topic_info = model.get_topic_info()\n",
    "        \n",
    "        # Get topics excluding the -1 outlier topic\n",
    "        if -1 in topic_info['Topic'].values:\n",
    "            top_topics = [topic for topic in topic_info['Topic'].tolist() \n",
    "                        if topic != -1][:top_n_topics]\n",
    "        else:\n",
    "            top_topics = topic_info['Topic'].tolist()[:top_n_topics]\n",
    "        \n",
    "        # Create grid of wordclouds\n",
    "        rows = (top_n_topics + 2) // 3  # Ceiling division to get enough rows\n",
    "        grid_fig = create_topic_wordclouds_grid(\n",
    "            model=model,\n",
    "            topics=top_topics,\n",
    "            rows=rows,\n",
    "            cols=3,\n",
    "            title=f\"{column_name} Topic Wordclouds\",\n",
    "            subject_main_dir=subject_main_dir,\n",
    "            subject_type=subject_type\n",
    "        )\n",
    "        \n",
    "        # Save the grid figure\n",
    "        grid_filename = f\"{column_name}_topic_wordclouds_grid.png\"\n",
    "        save_path = Path(subject_main_dir) / subject_type / grid_filename\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        plt.close(grid_fig)\n",
    "        \n",
    "        # Also create interactive wordcloud visualization using BERTopic's built-in method\n",
    "        try:\n",
    "            interactive_fig = model.visualize_topics(topics=top_topics)\n",
    "            html_filename = f\"{column_name}_topic_wordclouds.html\"\n",
    "            html_path = Path(subject_main_dir) / subject_type / html_filename\n",
    "            interactive_fig.write_html(str(html_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not create interactive wordcloud: {str(e)}\")\n",
    "        \n",
    "        return {\n",
    "            \"visualization\": f\"{subject_type}/{grid_filename}\",\n",
    "            \"type\": \"topic_wordclouds\",\n",
    "            \"topics\": top_topics\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\"error\": f\"Failed to create topic wordclouds: {str(e)}\\n{traceback.format_exc()}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f519ea-a367-4970-9e7d-82d64f4f9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import networkx as nx\n",
    "import plotly.express as px\n",
    "from typing import List, Dict, Tuple, Union, Optional, Any\n",
    "\n",
    "def create_topic_hierarchy(model, topics=None, title=\"Topic Hierarchy\"):\n",
    "    \"\"\"\n",
    "    Create a hierarchical visualization of topics with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model \n",
    "    topics (list, optional): List of topics to include\n",
    "    title (str): Title for the visualization\n",
    "    \n",
    "    Returns:\n",
    "    plotly.graph_objects.Figure: Interactive hierarchy figure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use built-in BERTopic visualization if available\n",
    "        fig = model.visualize_hierarchy(topics=topics, title=title)\n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Built-in hierarchy visualization failed: {str(e)}\")\n",
    "        print(\"Falling back to custom implementation...\")\n",
    "        \n",
    "        # If topics not specified, use all non-outlier topics\n",
    "        if topics is None:\n",
    "            topic_info = model.get_topic_info()\n",
    "            topics = [t for t in topic_info['Topic'].tolist() if t != -1]\n",
    "        \n",
    "        # Get topic labels with top words\n",
    "        topic_labels = {}\n",
    "        for topic in topics:\n",
    "            words = model.get_topic(topic)\n",
    "            if words:\n",
    "                top_words = \", \".join([word for word, _ in words[:3]])\n",
    "                topic_labels[topic] = f\"Topic {topic}: {top_words}\"\n",
    "            else:\n",
    "                topic_labels[topic] = f\"Topic {topic}\"\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = 1 - model._calculate_topic_similarities(topics, topics)\n",
    "        \n",
    "        # Perform hierarchical clustering\n",
    "        Z = linkage(similarity_matrix, 'ward')\n",
    "        \n",
    "        # Create dendrogram\n",
    "        fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        dendrogram(\n",
    "            Z,\n",
    "            labels=[topic_labels[t] for t in topics],\n",
    "            orientation='right',\n",
    "            leaf_font_size=10\n",
    "        )\n",
    "        ax.set_title(title, fontsize=16)\n",
    "        ax.set_xlabel('Distance', fontsize=12)\n",
    "        \n",
    "        return fig\n",
    "\n",
    "def create_topic_tree(model, topics=None, title=\"Topic Hierarchical Tree\"):\n",
    "    \"\"\"\n",
    "    Create a tree visualization of topic hierarchy with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    topics (list, optional): List of topics to include\n",
    "    title (str): Title for the visualization\n",
    "    \n",
    "    Returns:\n",
    "    plotly.graph_objects.Figure: Interactive tree figure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use built-in BERTopic visualization if available\n",
    "        fig = model.visualize_hierarchy(topics=topics, title=title)\n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create hierarchical tree: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_topic_network(model, topics=None, threshold=0.5, title=\"Topic Similarity Network\"):\n",
    "    \"\"\"\n",
    "    Create a network visualization of topic relationships with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    topics (list, optional): List of topics to include\n",
    "    threshold (float): Similarity threshold for displaying connections\n",
    "    title (str): Title for the visualization\n",
    "    \n",
    "    Returns:\n",
    "    plotly.graph_objects.Figure: Interactive network figure\n",
    "    \"\"\"\n",
    "    # If topics not specified, use all non-outlier topics\n",
    "    if topics is None:\n",
    "        topic_info = model.get_topic_info()\n",
    "        topics = [t for t in topic_info['Topic'].tolist() if t != -1]\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = 1 - model._calculate_topic_similarities(topics, topics)\n",
    "    \n",
    "    # Create graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for topic in topics:\n",
    "        # Get top words for the topic\n",
    "        words = model.get_topic(topic)\n",
    "        if words:\n",
    "            top_words = \", \".join([word for word, _ in words[:3]])\n",
    "            # Add size based on topic prevalence\n",
    "            size = model.get_topic_info().loc[model.get_topic_info()['Topic'] == topic, 'Count'].values[0]\n",
    "            G.add_node(topic, label=f\"Topic {topic}\", top_words=top_words, size=size)\n",
    "        else:\n",
    "            G.add_node(topic, label=f\"Topic {topic}\", top_words=\"\", size=10)\n",
    "    \n",
    "    # Add edges for topics that are similar above the threshold\n",
    "    for i, topic1 in enumerate(topics):\n",
    "        for j, topic2 in enumerate(topics):\n",
    "            if i < j:  # Only process each pair once\n",
    "                sim = similarity_matrix[i, j]\n",
    "                if sim > threshold:\n",
    "                    G.add_edge(topic1, topic2, weight=sim)\n",
    "    \n",
    "    # Get positions using a spring layout\n",
    "    pos = nx.spring_layout(G, k=0.5, seed=42)\n",
    "    \n",
    "    # Get node sizes based on topic prevalence (normalized)\n",
    "    sizes = [G.nodes[node]['size'] for node in G.nodes()]\n",
    "    max_size = max(sizes) if sizes else 100\n",
    "    sizes = [50 + (s/max_size) * 500 for s in sizes]\n",
    "    \n",
    "    # Create edge traces\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_traces = []\n",
    "    \n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        weight = G.edges[edge]['weight']\n",
    "        \n",
    "        edge_trace = go.Scatter(\n",
    "            x=[x0, x1, None], y=[y0, y1, None],\n",
    "            line=dict(width=weight*5, color='rgba(50,50,50,0.5)'),\n",
    "            hoverinfo='none',\n",
    "            mode='lines')\n",
    "        \n",
    "        edge_traces.append(edge_trace)\n",
    "    \n",
    "    # Create node trace\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "    \n",
    "    # Create a continuous colormap for node colors\n",
    "    node_colors = [node for node in G.nodes()]\n",
    "    \n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers+text',\n",
    "        text=[f\"T{node}\" for node in G.nodes()],\n",
    "        textposition=\"top center\",\n",
    "        textfont=dict(size=10, color='black'),\n",
    "        marker=dict(\n",
    "            showscale=True,\n",
    "            colorscale='Viridis',\n",
    "            color=node_colors,\n",
    "            size=sizes,\n",
    "            line_width=2,\n",
    "            line=dict(color='black', width=2)\n",
    "        ),\n",
    "        hoverinfo='text',\n",
    "        hovertext=[f\"Topic {node}<br>{G.nodes[node]['top_words']}<br>Size: {G.nodes[node]['size']}\" \n",
    "                  for node in G.nodes()]\n",
    "    )\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure(data=edge_traces + [node_trace],\n",
    "                   layout=go.Layout(\n",
    "                       title=title,\n",
    "                       showlegend=False,\n",
    "                       hovermode='closest',\n",
    "                       margin=dict(b=20, l=5, r=5, t=40),\n",
    "                       xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                       yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                       height=800,\n",
    "                       width=800\n",
    "                   ))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_topic_heatmap(model, topics=None, title=\"Topic Similarity Heatmap\"):\n",
    "    \"\"\"\n",
    "    Create a custom heatmap for topic similarity with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    topics (list, optional): List of topics to include in the heatmap\n",
    "    title (str): Title for the heatmap\n",
    "    \n",
    "    Returns:\n",
    "    plotly.graph_objects.Figure: Interactive heatmap figure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use built-in BERTopic visualization if available\n",
    "        fig = model.visualize_heatmap(topics=topics)\n",
    "        fig.update_layout(title=title)\n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Built-in heatmap visualization failed: {str(e)}\")\n",
    "        print(\"Falling back to custom implementation...\")\n",
    "        \n",
    "        # If topics not specified, use all non-outlier topics\n",
    "        if topics is None:\n",
    "            topic_info = model.get_topic_info()\n",
    "            topics = [t for t in topic_info['Topic'].tolist() if t != -1]\n",
    "        \n",
    "        # Calculate similarity matrix between topics\n",
    "        similarity_matrix = 1 - model._calculate_topic_similarities(topics, topics)\n",
    "        \n",
    "        # Create topic labels with top words\n",
    "        topic_labels = {}\n",
    "        for topic in topics:\n",
    "            words = model.get_topic(topic)\n",
    "            if words:\n",
    "                top_words = \", \".join([word for word, _ in words[:3]])\n",
    "                topic_labels[topic] = f\"Topic {topic}: {top_words}\"\n",
    "            else:\n",
    "                topic_labels[topic] = f\"Topic {topic}\"\n",
    "        \n",
    "        # Convert to list for plotly\n",
    "        labels = [topic_labels[t] for t in topics]\n",
    "        \n",
    "        # Create heatmap\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=similarity_matrix,\n",
    "            x=labels,\n",
    "            y=labels,\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            zmin=0, zmax=1\n",
    "        ))\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            xaxis_title=\"Topics\",\n",
    "            yaxis_title=\"Topics\",\n",
    "            height=800,\n",
    "            width=800,\n",
    "            xaxis={'tickangle': 45},\n",
    "            font=dict(size=10)\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "def visualize_hierarchical_documents(model, docs_embeddings=None, docs=None, topics=None,\n",
    "                                    title=\"Hierarchical Document Clustering\"):\n",
    "    \"\"\"\n",
    "    Visualize documents hierarchically with topic coloring\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    docs_embeddings (array, optional): Document embeddings\n",
    "    docs (list, optional): List of documents\n",
    "    topics (list, optional): List of document topics\n",
    "    title (str): Title for the visualization\n",
    "    \n",
    "    Returns:\n",
    "    plotly.graph_objects.Figure: Interactive hierarchy visualization\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use built-in BERTopic visualization if available\n",
    "        # Need to have the topic_model.topic_ attribute containing document-topic mapping\n",
    "        fig = model.visualize_hierarchical_documents(docs=docs, hierarchical_topics=topics, \n",
    "                                                   title=title)\n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create hierarchical document visualization: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "def visualize_document_datamap(model, docs=None, topics=None, embeddings=None, \n",
    "                              title=\"Document Datamap\"):\n",
    "    \"\"\"\n",
    "    Create a 2D map of documents colored by topics with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    docs (list, optional): List of documents\n",
    "    topics (list, optional): List of document topics\n",
    "    embeddings (array, optional): Document embeddings\n",
    "    title (str): Title for the visualization\n",
    "    \n",
    "    Returns:\n",
    "    plotly.graph_objects.Figure: Interactive document map\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use built-in BERTopic visualization if available\n",
    "        fig = model.visualize_documents(docs=docs, topics=topics, embeddings=embeddings,\n",
    "                                     title=title)\n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create document datamap: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176253f-a4c8-4be2-9026-04489676eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Union, Optional, Any\n",
    "\n",
    "def create_topic_barchart(model, topics=None, n_terms=10, title=\"Topic Term Importance\"):\n",
    "    \"\"\"\n",
    "    Create a bar chart visualization of term importance for topics with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    topics (list, optional): List of topics to visualize\n",
    "    n_terms (int): Number of terms to show per topic\n",
    "    title (str): Title for the visualization\n",
    "    \n",
    "    Returns:\n",
    "    plotly.graph_objects.Figure: Interactive bar chart\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use built-in BERTopic visualization if available\n",
    "        if topics is None:\n",
    "            # Get top topics excluding outliers\n",
    "            topic_info = model.get_topic_info()\n",
    "            topics = [t for t in topic_info['Topic'].tolist() if t != -1][:5]  # Limit to 5 topics by default\n",
    "            \n",
    "        fig = model.visualize_barchart(topics=topics, top_n_topics=len(topics), n_words=n_terms)\n",
    "        fig.update_layout(title=title)\n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Built-in barchart visualization failed: {str(e)}\")\n",
    "        print(\"Falling back to custom implementation...\")\n",
    "        \n",
    "        # If topics not specified, use top 5 non-outlier topics\n",
    "        if topics is None:\n",
    "            topic_info = model.get_topic_info()\n",
    "            filtered_topics = topic_info[topic_info['Topic'] != -1]\n",
    "            topics = filtered_topics.nlargest(5, 'Count')['Topic'].tolist()\n",
    "        \n",
    "        # Create subplots, 1 per topic\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Process each topic\n",
    "        for topic in topics:\n",
    "            words = model.get_topic(topic)\n",
    "            if not words:\n",
    "                continue\n",
    "                \n",
    "            # Extract words and scores\n",
    "            terms = [word for word, _ in words[:n_terms]]\n",
    "            scores = [score for _, score in words[:n_terms]]\n",
    "            \n",
    "            # Reverse lists for better visualization (largest on top)\n",
    "            terms.reverse()\n",
    "            scores.reverse()\n",
    "            \n",
    "            # Add trace for this topic\n",
    "            fig.add_trace(go.Bar(\n",
    "                y=terms,\n",
    "                x=scores,\n",
    "                name=f\"Topic {topic}\",\n",
    "                orientation='h'\n",
    "            ))\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            xaxis_title=\"Term Score\",\n",
    "            yaxis_title=\"Terms\",\n",
    "            height=600,\n",
    "            width=1000,\n",
    "            font=dict(size=12),\n",
    "            barmode='group'\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "def visualize_topic_barchart(model, subject_main_dir, subject_type, column_name, n_topics=10):\n",
    "    \"\"\"\n",
    "    Visualize top terms for each topic as bar charts with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    column_name (str): Name of the column analyzed\n",
    "    n_topics (int): Number of topics to show\n",
    "    \n",
    "    Returns:\n",
    "    dict: Visualization details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get topic info dataframe\n",
    "        topic_info = model.get_topic_info()\n",
    "        \n",
    "        # Exclude -1 topic (outliers)\n",
    "        topics = [topic for topic in topic_info['Topic'].tolist() \n",
    "                 if topic != -1][:n_topics]\n",
    "        \n",
    "        # Create visualization\n",
    "        fig = create_topic_barchart(\n",
    "            model=model, \n",
    "            topics=topics, \n",
    "            n_terms=15,  # Show more terms\n",
    "            title=f\"{column_name} Top Terms by Topic\"\n",
    "        )\n",
    "        \n",
    "        # Save the visualization\n",
    "        from pathlib import Path\n",
    "        Path(f\"{subject_main_dir}/{subject_type}\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        filename = f\"{column_name}_topic_barchart.html\"\n",
    "        fig.write_html(f\"{subject_main_dir}/{subject_type}/{filename}\")\n",
    "        \n",
    "        # Also save as PNG for embedding\n",
    "        png_filename = f\"{column_name}_topic_barchart.png\"\n",
    "        fig.write_image(f\"{subject_main_dir}/{subject_type}/{png_filename}\")\n",
    "        \n",
    "        return {\n",
    "            \"visualization\": f\"{subject_type}/{filename}\",\n",
    "            \"static_image\": f\"{subject_type}/{png_filename}\",\n",
    "            \"type\": \"topic_barchart\",\n",
    "            \"topics\": topics\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\"error\": f\"Failed to create topic barchart: {str(e)}\\n{traceback.format_exc()}\"}\n",
    "\n",
    "def create_topic_term_scores(model, topics=None, n_terms=10, title=\"Topic Term Scores\"):\n",
    "    \"\"\"\n",
    "    Visualize the importance scores of terms for each topic with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    topics (list, optional): List of topics to visualize\n",
    "    n_terms (int): Number of terms to show per topic\n",
    "    title (str): Title for the visualization\n",
    "    \n",
    "    Returns:\n",
    "    plotly.graph_objects.Figure: Interactive bar chart figure\n",
    "    \"\"\"\n",
    "    # If topics not specified, use top 5 non-outlier topics\n",
    "    if topics is None:\n",
    "        topic_info = model.get_topic_info()\n",
    "        filtered_topics = topic_info[topic_info['Topic'] != -1]\n",
    "        topics = filtered_topics.nlargest(5, 'Count')['Topic'].tolist()\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Process each topic\n",
    "    for topic in topics:\n",
    "        words = model.get_topic(topic)\n",
    "        if not words:\n",
    "            continue\n",
    "            \n",
    "        # Extract words and scores\n",
    "        terms = [word for word, _ in words[:n_terms]]\n",
    "        scores = [score for _, score in words[:n_terms]]\n",
    "        \n",
    "        # Reverse lists for better visualization (largest on top)\n",
    "        terms.reverse()\n",
    "        scores.reverse()\n",
    "        \n",
    "        # Add trace for this topic\n",
    "        fig.add_trace(go.Bar(\n",
    "            y=terms,\n",
    "            x=scores,\n",
    "            name=f\"Topic {topic}\",\n",
    "            orientation='h'\n",
    "        ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Term Score\",\n",
    "        yaxis_title=\"Terms\",\n",
    "        height=600,\n",
    "        width=1000,\n",
    "        font=dict(size=12),\n",
    "        barmode='group'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_term_rank(model, subject_main_dir, subject_type, column_name, n_topics=5, n_terms=10):\n",
    "    \"\"\"\n",
    "    Visualize the term score decline chart for topics with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    column_name (str): Name of the column analyzed\n",
    "    n_topics (int): Number of topics to visualize\n",
    "    n_terms (int): Number of terms to show per topic\n",
    "    \n",
    "    Returns:\n",
    "    dict: Visualization details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get topic info dataframe\n",
    "        topic_info = model.get_topic_info()\n",
    "        \n",
    "        # Exclude -1 topic (outliers) and get top N topics\n",
    "        topics = [topic for topic in topic_info['Topic'].tolist() \n",
    "                 if topic != -1][:n_topics]\n",
    "        \n",
    "        # Create figure\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Process each topic\n",
    "        for topic in topics:\n",
    "            words = model.get_topic(topic)\n",
    "            if not words:\n",
    "                continue\n",
    "                \n",
    "            # Get scores\n",
    "            scores = [score for _, score in words[:n_terms]]\n",
    "            \n",
    "            # Add line plot for this topic\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=list(range(1, len(scores) + 1)),\n",
    "                y=scores,\n",
    "                mode='lines+markers',\n",
    "                name=f\"Topic {topic}\"\n",
    "            ))\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f\"{column_name} Term Score Decline by Topic\",\n",
    "            xaxis_title=\"Term Rank\",\n",
    "            yaxis_title=\"Term Score\",\n",
    "            height=600,\n",
    "            width=900,\n",
    "            font=dict(size=12),\n",
    "            legend_title=\"Topics\"\n",
    "        )\n",
    "        \n",
    "        # Save the visualization\n",
    "        Path(f\"{subject_main_dir}/{subject_type}\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        filename = f\"{column_name}_term_rank.html\"\n",
    "        fig.write_html(f\"{subject_main_dir}/{subject_type}/{filename}\")\n",
    "        \n",
    "        # Also save as PNG for embedding\n",
    "        png_filename = f\"{column_name}_term_rank.png\"\n",
    "        fig.write_image(f\"{subject_main_dir}/{subject_type}/{png_filename}\")\n",
    "        \n",
    "        return {\n",
    "            \"visualization\": f\"{subject_type}/{filename}\",\n",
    "            \"static_image\": f\"{subject_type}/{png_filename}\",\n",
    "            \"type\": \"term_rank\",\n",
    "            \"topics\": topics\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\"error\": f\"Failed to create term rank visualization: {str(e)}\\n{traceback.format_exc()}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5428c6-82ee-4252-aed8-b22881a71a65",
   "metadata": {},
   "source": [
    "### Advanced analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a1a590-5055-4760-b287-0660ae6580dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Union, Optional, Any\n",
    "\n",
    "def create_topic_evolution(model, df, topic_column, time_column, interval='year',\n",
    "                         topics=None, top_n=10, title=\"Topic Evolution Over Time\"):\n",
    "    \"\"\"\n",
    "    Visualize how topics evolve over time with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    df (DataFrame): DataFrame with topics assigned\n",
    "    topic_column (str): Column name containing the assigned topics\n",
    "    time_column (str): Column name containing the time information\n",
    "    interval (str): Time interval for grouping ('year', 'month', 'day', 'decade')\n",
    "    topics (list, optional): List of topics to include\n",
    "    top_n (int): Number of top topics to show\n",
    "    title (str): Title for the visualization\n",
    "    \n",
    "    Returns:\n",
    "    plotly.graph_objects.Figure: Interactive time series figure\n",
    "    \"\"\"\n",
    "    # Ensure the time column is in datetime format\n",
    "    if not pd.api.types.is_datetime64_dtype(df[time_column]):\n",
    "        try:\n",
    "            df[time_column] = pd.to_datetime(df[time_column])\n",
    "        except:\n",
    "            # If can't convert to datetime, try treating as year values\n",
    "            try:\n",
    "                df[time_column] = pd.to_datetime(df[time_column].astype(str) + '-01-01')\n",
    "            except:\n",
    "                raise ValueError(f\"Could not convert {time_column} to datetime format\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Add interval column for grouping\n",
    "    if interval == 'year':\n",
    "        data['interval'] = data[time_column].dt.year\n",
    "    elif interval == 'month':\n",
    "        data['interval'] = data[time_column].dt.to_period('M').astype(str)\n",
    "    elif interval == 'day':\n",
    "        data['interval'] = data[time_column].dt.date\n",
    "    elif interval == 'decade':\n",
    "        data['interval'] = (data[time_column].dt.year // 10) * 10\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported interval: {interval}\")\n",
    "    \n",
    "    # Get topic labels with top words\n",
    "    topic_info = model.get_topic_info()\n",
    "    \n",
    "    # If topics not specified, get top N non-outlier topics by count\n",
    "    if topics is None:\n",
    "        # Filter out outlier topic (-1)\n",
    "        filtered_topics = topic_info[topic_info['Topic'] != -1]\n",
    "        # Get top N topics by count\n",
    "        topics = filtered_topics.nlargest(top_n, 'Count')['Topic'].tolist()\n",
    "    \n",
    "    # Create topic labels with top words\n",
    "    topic_labels = {}\n",
    "    for topic in topics:\n",
    "        words = model.get_topic(topic)\n",
    "        if words:\n",
    "            top_words = \", \".join([word for word, _ in words[:3]])\n",
    "            topic_labels[topic] = f\"Topic {topic}: {top_words}\"\n",
    "        else:\n",
    "            topic_labels[topic] = f\"Topic {topic}\"\n",
    "    \n",
    "    # Group by interval and topic, count documents\n",
    "    topic_counts = data.groupby(['interval', topic_column]).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Keep only the selected topics, if they exist in the data\n",
    "    available_topics = [t for t in topics if t in topic_counts.columns]\n",
    "    if available_topics:\n",
    "        topic_counts = topic_counts[available_topics]\n",
    "    \n",
    "    # Calculate the topic proportion for each interval\n",
    "    topic_props = topic_counts.div(topic_counts.sum(axis=1), axis=0) * 100\n",
    "    \n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add a line for each topic\n",
    "    for topic in topic_props.columns:\n",
    "        if topic in topic_labels:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=topic_props.index,\n",
    "                y=topic_props[topic],\n",
    "                mode='lines+markers',\n",
    "                name=topic_labels[topic],\n",
    "                line=dict(width=2),\n",
    "                marker=dict(size=8)\n",
    "            ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=f\"Time ({interval.capitalize()})\",\n",
    "        yaxis_title=\"Topic Proportion (%)\",\n",
    "        legend_title=\"Topics\",\n",
    "        hovermode=\"x unified\",\n",
    "        height=600,\n",
    "        width=1000,\n",
    "        font=dict(size=12)\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_topics_over_time(model, processed_df, decade_column, subject_main_dir, \n",
    "                             subject_type, column_name, top_n_topics=10, interval='decade'):\n",
    "    \"\"\"\n",
    "    Visualize topic trends over time with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model (BERTopic): Fitted BERTopic model\n",
    "    processed_df (DataFrame): DataFrame with assigned topics\n",
    "    decade_column (str): Name of the column with time information\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    column_name (str): Name of the column analyzed\n",
    "    top_n_topics (int): Number of top topics to visualize\n",
    "    interval (str): Time interval for grouping ('year', 'month', 'day', 'decade')\n",
    "    \n",
    "    Returns:\n",
    "    dict: Visualization details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure decade column exists\n",
    "        if decade_column not in processed_df.columns:\n",
    "            return {\"error\": f\"Time column '{decade_column}' not found in DataFrame\"}\n",
    "        \n",
    "        # Create the topic evolution visualization\n",
    "        fig = create_topic_evolution(\n",
    "            model=model,\n",
    "            df=processed_df,\n",
    "            topic_column='bertopic_topic',  # Column with assigned topics\n",
    "            time_column=decade_column,\n",
    "            interval=interval,\n",
    "            top_n=top_n_topics,\n",
    "            title=f\"Topic Evolution for {column_name} Over Time\"\n",
    "        )\n",
    "        \n",
    "        # Save the visualization\n",
    "        Path(f\"{subject_main_dir}/{subject_type}\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        html_filename = f\"{column_name}_topics_over_time.html\"\n",
    "        fig.write_html(f\"{subject_main_dir}/{subject_type}/{html_filename}\")\n",
    "        \n",
    "        # Also save as PNG for static viewing/embedding\n",
    "        png_filename = f\"{column_name}_topics_over_time.png\"\n",
    "        fig.write_image(f\"{subject_main_dir}/{subject_type}/{png_filename}\")\n",
    "        \n",
    "        # Also create a simpler matplotlib version for better embedding in reports\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Group data by time and topic\n",
    "        topic_counts = processed_df.groupby([decade_column, 'bertopic_topic']).size().unstack(fill_value=0)\n",
    "        \n",
    "        # Get top topics\n",
    "        topic_info = model.get_topic_info()\n",
    "        filtered_topics = topic_info[topic_info['Topic'] != -1]\n",
    "        top_topics = filtered_topics.nlargest(top_n_topics, 'Count')['Topic'].tolist()\n",
    "        \n",
    "        # Filter to available top topics\n",
    "        available_topics = [t for t in top_topics if t in topic_counts.columns]\n",
    "        if available_topics:\n",
    "            topic_counts = topic_counts[available_topics]\n",
    "        \n",
    "        # Calculate proportions\n",
    "        topic_props = topic_counts.div(topic_counts.sum(axis=1), axis=0) * 100\n",
    "        \n",
    "        # Create topic labels\n",
    "        topic_labels = {}\n",
    "        for topic in available_topics:\n",
    "            words = model.get_topic(topic)\n",
    "            if words:\n",
    "                top_words = \", \".join([word for word, _ in words[:3]])\n",
    "                topic_labels[topic] = f\"Topic {topic}: {top_words}\"\n",
    "            else:\n",
    "                topic_labels[topic] = f\"Topic {topic}\"\n",
    "        \n",
    "        # Plot each topic\n",
    "        for topic in topic_props.columns:\n",
    "            if topic in topic_labels:\n",
    "                plt.plot(topic_props.index, topic_props[topic], marker='o', linewidth=2, label=topic_labels[topic])\n",
    "        \n",
    "        plt.title(f\"Topic Trends for {column_name}\", fontsize=14)\n",
    "        plt.xlabel(decade_column, fontsize=12)\n",
    "        plt.ylabel(\"Topic Proportion (%)\", fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(loc='best')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the matplotlib version\n",
    "        mpl_filename = f\"{column_name}_topics_over_time_mpl.png\"\n",
    "        plt.savefig(f\"{subject_main_dir}/{subject_type}/{mpl_filename}\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        return {\n",
    "            \"visualizations\": [\n",
    "                f\"{subject_type}/{html_filename}\",\n",
    "                f\"{subject_type}/{png_filename}\",\n",
    "                f\"{subject_type}/{mpl_filename}\"\n",
    "            ],\n",
    "            \"type\": \"topics_over_time\",\n",
    "            \"topics\": available_topics\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\"error\": f\"Failed to create topics over time visualization: {str(e)}\\n{traceback.format_exc()}\"}\n",
    "\n",
    "def create_topic_distribution_over_time(model, df, topic_column, time_column, interval='year',\n",
    "                                    top_n_topics=5, title=\"Topic Distribution Over Time\"):\n",
    "    \"\"\"\n",
    "    Create a stacked area chart showing topic distribution over time with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    df (DataFrame): DataFrame with topics assigned\n",
    "    topic_column (str): Column name containing the assigned topics\n",
    "    time_column (str): Column name containing the time information\n",
    "    interval (str): Time interval for grouping ('year', 'month', 'day', 'decade')\n",
    "    top_n_topics (int): Number of top topics to show\n",
    "    title (str): Title for the visualization\n",
    "    \n",
    "    Returns:\n",
    "    plotly.graph_objects.Figure: Interactive stacked area chart\n",
    "    \"\"\"\n",
    "    # Ensure the time column is in datetime format\n",
    "    if not pd.api.types.is_datetime64_dtype(df[time_column]):\n",
    "        try:\n",
    "            df[time_column] = pd.to_datetime(df[time_column])\n",
    "        except:\n",
    "            # Try treating as year values\n",
    "            try:\n",
    "                df[time_column] = pd.to_datetime(df[time_column].astype(str) + '-01-01')\n",
    "            except:\n",
    "                raise ValueError(f\"Could not convert {time_column} to datetime format\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Add interval column for grouping\n",
    "    if interval == 'year':\n",
    "        data['interval'] = data[time_column].dt.year\n",
    "    elif interval == 'month':\n",
    "        data['interval'] = data[time_column].dt.to_period('M').astype(str)\n",
    "    elif interval == 'day':\n",
    "        data['interval'] = data[time_column].dt.date\n",
    "    elif interval == 'decade':\n",
    "        data['interval'] = (data[time_column].dt.year // 10) * 10\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported interval: {interval}\")\n",
    "    \n",
    "    # Get top N topics by count (excluding outliers)\n",
    "    topic_counts = data[topic_column].value_counts()\n",
    "    top_topics = [t for t in topic_counts.index if t != -1][:top_n_topics]\n",
    "    \n",
    "    # Create an \"Other\" category for all other topics\n",
    "    data['topic_group'] = data[topic_column].apply(\n",
    "        lambda x: x if x in top_topics else 'Other')\n",
    "    \n",
    "    # Group by interval and topic, count documents\n",
    "    grouped = data.groupby(['interval', 'topic_group']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Calculate proportions\n",
    "    props = grouped.div(grouped.sum(axis=1), axis=0) * 100\n",
    "    \n",
    "    # Get topic labels with top words\n",
    "    topic_labels = {}\n",
    "    for topic in top_topics:\n",
    "        words = model.get_topic(topic)\n",
    "        if words:\n",
    "            top_words = \", \".join([word for word, _ in words[:3]])\n",
    "            topic_labels[topic] = f\"Topic {topic}: {top_words}\"\n",
    "        else:\n",
    "            topic_labels[topic] = f\"Topic {topic}\"\n",
    "    \n",
    "    topic_labels['Other'] = \"Other Topics\"\n",
    "    \n",
    "    # Create stacked area chart\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add area for each topic\n",
    "    for topic in props.columns:\n",
    "        if topic in topic_labels:\n",
    "            name = topic_labels.get(topic, f\"Topic {topic}\")\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=props.index,\n",
    "                y=props[topic],\n",
    "                mode='lines',\n",
    "                stackgroup='one',\n",
    "                name=name\n",
    "            ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=f\"Time ({interval.capitalize()})\",\n",
    "        yaxis_title=\"Topic Proportion (%)\",\n",
    "        legend_title=\"Topics\",\n",
    "        hovermode=\"x unified\",\n",
    "        height=600,\n",
    "        width=1000,\n",
    "        font=dict(size=12)\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_topic_distribution_over_time(model, processed_df, time_column, subject_main_dir,\n",
    "                                        subject_type, column_name, top_n_topics=5, interval='decade'):\n",
    "    \"\"\"\n",
    "    Visualize topic distribution as a stacked area chart over time with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model (BERTopic): Fitted BERTopic model\n",
    "    processed_df (DataFrame): DataFrame with assigned topics\n",
    "    time_column (str): Name of the column with time information\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    column_name (str): Name of the column analyzed\n",
    "    top_n_topics (int): Number of top topics to visualize\n",
    "    interval (str): Time interval for grouping ('year', 'month', 'day', 'decade')\n",
    "    \n",
    "    Returns:\n",
    "    dict: Visualization details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure time column exists\n",
    "        if time_column not in processed_df.columns:\n",
    "            return {\"error\": f\"Time column '{time_column}' not found in DataFrame\"}\n",
    "        \n",
    "        # Create the stacked area chart\n",
    "        fig = create_topic_distribution_over_time(\n",
    "            model=model,\n",
    "            df=processed_df,\n",
    "            topic_column='bertopic_topic',  # Column with assigned topics\n",
    "            time_column=time_column,\n",
    "            interval=interval,\n",
    "            top_n_topics=top_n_topics,\n",
    "            title=f\"Topic Distribution for {column_name} Over Time\"\n",
    "        )\n",
    "        \n",
    "        # Save the visualization\n",
    "        Path(f\"{subject_main_dir}/{subject_type}\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        html_filename = f\"{column_name}_topic_distribution_over_time.html\"\n",
    "        fig.write_html(f\"{subject_main_dir}/{subject_type}/{html_filename}\")\n",
    "        \n",
    "        # Also save as PNG for static viewing/embedding\n",
    "        png_filename = f\"{column_name}_topic_distribution_over_time.png\"\n",
    "        fig.write_image(f\"{subject_main_dir}/{subject_type}/{png_filename}\")\n",
    "        \n",
    "        return {\n",
    "            \"visualizations\": [\n",
    "                f\"{subject_type}/{html_filename}\",\n",
    "                f\"{subject_type}/{png_filename}\"\n",
    "            ],\n",
    "            \"type\": \"topic_distribution_over_time\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\"error\": f\"Failed to create topic distribution over time: {str(e)}\\n{traceback.format_exc()}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9057e-87c9-4caf-b846-227cbf78fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Union, Optional, Any\n",
    "import umap\n",
    "\n",
    "def create_topic_distribution(model, topics, probabilities, title=\"Topic Probability Distribution\"):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of topic probabilities with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    topics (list): List of assigned topics for each document\n",
    "    probabilities (list/array): Topic probabilities for each document\n",
    "    title (str): Title for the visualization\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: The figure containing the distribution plot\n",
    "    \"\"\"\n",
    "    # Convert topics and probabilities to dataframe\n",
    "    df = pd.DataFrame({'topic': topics})\n",
    "    \n",
    "    # Count documents per topic\n",
    "    topic_counts = df['topic'].value_counts().sort_index()\n",
    "    \n",
    "    # Get probabilities per topic\n",
    "    topic_probs = {}\n",
    "    \n",
    "    # Handle different probability formats\n",
    "    if hasattr(probabilities, 'ndim') and probabilities.ndim == 2:\n",
    "        # 2D array of probabilities\n",
    "        for i, topic in enumerate(topics):\n",
    "            if topic not in topic_probs:\n",
    "                topic_probs[topic] = []\n",
    "            topic_probs[topic].append(probabilities[i].max())\n",
    "    else:\n",
    "        # List of probabilities or 1D array\n",
    "        for i, topic in enumerate(topics):\n",
    "            if topic not in topic_probs:\n",
    "                topic_probs[topic] = []\n",
    "            \n",
    "            if hasattr(probabilities[i], 'max'):\n",
    "                # If it's an array with a max method\n",
    "                topic_probs[topic].append(probabilities[i].max())\n",
    "            elif hasattr(probabilities[i], '__iter__'):\n",
    "                # If it's iterable, find max\n",
    "                topic_probs[topic].append(max(probabilities[i]) if len(probabilities[i]) > 0 else 0)\n",
    "            else:\n",
    "                # Just use the value\n",
    "                topic_probs[topic].append(probabilities[i])\n",
    "    \n",
    "    # Calculate mean probability per topic\n",
    "    topic_mean_probs = {topic: np.mean(probs) for topic, probs in topic_probs.items() if len(probs) > 0}\n",
    "    \n",
    "    # Create plot with two subplots side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Get topic labels with top words\n",
    "    topic_labels = {}\n",
    "    for topic in topic_counts.index:\n",
    "        if topic == -1:\n",
    "            topic_labels[topic] = \"Topic -1 (Outliers)\"\n",
    "            continue\n",
    "            \n",
    "        words = model.get_topic(topic)\n",
    "        if words:\n",
    "            top_words = \", \".join([word for word, _ in words[:3]])\n",
    "            topic_labels[topic] = f\"T{topic}: {top_words}\"\n",
    "        else:\n",
    "            topic_labels[topic] = f\"Topic {topic}\"\n",
    "    \n",
    "    # Plot topic counts\n",
    "    topic_df = pd.DataFrame({\n",
    "        'Topic': [topic_labels.get(t, f\"Topic {t}\") for t in topic_counts.index],\n",
    "        'Count': topic_counts.values,\n",
    "        'RawTopic': topic_counts.index\n",
    "    })\n",
    "    topic_df = topic_df.sort_values('Count', ascending=False)\n",
    "    \n",
    "    # For top 20 topics only\n",
    "    top_topics_df = topic_df.head(20)\n",
    "    \n",
    "    # Plot with shorter topic labels for readability\n",
    "    ax1.bar(range(len(top_topics_df)), top_topics_df['Count'], \n",
    "           tick_label=[f\"T{t}\" for t in top_topics_df['RawTopic']])\n",
    "    ax1.set_title('Top 20 Topics by Document Count')\n",
    "    ax1.set_xlabel('Topic')\n",
    "    ax1.set_ylabel('Document Count')\n",
    "    ax1.tick_params(axis='x', rotation=90)\n",
    "    \n",
    "    # Plot mean probabilities\n",
    "    if topic_mean_probs:\n",
    "        prob_df = pd.DataFrame({\n",
    "            'Topic': [topic_labels.get(t, f\"Topic {t}\") for t in topic_mean_probs.keys()],\n",
    "            'Mean Probability': list(topic_mean_probs.values()),\n",
    "            'RawTopic': list(topic_mean_probs.keys())\n",
    "        })\n",
    "        prob_df = prob_df.sort_values('Mean Probability', ascending=False)\n",
    "        \n",
    "        # For top 20 topics only\n",
    "        top_prob_df = prob_df.head(20)\n",
    "        \n",
    "        # Plot with shorter topic labels for readability\n",
    "        ax2.bar(range(len(top_prob_df)), top_prob_df['Mean Probability'],\n",
    "               tick_label=[f\"T{t}\" for t in top_prob_df['RawTopic']])\n",
    "        ax2.set_title('Top 20 Topics by Mean Probability')\n",
    "        ax2.set_xlabel('Topic')\n",
    "        ax2.set_ylabel('Mean Probability')\n",
    "        ax2.tick_params(axis='x', rotation=90)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=16, y=1.05)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_topic_distribution(model, topics, probabilities, subject_main_dir, subject_type, column_name):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of topics and their probabilities with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    topics (list): List of assigned topics for each document\n",
    "    probabilities (list/array): Topic probabilities for each document\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    column_name (str): Name of the column analyzed\n",
    "    \n",
    "    Returns:\n",
    "    dict: Visualization details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create the distribution plot\n",
    "        fig = create_topic_distribution(\n",
    "            model=model,\n",
    "            topics=topics,\n",
    "            probabilities=probabilities,\n",
    "            title=f\"{column_name} Topic Distribution\"\n",
    "        )\n",
    "        \n",
    "        # Save the plot\n",
    "        Path(f\"{subject_main_dir}/{subject_type}\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        filename = f\"{column_name}_topic_distribution.png\"\n",
    "        plt.savefig(f\"{subject_main_dir}/{subject_type}/{filename}\", bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Also create a more interactive visualization with Plotly\n",
    "        try:\n",
    "            # Count documents per topic\n",
    "            topic_df = pd.DataFrame({'topic': topics})\n",
    "            topic_counts = topic_df['topic'].value_counts().sort_index()\n",
    "            \n",
    "            # Get topic labels\n",
    "            topic_labels = {}\n",
    "            for topic in topic_counts.index:\n",
    "                if topic == -1:\n",
    "                    topic_labels[topic] = \"Topic -1 (Outliers)\"\n",
    "                    continue\n",
    "                    \n",
    "                words = model.get_topic(topic)\n",
    "                if words:\n",
    "                    top_words = \", \".join([word for word, _ in words[:3]])\n",
    "                    topic_labels[topic] = f\"Topic {topic}: {top_words}\"\n",
    "                else:\n",
    "                    topic_labels[topic] = f\"Topic {topic}\"\n",
    "            \n",
    "            # Create interactive bar chart\n",
    "            plotly_df = pd.DataFrame({\n",
    "                'Topic': [topic_labels.get(t, f\"Topic {t}\") for t in topic_counts.index],\n",
    "                'Count': topic_counts.values,\n",
    "                'RawTopic': topic_counts.index\n",
    "            })\n",
    "            plotly_df = plotly_df.sort_values('Count', ascending=False)\n",
    "            \n",
    "            # Limit to top 30 topics for readability\n",
    "            plotly_df = plotly_df.head(30)\n",
    "            \n",
    "            fig_plotly = px.bar(\n",
    "                plotly_df, \n",
    "                x='RawTopic', \n",
    "                y='Count', \n",
    "                hover_data=['Topic'],\n",
    "                labels={'Count': 'Document Count', 'RawTopic': 'Topic ID'},\n",
    "                title=f\"{column_name} Topic Distribution (Top 30 Topics)\",\n",
    "                color='Count',\n",
    "                color_continuous_scale='viridis'\n",
    "            )\n",
    "            \n",
    "            # Update layout\n",
    "            fig_plotly.update_layout(\n",
    "                xaxis_title=\"Topic ID\",\n",
    "                yaxis_title=\"Document Count\",\n",
    "                height=600,\n",
    "                width=1000,\n",
    "                font=dict(size=12)\n",
    "            )\n",
    "            \n",
    "            # Save as HTML\n",
    "            html_filename = f\"{column_name}_topic_distribution.html\"\n",
    "            fig_plotly.write_html(f\"{subject_main_dir}/{subject_type}/{html_filename}\")\n",
    "            \n",
    "            # Also save as PNG\n",
    "            plotly_png_filename = f\"{column_name}_topic_distribution_plotly.png\"\n",
    "            fig_plotly.write_image(f\"{subject_main_dir}/{subject_type}/{plotly_png_filename}\")\n",
    "            \n",
    "            return {\n",
    "                \"visualizations\": [\n",
    "                    f\"{subject_type}/{filename}\",\n",
    "                    f\"{subject_type}/{html_filename}\",\n",
    "                    f\"{subject_type}/{plotly_png_filename}\"\n",
    "                ],\n",
    "                \"type\": \"topic_distribution\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not create interactive visualization: {str(e)}\")\n",
    "            return {\n",
    "                \"visualization\": f\"{subject_type}/{filename}\",\n",
    "                \"type\": \"topic_distribution\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\"error\": f\"Failed to create topic distribution: {str(e)}\\n{traceback.format_exc()}\"}\n",
    "\n",
    "def create_document_topic_map(model, embeddings, topics, probabilities=None, top_n=500, \n",
    "                             sample_method='random', title=\"Document-Topic Map\"):\n",
    "    \"\"\"\n",
    "    Create an interactive scatter plot of documents colored by their topics with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    embeddings (array): Document embeddings\n",
    "    topics (list): Topic assignments\n",
    "    probabilities (list/array, optional): Topic probabilities\n",
    "    top_n (int): Number of documents to visualize\n",
    "    sample_method (str): Method to sample documents ('random', 'probability')\n",
    "    title (str): Title for the visualization\n",
    "    \n",
    "    Returns:\n",
    "    plotly.graph_objects.Figure: Interactive scatter plot\n",
    "    \"\"\"\n",
    "    # Convert inputs to numpy arrays if they aren't already\n",
    "    embeddings = np.array(embeddings)\n",
    "    topics = np.array(topics)\n",
    "    \n",
    "    # Determine document sample based on method\n",
    "    if len(embeddings) > top_n:\n",
    "        if sample_method == 'random':\n",
    "            # Random sampling\n",
    "            indices = np.random.choice(len(embeddings), top_n, replace=False)\n",
    "        elif sample_method == 'probability' and probabilities is not None:\n",
    "            # Sample based on highest probability\n",
    "            if hasattr(probabilities, 'ndim') and probabilities.ndim == 2:\n",
    "                # 2D array of probabilities\n",
    "                probs = np.array([prob.max() for prob in probabilities])\n",
    "            else:\n",
    "                # Convert to array if it's a list\n",
    "                probs = np.array([\n",
    "                    prob.max() if hasattr(prob, 'max') else \n",
    "                    (max(prob) if hasattr(prob, '__iter__') and len(prob) > 0 else prob) \n",
    "                    for prob in probabilities\n",
    "                ])\n",
    "            \n",
    "            # Get indices of highest probability documents\n",
    "            indices = np.argsort(probs)[-top_n:]\n",
    "        else:\n",
    "            # Default to random\n",
    "            indices = np.random.choice(len(embeddings), top_n, replace=False)\n",
    "            \n",
    "        # Apply sampling\n",
    "        embeddings = embeddings[indices]\n",
    "        topics = topics[indices]\n",
    "        \n",
    "    # Reduce dimensionality to 2D for visualization\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, metric='cosine')\n",
    "    umap_embeddings = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # Create a dataframe for plotting\n",
    "    df = pd.DataFrame({\n",
    "        'x': umap_embeddings[:, 0],\n",
    "        'y': umap_embeddings[:, 1],\n",
    "        'topic': topics\n",
    "    })\n",
    "    \n",
    "    # Get topic labels with top words\n",
    "    topic_labels = {}\n",
    "    for topic in np.unique(topics):\n",
    "        if topic == -1:\n",
    "            topic_labels[topic] = \"Topic -1 (Outliers)\"\n",
    "            continue\n",
    "            \n",
    "        words = model.get_topic(topic)\n",
    "        if words:\n",
    "            top_words = \", \".join([word for word, _ in words[:3]])\n",
    "            topic_labels[topic] = f\"Topic {topic}: {top_words}\"\n",
    "        else:\n",
    "            topic_labels[topic] = f\"Topic {topic}\"\n",
    "    \n",
    "    # Add topic labels to dataframe\n",
    "    df['topic_label'] = df['topic'].map(topic_labels)\n",
    "    \n",
    "    # Create Plotly figure\n",
    "    fig = px.scatter(\n",
    "        df, x='x', y='y', color='topic_label',\n",
    "        hover_data=['topic_label'],\n",
    "        color_discrete_sequence=px.colors.qualitative.Bold,\n",
    "        title=title\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        legend_title=\"Topics\",\n",
    "        font=dict(size=12),\n",
    "        legend=dict(itemsizing='constant')\n",
    "    )\n",
    "    \n",
    "    # Update traces\n",
    "    fig.update_traces(\n",
    "        marker=dict(size=8, opacity=0.7, line=dict(width=1, color='white')),\n",
    "        selector=dict(mode='markers')\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_document_topic_map(model, embeddings, topics, probabilities, subject_main_dir, \n",
    "                                subject_type, column_name, top_n=500):\n",
    "    \"\"\"\n",
    "    Visualize documents in 2D space colored by their topics with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    embeddings (array): Document embeddings\n",
    "    topics (list): Topic assignments\n",
    "    probabilities (list/array): Topic probabilities\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    column_name (str): Name of the column analyzed\n",
    "    top_n (int): Number of documents to visualize\n",
    "    \n",
    "    Returns:\n",
    "    dict: Visualization details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create the document map\n",
    "        fig = create_document_topic_map(\n",
    "            model=model,\n",
    "            embeddings=embeddings,\n",
    "            topics=topics,\n",
    "            probabilities=probabilities,\n",
    "            top_n=top_n,\n",
    "            title=f\"{column_name} Document-Topic Map\"\n",
    "        )\n",
    "        \n",
    "        # Save the visualization\n",
    "        Path(f\"{subject_main_dir}/{subject_type}\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        html_filename = f\"{column_name}_document_topic_map.html\"\n",
    "        fig.write_html(f\"{subject_main_dir}/{subject_type}/{html_filename}\")\n",
    "        \n",
    "        # Also save as PNG\n",
    "        png_filename = f\"{column_name}_document_topic_map.png\"\n",
    "        fig.write_image(f\"{subject_main_dir}/{subject_type}/{png_filename}\")\n",
    "        \n",
    "        return {\n",
    "            \"visualizations\": [\n",
    "                f\"{subject_type}/{html_filename}\",\n",
    "                f\"{subject_type}/{png_filename}\"\n",
    "            ],\n",
    "            \"type\": \"document_topic_map\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\"error\": f\"Failed to create document topic map: {str(e)}\\n{traceback.format_exc()}\"}\n",
    "\n",
    "def create_topic_class_distribution(model, df, topic_column, class_column, \n",
    "                                  topics=None, top_n=10, normalize=True,\n",
    "                                  title=\"Topic Distribution by Class\"):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of topics across different classes with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    df (DataFrame): DataFrame with topics assigned\n",
    "    topic_column (str): Column name containing the assigned topics\n",
    "    class_column (str): Column name containing the class information\n",
    "    topics (list, optional): List of topics to include\n",
    "    top_n (int): Number of top topics to show\n",
    "    normalize (bool): Whether to normalize counts to percentages\n",
    "    title (str): Title for the visualization\n",
    "    \n",
    "    Returns:\n",
    "    plotly.graph_objects.Figure: Interactive heatmap figure\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # If topics not specified, get top N non-outlier topics by count\n",
    "    if topics is None:\n",
    "        topic_counts = data[topic_column].value_counts()\n",
    "        # Filter out outlier topic (-1)\n",
    "        filtered_topics = [t for t in topic_counts.index if t != -1]\n",
    "        # Get top N topics\n",
    "        topics = [t for t in filtered_topics[:top_n]]\n",
    "    \n",
    "    # Get unique classes\n",
    "    classes = data[class_column].unique()\n",
    "    \n",
    "    # Create topic labels with top words\n",
    "    topic_labels = {}\n",
    "    for topic in topics:\n",
    "        words = model.get_topic(topic)\n",
    "        if words:\n",
    "            top_words = \", \".join([word for word, _ in words[:3]])\n",
    "            topic_labels[topic] = f\"Topic {topic}: {top_words}\"\n",
    "        else:\n",
    "            topic_labels[topic] = f\"Topic {topic}\"\n",
    "    \n",
    "    # Group by class and topic, count documents\n",
    "    cross_tab = pd.crosstab(\n",
    "        data[class_column], \n",
    "        data[topic_column],\n",
    "        normalize='index' if normalize else False\n",
    "    )\n",
    "    \n",
    "    # Keep only selected topics\n",
    "    available_topics = [t for t in topics if t in cross_tab.columns]\n",
    "    if available_topics:\n",
    "        cross_tab = cross_tab[available_topics]\n",
    "    \n",
    "    # Rename columns with topic labels\n",
    "    cross_tab = cross_tab.rename(columns=topic_labels)\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=cross_tab.values,\n",
    "        x=cross_tab.columns,\n",
    "        y=cross_tab.index,\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(\n",
    "            title=\"Percentage\" if normalize else \"Count\",\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Topics\",\n",
    "        yaxis_title=class_column,\n",
    "        height=600,\n",
    "        width=1000,\n",
    "        font=dict(size=12),\n",
    "        xaxis={'tickangle': 45}\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_topic_class_distribution(model, processed_df, class_column, subject_main_dir,\n",
    "                                     subject_type, column_name, top_n_topics=10):\n",
    "    \"\"\"\n",
    "    Visualize how topics are distributed across classes with topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    model: BERTopic model\n",
    "    processed_df (DataFrame): DataFrame with topics assigned\n",
    "    class_column (str): Column name containing the class information\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    column_name (str): Name of the column analyzed\n",
    "    top_n_topics (int): Number of top topics to visualize\n",
    "    \n",
    "    Returns:\n",
    "    dict: Visualization details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure class column exists\n",
    "        if class_column not in processed_df.columns:\n",
    "            return {\"error\": f\"Class column '{class_column}' not found in DataFrame\"}\n",
    "        \n",
    "        # Create the class distribution visualization\n",
    "        fig = create_topic_class_distribution(\n",
    "            model=model,\n",
    "            df=processed_df,\n",
    "            topic_column='bertopic_topic',  # Column with assigned topics\n",
    "            class_column=class_column,\n",
    "            top_n=top_n_topics,\n",
    "            normalize=True,  # Use percentages\n",
    "            title=f\"Topic Distribution for {column_name} by {class_column}\"\n",
    "        )\n",
    "        \n",
    "        # Save the visualization\n",
    "        Path(f\"{subject_main_dir}/{subject_type}\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        html_filename = f\"{column_name}_topic_by_{class_column}.html\"\n",
    "        fig.write_html(f\"{subject_main_dir}/{subject_type}/{html_filename}\")\n",
    "        \n",
    "        # Also save as PNG\n",
    "        png_filename = f\"{column_name}_topic_by_{class_column}.png\"\n",
    "        fig.write_image(f\"{subject_main_dir}/{subject_type}/{png_filename}\")\n",
    "        \n",
    "        return {\n",
    "            \"visualizations\": [\n",
    "                f\"{subject_type}/{html_filename}\",\n",
    "                f\"{subject_type}/{png_filename}\"\n",
    "            ],\n",
    "            \"type\": \"topic_class_distribution\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\"error\": f\"Failed to create topic class distribution: {str(e)}\\n{traceback.format_exc()}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5164888e-5ce7-4aef-b5ad-81f9fb8ee395",
   "metadata": {},
   "source": [
    "### Main BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b7107-ddc7-4289-8516-33d1498e73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Union, Optional, Any\n",
    "\n",
    "def run_comprehensive_bertopic_analysis(\n",
    "    df, \n",
    "    column_name, \n",
    "    time_column=None,\n",
    "    class_column=None,\n",
    "    subject_main_dir=\"analyze_dataset\", \n",
    "    language=\"english\", \n",
    "    nr_topics=\"auto\", \n",
    "    min_topic_size=10, \n",
    "    top_n_topics=10,\n",
    "    visualize_documents=True,\n",
    "    sample_documents=500\n",
    "):\n",
    "    \"\"\"\n",
    "    Run comprehensive BERTopic analysis on text data with all visualizations including topic numbers in labels\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the text column\n",
    "    column_name (str): Name of the column to analyze\n",
    "    time_column (str, optional): Name of the column with time information for temporal analysis\n",
    "    class_column (str, optional): Name of the column with class information for class distribution\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    language (str): Language for stopwords removal and preprocessing\n",
    "    nr_topics (int or str): Number of topics to find ('auto' or specific number)\n",
    "    min_topic_size (int): Minimum size of topics\n",
    "    top_n_topics (int): Number of top topics to visualize\n",
    "    visualize_documents (bool): Whether to visualize document-topic maps\n",
    "    sample_documents (int): Number of documents to sample for document visualizations\n",
    "    \n",
    "    Returns:\n",
    "    dict: Analysis results\n",
    "    \"\"\"\n",
    "    # Print status\n",
    "    print(f\"Running enhanced BERTopic analysis on '{column_name}'...\")\n",
    "    subject_type = column_name.lower()\n",
    "    \n",
    "    try:\n",
    "        # Import required packages\n",
    "        from bertopic import BERTopic\n",
    "        import nltk\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        from pathlib import Path\n",
    "        \n",
    "        # Set environment variable to avoid the tokenizers parallelism warning\n",
    "        import os\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "        \n",
    "        # Create subject directory if it doesn't exist\n",
    "        if subject_main_dir:\n",
    "            subject_dir = Path(subject_main_dir) / subject_type\n",
    "            subject_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Set up BERTopic model\n",
    "        def setup_bertopic(language=\"english\", nr_topics=\"auto\", min_topic_size=10):\n",
    "            try:\n",
    "                # Download stopwords if needed\n",
    "                try:\n",
    "                    nltk.data.find('corpora/stopwords')\n",
    "                except LookupError:\n",
    "                    nltk.download('stopwords')\n",
    "                \n",
    "                # Get stopwords for the specified language\n",
    "                from nltk.corpus import stopwords\n",
    "                stop_words = stopwords.words(language)\n",
    "                \n",
    "                # Set up the vectorizer with stopwords\n",
    "                vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "                \n",
    "                # Create and return the BERTopic model\n",
    "                model = BERTopic(\n",
    "                    language=language,\n",
    "                    nr_topics=nr_topics,\n",
    "                    min_topic_size=min_topic_size,\n",
    "                    vectorizer_model=vectorizer,\n",
    "                    calculate_probabilities=True  # Always calculate probabilities for better visualizations\n",
    "                )\n",
    "                return model\n",
    "            \n",
    "            except ImportError:\n",
    "                print(\"Please install required packages with: pip install bertopic nltk\")\n",
    "                return None\n",
    "        \n",
    "        # Create the model\n",
    "        model = setup_bertopic(language, nr_topics, min_topic_size)\n",
    "        if model is None:\n",
    "            return {\"error\": \"Failed to initialize BERTopic model\"}\n",
    "        \n",
    "        # Extract text data, filtering out non-string and NaN values\n",
    "        processed_df = df.copy()\n",
    "        processed_df = processed_df[processed_df[column_name].apply(\n",
    "            lambda x: isinstance(x, str) and pd.notna(x))]\n",
    "        \n",
    "        if len(processed_df) == 0:\n",
    "            return {\"error\": f\"No valid text data found in column {column_name}\"}\n",
    "        \n",
    "        # Apply specific filtering if needed\n",
    "        if column_name == \"Abstract\":\n",
    "            processed_df = processed_df[processed_df[\"Abstract\"] != \"No abstract available\"]\n",
    "        \n",
    "        # Get clean text data\n",
    "        documents = processed_df[column_name].tolist()\n",
    "        \n",
    "        # Fit the model on our text data\n",
    "        print(f\"Fitting BERTopic model on {len(documents)} documents...\")\n",
    "        \n",
    "        # Check if we should use embedding models\n",
    "        use_embeddings = len(documents) > 100  # Only use embeddings for larger datasets\n",
    "        \n",
    "        if use_embeddings:\n",
    "            try:\n",
    "                # Try to use sentence-transformers for embedding if available\n",
    "                from sentence_transformers import SentenceTransformer\n",
    "                \n",
    "                # Use a smaller multilingual model for efficiency\n",
    "                embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "                embeddings = embedding_model.encode(documents, show_progress_bar=True)\n",
    "                \n",
    "                # Fit the model with embeddings\n",
    "                topics, probabilities = model.fit_transform(documents, embeddings)\n",
    "                print(\"Used sentence-transformers for document embedding\")\n",
    "                \n",
    "            except ImportError:\n",
    "                # Fall back to default BERTopic embedding\n",
    "                topics, probabilities = model.fit_transform(documents)\n",
    "                embeddings = model.embedding_model.transform(documents)\n",
    "                print(\"Used default BERTopic embedding\")\n",
    "        else:\n",
    "            # For small datasets, use default BERTopic approach\n",
    "            topics, probabilities = model.fit_transform(documents)\n",
    "            # Get embeddings for visualizations\n",
    "            embeddings = model.embedding_model.transform(documents)\n",
    "        \n",
    "        # Add topics to the dataframe\n",
    "        processed_df[\"bertopic_topic\"] = topics\n",
    "        \n",
    "        # Handle probabilities properly\n",
    "        if hasattr(probabilities, 'ndim') and probabilities.ndim == 2:\n",
    "            # If probabilities is a 2D array, get the max value for each row\n",
    "            processed_df[\"bertopic_probability\"] = [prob.max() for prob in probabilities]\n",
    "        elif hasattr(probabilities, 'ndim') and probabilities.ndim == 1:\n",
    "            # If probabilities is a 1D array, use it directly\n",
    "            processed_df[\"bertopic_probability\"] = probabilities\n",
    "        else:\n",
    "            # Handle case where probabilities might be a list of arrays or other structures\n",
    "            processed_df[\"bertopic_probability\"] = [\n",
    "                prob.max() if hasattr(prob, 'max') else \n",
    "                (max(prob) if hasattr(prob, '__len__') and len(prob) > 0 else 0) \n",
    "                for prob in probabilities\n",
    "            ]\n",
    "        \n",
    "        # Get basic topic information\n",
    "        topic_info = model.get_topic_info()\n",
    "        print(f\"Identified {len(topic_info)-1} topics (excluding outlier topic)\")\n",
    "        \n",
    "        # Generate visualizations\n",
    "        results = {\n",
    "            \"column_analyzed\": column_name,\n",
    "            \"topic_count\": len(model.get_topics()),\n",
    "            \"visualizations\": []\n",
    "        }\n",
    "        \n",
    "        \n",
    "        # 1. Topic Word Clouds\n",
    "        print(\"Generating topic wordclouds...\")\n",
    "        wc_result = visualize_topic_wordclouds(\n",
    "            model=model,\n",
    "            subject_main_dir=subject_main_dir,\n",
    "            subject_type=subject_type,\n",
    "            column_name=column_name,\n",
    "            top_n_topics=top_n_topics\n",
    "        )\n",
    "        if \"error\" not in wc_result:\n",
    "            if \"visualization\" in wc_result:\n",
    "                results[\"visualizations\"].append(wc_result[\"visualization\"])\n",
    "            if \"visualizations\" in wc_result:\n",
    "                results[\"visualizations\"].extend(wc_result[\"visualizations\"])\n",
    "        else:\n",
    "            print(f\"Error generating wordclouds: {wc_result['error']}\")\n",
    "        \n",
    "        # 2. Topic Bar Charts\n",
    "        print(\"Generating topic bar charts...\")\n",
    "        bar_result = visualize_topic_barchart(\n",
    "            model=model,\n",
    "            subject_main_dir=subject_main_dir,\n",
    "            subject_type=subject_type,\n",
    "            column_name=column_name,\n",
    "            n_topics=top_n_topics\n",
    "        )\n",
    "        if \"error\" not in bar_result:\n",
    "            if \"visualization\" in bar_result:\n",
    "                results[\"visualizations\"].append(bar_result[\"visualization\"])\n",
    "            if \"visualizations\" in bar_result:\n",
    "                results[\"visualizations\"].extend(bar_result[\"visualizations\"])\n",
    "        else:\n",
    "            print(f\"Error generating bar charts: {bar_result['error']}\")\n",
    "        \n",
    "        # 3. Term Score Decline\n",
    "        print(\"Generating term score decline visualization...\")\n",
    "        term_rank_result = visualize_term_rank(\n",
    "            model=model,\n",
    "            subject_main_dir=subject_main_dir,\n",
    "            subject_type=subject_type,\n",
    "            column_name=column_name,\n",
    "            n_topics=5  # Use fewer topics for readability\n",
    "        )\n",
    "        if \"error\" not in term_rank_result:\n",
    "            if \"visualization\" in term_rank_result:\n",
    "                results[\"visualizations\"].append(term_rank_result[\"visualization\"])\n",
    "            if \"visualizations\" in term_rank_result:\n",
    "                results[\"visualizations\"].extend(term_rank_result[\"visualizations\"])\n",
    "        else:\n",
    "            print(f\"Error generating term rank visualization: {term_rank_result['error']}\")\n",
    "        \n",
    "        # 4. Topic Similarity Heatmap\n",
    "        print(\"Generating topic similarity heatmap...\")\n",
    "        try:\n",
    "            heatmap_fig = create_topic_heatmap(\n",
    "                model=model,\n",
    "                title=f\"{column_name} Topic Similarity Heatmap\"\n",
    "            )\n",
    "            heatmap_filename = f\"{column_name}_topic_similarity.html\"\n",
    "            heatmap_path = Path(subject_main_dir) / subject_type / heatmap_filename\n",
    "            heatmap_fig.write_html(str(heatmap_path))\n",
    "            \n",
    "            # Also save as PNG\n",
    "            heatmap_png = f\"{column_name}_topic_similarity.png\"\n",
    "            heatmap_png_path = Path(subject_main_dir) / subject_type / heatmap_png\n",
    "            heatmap_fig.write_image(str(heatmap_png_path))\n",
    "            \n",
    "            results[\"visualizations\"].extend([\n",
    "                f\"{subject_type}/{heatmap_filename}\",\n",
    "                f\"{subject_type}/{heatmap_png}\"\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating similarity heatmap: {str(e)}\")\n",
    "        \n",
    "        # 5. Topic Hierarchy\n",
    "        print(\"Generating topic hierarchy visualization...\")\n",
    "        try:\n",
    "            hierarchy_fig = create_topic_hierarchy(\n",
    "                model=model,\n",
    "                title=f\"{column_name} Topic Hierarchy\"\n",
    "            )\n",
    "            \n",
    "            if isinstance(hierarchy_fig, plt.Figure):\n",
    "                # Matplotlib figure\n",
    "                hierarchy_filename = f\"{column_name}_topic_hierarchy.png\"\n",
    "                hierarchy_path = Path(subject_main_dir) / subject_type / hierarchy_filename\n",
    "                hierarchy_fig.savefig(hierarchy_path, bbox_inches='tight', dpi=300)\n",
    "                plt.close(hierarchy_fig)\n",
    "                results[\"visualizations\"].append(f\"{subject_type}/{hierarchy_filename}\")\n",
    "            else:\n",
    "                # Plotly figure\n",
    "                hierarchy_filename = f\"{column_name}_topic_hierarchy.html\"\n",
    "                hierarchy_path = Path(subject_main_dir) / subject_type / hierarchy_filename\n",
    "                hierarchy_fig.write_html(str(hierarchy_path))\n",
    "                \n",
    "                # Also save as PNG\n",
    "                hierarchy_png = f\"{column_name}_topic_hierarchy.png\"\n",
    "                hierarchy_png_path = Path(subject_main_dir) / subject_type / hierarchy_png\n",
    "                hierarchy_fig.write_image(str(hierarchy_png_path))\n",
    "                \n",
    "                results[\"visualizations\"].extend([\n",
    "                    f\"{subject_type}/{hierarchy_filename}\",\n",
    "                    f\"{subject_type}/{hierarchy_png}\"\n",
    "                ])\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating topic hierarchy: {str(e)}\")\n",
    "        \n",
    "        # 6. Topic Network\n",
    "        print(\"Generating topic network visualization...\")\n",
    "        try:\n",
    "            network_fig = create_topic_network(\n",
    "                model=model,\n",
    "                title=f\"{column_name} Topic Similarity Network\"\n",
    "            )\n",
    "            network_filename = f\"{column_name}_topic_network.html\"\n",
    "            network_path = Path(subject_main_dir) / subject_type / network_filename\n",
    "            network_fig.write_html(str(network_path))\n",
    "            \n",
    "            # Also save as PNG\n",
    "            network_png = f\"{column_name}_topic_network.png\"\n",
    "            network_png_path = Path(subject_main_dir) / subject_type / network_png\n",
    "            network_fig.write_image(str(network_png_path))\n",
    "            \n",
    "            results[\"visualizations\"].extend([\n",
    "                f\"{subject_type}/{network_filename}\",\n",
    "                f\"{subject_type}/{network_png}\"\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating topic network: {str(e)}\")\n",
    "        \n",
    "        # 7. Topic Distribution\n",
    "        print(\"Generating topic distribution visualization...\")\n",
    "        dist_result = visualize_topic_distribution(\n",
    "            model=model,\n",
    "            topics=topics,\n",
    "            probabilities=probabilities,\n",
    "            subject_main_dir=subject_main_dir,\n",
    "            subject_type=subject_type,\n",
    "            column_name=column_name\n",
    "        )\n",
    "        if \"error\" not in dist_result:\n",
    "            if \"visualization\" in dist_result:\n",
    "                results[\"visualizations\"].append(dist_result[\"visualization\"])\n",
    "            if \"visualizations\" in dist_result:\n",
    "                results[\"visualizations\"].extend(dist_result[\"visualizations\"])\n",
    "        else:\n",
    "            print(f\"Error generating topic distribution: {dist_result['error']}\")\n",
    "        \n",
    "        # 8. Document-Topic Map (if requested)\n",
    "        if visualize_documents:\n",
    "            print(\"Generating document-topic map...\")\n",
    "            try:\n",
    "                map_result = visualize_document_topic_map(\n",
    "                    model=model,\n",
    "                    embeddings=embeddings,\n",
    "                    topics=topics,\n",
    "                    probabilities=probabilities,\n",
    "                    subject_main_dir=subject_main_dir,\n",
    "                    subject_type=subject_type,\n",
    "                    column_name=column_name,\n",
    "                    top_n=sample_documents  # Sample a subset of documents\n",
    "                )\n",
    "                if \"error\" not in map_result:\n",
    "                    if \"visualization\" in map_result:\n",
    "                        results[\"visualizations\"].append(map_result[\"visualization\"])\n",
    "                    if \"visualizations\" in map_result:\n",
    "                        results[\"visualizations\"].extend(map_result[\"visualizations\"])\n",
    "                else:\n",
    "                    print(f\"Error generating document-topic map: {map_result['error']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating document-topic map: {str(e)}\")\n",
    "                \n",
    "            # 9. Document DataMap\n",
    "            print(\"Generating document datamap visualization...\")\n",
    "            try:\n",
    "                datamap_fig = visualize_document_datamap(\n",
    "                    model=model,\n",
    "                    docs=documents[:sample_documents] if len(documents) > sample_documents else documents,\n",
    "                    topics=topics[:sample_documents] if len(topics) > sample_documents else topics,\n",
    "                    embeddings=embeddings[:sample_documents] if len(embeddings) > sample_documents else embeddings,\n",
    "                    title=f\"{column_name} Document DataMap\"\n",
    "                )\n",
    "                \n",
    "                if datamap_fig:\n",
    "                    datamap_filename = f\"{column_name}_document_datamap.html\"\n",
    "                    datamap_path = Path(subject_main_dir) / subject_type / datamap_filename\n",
    "                    datamap_fig.write_html(str(datamap_path))\n",
    "                    \n",
    "                    # Also save as PNG\n",
    "                    datamap_png = f\"{column_name}_document_datamap.png\"\n",
    "                    datamap_png_path = Path(subject_main_dir) / subject_type / datamap_png\n",
    "                    datamap_fig.write_image(str(datamap_png_path))\n",
    "                    \n",
    "                    results[\"visualizations\"].extend([\n",
    "                        f\"{subject_type}/{datamap_filename}\",\n",
    "                        f\"{subject_type}/{datamap_png}\"\n",
    "                    ])\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating document datamap: {str(e)}\")\n",
    "        \n",
    "        # 10. Topics over Time (if time column provided)\n",
    "        if time_column and time_column in processed_df.columns:\n",
    "            print(\"Generating topics over time visualization...\")\n",
    "            time_result = visualize_topics_over_time(\n",
    "                model=model,\n",
    "                processed_df=processed_df,\n",
    "                decade_column=time_column,\n",
    "                subject_main_dir=subject_main_dir,\n",
    "                subject_type=subject_type,\n",
    "                column_name=column_name,\n",
    "                top_n_topics=top_n_topics\n",
    "            )\n",
    "            if \"error\" not in time_result:\n",
    "                if \"visualization\" in time_result:\n",
    "                    results[\"visualizations\"].append(time_result[\"visualization\"])\n",
    "                if \"visualizations\" in time_result:\n",
    "                    results[\"visualizations\"].extend(time_result[\"visualizations\"])\n",
    "            else:\n",
    "                print(f\"Error generating topics over time: {time_result['error']}\")\n",
    "                \n",
    "            # 11. Topic Distribution over Time\n",
    "            print(\"Generating topic distribution over time...\")\n",
    "            dist_time_result = visualize_topic_distribution_over_time(\n",
    "                model=model,\n",
    "                processed_df=processed_df,\n",
    "                time_column=time_column,\n",
    "                subject_main_dir=subject_main_dir,\n",
    "                subject_type=subject_type,\n",
    "                column_name=column_name,\n",
    "                top_n_topics=top_n_topics\n",
    "            )\n",
    "            if \"error\" not in dist_time_result:\n",
    "                if \"visualization\" in dist_time_result:\n",
    "                    results[\"visualizations\"].append(dist_time_result[\"visualization\"])\n",
    "                if \"visualizations\" in dist_time_result:\n",
    "                    results[\"visualizations\"].extend(dist_time_result[\"visualizations\"])\n",
    "            else:\n",
    "                print(f\"Error generating topic distribution over time: {dist_time_result['error']}\")\n",
    "        else:\n",
    "            print(f\"Skipping temporal analysis: No time column provided or column not found in dataframe\")\n",
    "        \n",
    "        # 12. Topic Class Distribution (if class column provided)\n",
    "        if class_column and class_column in processed_df.columns:\n",
    "            print(\"Generating topic class distribution visualization...\")\n",
    "            class_result = visualize_topic_class_distribution(\n",
    "                model=model,\n",
    "                processed_df=processed_df,\n",
    "                class_column=class_column,\n",
    "                subject_main_dir=subject_main_dir,\n",
    "                subject_type=subject_type,\n",
    "                column_name=column_name,\n",
    "                top_n_topics=top_n_topics\n",
    "            )\n",
    "            if \"error\" not in class_result:\n",
    "                if \"visualization\" in class_result:\n",
    "                    results[\"visualizations\"].append(class_result[\"visualization\"])\n",
    "                if \"visualizations\" in class_result:\n",
    "                    results[\"visualizations\"].extend(class_result[\"visualizations\"])\n",
    "            else:\n",
    "                print(f\"Error generating topic class distribution: {class_result['error']}\")\n",
    "        else:\n",
    "            print(f\"Skipping class distribution analysis: No class column provided or column not found in dataframe\")\n",
    "        \n",
    "        # 13. Topic Probability Distribution\n",
    "        try:\n",
    "            print(\"Generating topic probability distribution visualization...\")\n",
    "            if hasattr(model, 'visualize_distribution'):\n",
    "                # Use built-in method if available (newer BERTopic versions)\n",
    "                fig = model.visualize_distribution(probabilities[0], min_probability=0.01)\n",
    "                dist_filename = f\"{column_name}_topic_probability_distribution.html\"\n",
    "                dist_path = Path(subject_main_dir) / subject_type / dist_filename\n",
    "                fig.write_html(str(dist_path))\n",
    "                \n",
    "                # Also save as PNG\n",
    "                dist_png = f\"{column_name}_topic_probability_distribution.png\"\n",
    "                dist_png_path = Path(subject_main_dir) / subject_type / dist_png\n",
    "                fig.write_image(str(dist_png_path))\n",
    "                \n",
    "                results[\"visualizations\"].extend([\n",
    "                    f\"{subject_type}/{dist_filename}\",\n",
    "                    f\"{subject_type}/{dist_png}\"\n",
    "                ])\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating probability distribution: {str(e)}\")\n",
    "        \n",
    "        # 14. Add summary visualization with links to all visualizations\n",
    "        try:\n",
    "            print(\"Generating visualization summary...\")\n",
    "            html_content = f\"\"\"\n",
    "            <html>\n",
    "            <head>\n",
    "                <title>{column_name} Topic Modeling Analysis</title>\n",
    "                <style>\n",
    "                    body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "                    h1, h2 {{ color: #2c3e50; }}\n",
    "                    .viz-container {{ display: flex; flex-wrap: wrap; gap: 20px; }}\n",
    "                    .viz-item {{ \n",
    "                        border: 1px solid #ddd; \n",
    "                        border-radius: 5px; \n",
    "                        padding: 15px; \n",
    "                        margin-bottom: 15px;\n",
    "                        width: 45%;\n",
    "                    }}\n",
    "                    .viz-item h3 {{ margin-top: 0; }}\n",
    "                    .viz-item img {{ max-width: 100%; height: auto; }}\n",
    "                    a {{ color: #3498db; text-decoration: none; }}\n",
    "                    a:hover {{ text-decoration: underline; }}\n",
    "                </style>\n",
    "            </head>\n",
    "            <body>\n",
    "                <h1>{column_name} Topic Modeling Analysis</h1>\n",
    "                <p>Analysis performed with BERTopic using {len(model.get_topics())} topics.</p>\n",
    "                \n",
    "                <h2>Available Visualizations:</h2>\n",
    "                <div class=\"viz-container\">\n",
    "            \"\"\"\n",
    "            \n",
    "            # Group visualizations by type\n",
    "            viz_types = {\n",
    "                'wordclouds': ['wordcloud', 'wordclouds'],\n",
    "                'barchart': ['barchart', 'bar'],\n",
    "                'term_rank': ['term_rank', 'term_score'],\n",
    "                'similarity': ['similarity', 'heatmap'],\n",
    "                'hierarchy': ['hierarchy', 'tree'],\n",
    "                'network': ['network'],\n",
    "                'distribution': ['distribution'],\n",
    "                'document_map': ['document', 'datamap'],\n",
    "                'time': ['time', 'temporal'],\n",
    "                'class': ['class']\n",
    "            }\n",
    "            \n",
    "            # Add each visualization to the HTML\n",
    "            for viz in results[\"visualizations\"]:\n",
    "                if isinstance(viz, str):\n",
    "                    viz_path = viz\n",
    "                    viz_name = viz.split('/')[-1]\n",
    "                    \n",
    "                    # Determine visualization type for grouping\n",
    "                    viz_type = \"Other\"\n",
    "                    for t_name, keywords in viz_types.items():\n",
    "                        if any(keyword in viz_name.lower() for keyword in keywords):\n",
    "                            viz_type = t_name\n",
    "                            break\n",
    "                    \n",
    "                    # Get appropriate title based on filename\n",
    "                    viz_title = viz_name.replace('_', ' ').replace('.html', '').replace('.png', '').title()\n",
    "                    \n",
    "                    # Determine if it's an image or HTML file\n",
    "                    is_image = viz_name.endswith(('.png', '.jpg', '.jpeg', '.gif'))\n",
    "                    is_html = viz_name.endswith('.html')\n",
    "                    \n",
    "                    html_content += f\"\"\"\n",
    "                    <div class=\"viz-item\">\n",
    "                        <h3>{viz_title}</h3>\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    if is_image:\n",
    "                        html_content += f\"\"\"\n",
    "                        <img src=\"../{viz_path}\" alt=\"{viz_title}\">\n",
    "                        <p><a href=\"../{viz_path}\" target=\"_blank\">Open full-size image</a></p>\n",
    "                        \"\"\"\n",
    "                    elif is_html:\n",
    "                        html_content += f\"\"\"\n",
    "                        <iframe src=\"../{viz_path}\" width=\"100%\" height=\"300px\"></iframe>\n",
    "                        <p><a href=\"../{viz_path}\" target=\"_blank\">Open interactive visualization</a></p>\n",
    "                        \"\"\"\n",
    "                    else:\n",
    "                        html_content += f\"\"\"\n",
    "                        <p><a href=\"../{viz_path}\" target=\"_blank\">Open visualization</a></p>\n",
    "                        \"\"\"\n",
    "                    \n",
    "                    html_content += \"</div>\\n\"\n",
    "            \n",
    "            html_content += \"\"\"\n",
    "                </div>\n",
    "            </body>\n",
    "            </html>\n",
    "            \"\"\"\n",
    "            \n",
    "            # Save the HTML summary\n",
    "            summary_filename = f\"{column_name}_topic_modeling_summary.html\"\n",
    "            summary_path = Path(subject_main_dir) / subject_type / summary_filename\n",
    "            with open(summary_path, 'w') as f:\n",
    "                f.write(html_content)\n",
    "            \n",
    "            results[\"summary\"] = f\"{subject_type}/{summary_filename}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating visualization summary: {str(e)}\")\n",
    "        \n",
    "        # Save topic info as CSV\n",
    "        if subject_main_dir:\n",
    "            topic_info_path = f\"{subject_main_dir}/{subject_type}/{column_name}_topic_info.csv\"\n",
    "            topic_info.to_csv(topic_info_path, index=False)\n",
    "            results[\"topic_info_path\"] = topic_info_path\n",
    "            print(f\"Topic information saved to {topic_info_path}\")\n",
    "        \n",
    "        # Add topic assignments to results\n",
    "        results[\"document_topics\"] = topics\n",
    "        results[\"document_topic_probs\"] = probabilities\n",
    "        results[\"processed_df\"] = processed_df\n",
    "        results[\"model\"] = model\n",
    "        \n",
    "        print(\"Comprehensive BERTopic analysis completed successfully!\")\n",
    "        return results\n",
    "        \n",
    "    except ImportError as e:\n",
    "        return {\"error\": f\"Required package missing: {str(e)}. Please install with: pip install bertopic nltk scikit-learn plotly matplotlib pandas\"}\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\"error\": f\"BERTopic analysis failed: {str(e)}\\n{traceback.format_exc()}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6046f-73c9-407f-b7e9-662c15250a33",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8363abe2-d708-4ae9-ad42-23353414700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sentiment_analysis(df, column_name, subject_type=None, model_name=\"nlptown/bert-base-multilingual-uncased-sentiment\", \n",
    "                           batch_size=32, max_samples=None, save_results=True):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on a text column using a multilingual transformer model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing the text data\n",
    "    column_name : str\n",
    "        The name of the column to analyze\n",
    "    subject_type : str, optional\n",
    "        The type of subject for saving results. If None, uses lowercase column_name\n",
    "    model_name : str, optional\n",
    "        The HuggingFace model to use for sentiment analysis\n",
    "    batch_size : int, optional\n",
    "        Batch size for processing texts\n",
    "    max_samples : int, optional\n",
    "        Maximum number of samples to process (for testing)\n",
    "    save_results : bool, optional\n",
    "        Whether to save results to disk\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        The original dataframe with added sentiment columns\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import time\n",
    "    from transformers import pipeline\n",
    "    import torch\n",
    "    print(f\"Running sentiment analysis on column: {column_name}\")\n",
    "    \n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Set subject type\n",
    "    if subject_type is None:\n",
    "        subject_type = column_name.lower()\n",
    "    \n",
    "    # Prepare directory for saving results\n",
    "    if save_results:\n",
    "        # Define subject_main_dir if not already defined in your environment\n",
    "        subject_main_dir = os.environ.get('SUBJECT_MAIN_DIR', 'analyze_dataset')\n",
    "        save_dir = os.path.join(subject_main_dir, subject_type, \"sentiment_analysis\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        print(f\"Results will be saved to: {save_dir}\")\n",
    "    \n",
    "    # Filter out missing values\n",
    "    valid_mask = result_df[column_name].notna()\n",
    "    texts = result_df.loc[valid_mask, column_name].astype(str)\n",
    "    \n",
    "    # Limit the number of samples if specified\n",
    "    if max_samples is not None and len(texts) > max_samples:\n",
    "        sample_indices = np.random.choice(texts.index, max_samples, replace=False)\n",
    "        texts = texts.loc[sample_indices]\n",
    "        print(f\"Analyzing {max_samples} random samples out of {valid_mask.sum()} valid texts\")\n",
    "    else:\n",
    "        print(f\"Analyzing all {valid_mask.sum()} valid texts\")\n",
    "    \n",
    "    # Initialize the sentiment analysis pipeline\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    print(f\"Device set to use cuda:{device}\" if device >= 0 else \"Device set to use CPU\")\n",
    "    sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model_name, device=device)\n",
    "    \n",
    "    # Process texts in batches\n",
    "    all_results = []\n",
    "    texts_list = texts.tolist()\n",
    "    \n",
    "    # Use custom progress tracking instead of tqdm\n",
    "    total_batches = (len(texts_list) + batch_size - 1) // batch_size\n",
    "    print(f\"Processing {total_batches} batches...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i in range(0, len(texts_list), batch_size):\n",
    "        # Calculate and display progress\n",
    "        batch_num = i // batch_size + 1\n",
    "        elapsed = time.time() - start_time\n",
    "        if batch_num > 1:\n",
    "            avg_time_per_batch = elapsed / (batch_num - 1)\n",
    "            est_remaining_time = avg_time_per_batch * (total_batches - batch_num + 1)\n",
    "            print(f\"Analyzing sentiment: batch {batch_num}/{total_batches} - \" \n",
    "                  f\"{batch_num/total_batches*100:.1f}% complete - \"\n",
    "                  f\"Est. remaining: {est_remaining_time:.1f}s\", end='\\r')\n",
    "        \n",
    "        batch = texts_list[i:i+batch_size]\n",
    "        \n",
    "        # Some texts might be too long - truncate them\n",
    "        truncated_batch = [text[:512] if len(text) > 512 else text for text in batch]\n",
    "        \n",
    "        try:\n",
    "            results = sentiment_analyzer(truncated_batch)\n",
    "            all_results.extend(results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i//batch_size}: {e}\")\n",
    "            # Add empty results to maintain alignment\n",
    "            all_results.extend([{\"label\": \"UNKNOWN\", \"score\": 0.0}] * len(batch))\n",
    "    \n",
    "    print(\"\\nSentiment analysis complete!\")\n",
    "    \n",
    "    # Convert sentiment labels to numerical scores\n",
    "    # The model uses labels like \"1 star\", \"2 stars\", etc.\n",
    "    sentiment_scores = []\n",
    "    for result in all_results:\n",
    "        label = result[\"label\"]\n",
    "        score = result[\"score\"]\n",
    "        \n",
    "        # Extract numerical rating if available (for star-based models)\n",
    "        if label.startswith((\"1 \", \"2 \", \"3 \", \"4 \", \"5 \")):\n",
    "            numeric_score = int(label[0])\n",
    "        # Handle POSITIVE/NEGATIVE/NEUTRAL labels\n",
    "        elif label == \"POSITIVE\":\n",
    "            numeric_score = 5\n",
    "        elif label == \"NEGATIVE\":\n",
    "            numeric_score = 1\n",
    "        elif label == \"NEUTRAL\":\n",
    "            numeric_score = 3\n",
    "        else:\n",
    "            numeric_score = 0  # Unknown label format\n",
    "            \n",
    "        sentiment_scores.append({\n",
    "            \"sentiment_label\": label,\n",
    "            \"sentiment_confidence\": score,\n",
    "            \"sentiment_score\": numeric_score\n",
    "        })\n",
    "    \n",
    "    # Add sentiment results to the dataframe\n",
    "    for i, idx in enumerate(texts.index):\n",
    "        if i < len(sentiment_scores):  # Safety check\n",
    "            for key, value in sentiment_scores[i].items():\n",
    "                result_df.loc[idx, key] = value\n",
    "    \n",
    "    # Fill NaN sentiment values for rows that weren't analyzed\n",
    "    sentiment_columns = [\"sentiment_label\", \"sentiment_confidence\", \"sentiment_score\"]\n",
    "    for col in sentiment_columns:\n",
    "        if col in result_df.columns:\n",
    "            missing_mask = ~result_df[col].notna()\n",
    "            result_df.loc[missing_mask, col] = \"N/A\" if col == \"sentiment_label\" else np.nan\n",
    "    \n",
    "    # Generate visualizations if save_results is True\n",
    "    if save_results:\n",
    "        # Distribution of sentiment scores\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(result_df[\"sentiment_score\"].dropna(), bins=5, kde=True)\n",
    "        plt.title(f\"Distribution of Sentiment Scores for {column_name}\")\n",
    "        plt.xlabel(\"Sentiment Score (1-5)\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, \"sentiment_distribution.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "        # Pie chart of sentiment categories\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sentiment_counts = result_df[\"sentiment_label\"].value_counts()\n",
    "        plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
    "        plt.title(f\"Sentiment Categories for {column_name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, \"sentiment_categories_pie.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "        # Save the results to CSV\n",
    "        result_df[sentiment_columns + [column_name]].to_csv(\n",
    "            os.path.join(save_dir, f\"{subject_type}_sentiment_analysis.csv\"), index=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Sentiment analysis results saved to {save_dir}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Example usage:\n",
    "# sentiment_df = run_sentiment_analysis(df, \"Abstract\", max_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6af0e26-a803-4fe3-ad9f-36915ab1b001",
   "metadata": {},
   "source": [
    "### Zero shot sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101120b-e1e1-455e-aa7f-4d26648864dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def run_zero_shot_analysis(df, column_name, decade_column=\"decade\", subject_type=None, \n",
    "                           model_name=\"facebook/bart-large-mnli\", \n",
    "                           labels=None, top_n_labels=3,\n",
    "                           batch_size=16, max_samples=None, save_results=True):\n",
    "    \"\"\"\n",
    "    Analyze academic papers using zero-shot classification with enhanced visualizations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing the text data\n",
    "    column_name : str\n",
    "        The name of the column to analyze\n",
    "    decade_column : str\n",
    "        Name of the column containing decade information for trend analysis\n",
    "    subject_type : str, optional\n",
    "        The type of subject for saving results. If None, uses lowercase column_name\n",
    "    model_name : str, optional\n",
    "        The HuggingFace model to use for zero-shot classification\n",
    "    labels : list, optional\n",
    "        List of class labels to use for classification\n",
    "    top_n_labels : int, optional\n",
    "        Number of top labels to include in visualizations (default: 3)\n",
    "    batch_size : int, optional\n",
    "        Batch size for processing texts\n",
    "    max_samples : int, optional\n",
    "        Maximum number of samples to process (for testing)\n",
    "    save_results : bool, optional\n",
    "        Whether to save results to disk\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        The original dataframe with added classification columns\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import time\n",
    "    import re\n",
    "    import torch\n",
    "    \n",
    "    # Default labels for antisemitism research if none provided\n",
    "    if labels is None:\n",
    "        labels = [\n",
    "            \"historical analysis of antisemitism\",\n",
    "            \"contemporary antisemitism\", \n",
    "            \"antisemitism in politics\",\n",
    "            \"antisemitism in media\",\n",
    "            \"antisemitism in religion\",\n",
    "            \"addressing or combating antisemitism\",\n",
    "            \"causes of antisemitism\",\n",
    "            \"impact of antisemitism\",\n",
    "            \"antisemitic incidents\",\n",
    "            \"antisemitic theories and ideologies\"\n",
    "        ]\n",
    "    \n",
    "    print(f\"Running zero-shot classification on column: {column_name}\")\n",
    "    print(f\"Using labels: {labels}\")\n",
    "    \n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Set subject type\n",
    "    if subject_type is None:\n",
    "        subject_type = column_name.lower()\n",
    "    \n",
    "    # Prepare directory for saving results\n",
    "    if save_results:\n",
    "        # Define subject_main_dir if not already defined in your environment\n",
    "        subject_main_dir = os.environ.get('SUBJECT_MAIN_DIR', 'analyze_dataset')\n",
    "        save_dir = os.path.join(subject_main_dir, subject_type, \"zero_shot_analysis\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        print(f\"Results will be saved to: {save_dir}\")\n",
    "    \n",
    "    # Filter out missing values\n",
    "    valid_mask = result_df[column_name].notna()\n",
    "    texts = result_df.loc[valid_mask, column_name].astype(str)\n",
    "    \n",
    "    # Remove very short texts (likely not meaningful for analysis)\n",
    "    texts = texts[texts.str.len() > 10]\n",
    "    \n",
    "    # Limit the number of samples if specified\n",
    "    if max_samples is not None and len(texts) > max_samples:\n",
    "        sample_indices = np.random.choice(texts.index, max_samples, replace=False)\n",
    "        texts = texts.loc[sample_indices]\n",
    "        print(f\"Analyzing {max_samples} random samples out of {valid_mask.sum()} valid texts\")\n",
    "    else:\n",
    "        print(f\"Analyzing all {len(texts)} valid texts\")\n",
    "    \n",
    "    # Initialize the zero-shot classification pipeline\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    print(f\"Device set to use cuda:{device}\" if device >= 0 else \"Device set to use CPU\")\n",
    "    \n",
    "    # Load model and tokenizer separately\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    \n",
    "    # Create the pipeline\n",
    "    classifier = pipeline(\"zero-shot-classification\", \n",
    "                         model=model, \n",
    "                         tokenizer=tokenizer,\n",
    "                         device=0 if torch.cuda.is_available() else -1)\n",
    "    \n",
    "    # Process texts in batches\n",
    "    all_results = []\n",
    "    texts_list = texts.tolist()\n",
    "    \n",
    "    # Use custom progress tracking\n",
    "    total_batches = (len(texts_list) + batch_size - 1) // batch_size\n",
    "    print(f\"Processing {total_batches} batches...\")\n",
    "        \n",
    "    start_time = time.time()\n",
    "    for i in range(0, len(texts_list), batch_size):\n",
    "        # Calculate and display progress\n",
    "        batch_num = i // batch_size + 1\n",
    "        elapsed = time.time() - start_time\n",
    "        if batch_num > 1:\n",
    "            avg_time_per_batch = elapsed / (batch_num - 1)\n",
    "            est_remaining_time = avg_time_per_batch * (total_batches - batch_num + 1)\n",
    "            print(f\"Classifying: batch {batch_num}/{total_batches} - \" \n",
    "                  f\"{batch_num/total_batches*100:.1f}% complete - \"\n",
    "                  f\"Est. remaining: {est_remaining_time:.1f}s\", end='\\r')\n",
    "        \n",
    "        batch = texts_list[i:i+batch_size]\n",
    "        \n",
    "        # Clean the academic text - remove citations, clean up formatting\n",
    "        cleaned_batch = []\n",
    "        for text in batch:\n",
    "            # Remove citation patterns like [1], [2-4], etc.\n",
    "            text = re.sub(r'\\[\\d+(?:-\\d+)?\\]', '', text)\n",
    "            # Remove excessive whitespace\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            # Truncate if too long for the model (most models have 512-1024 token limits)\n",
    "            text = text[:1024] if len(text) > 1024 else text\n",
    "            cleaned_batch.append(text)\n",
    "        \n",
    "        try:\n",
    "            # Process each text individually to handle errors better\n",
    "            batch_results = []\n",
    "            for text in cleaned_batch:\n",
    "                try:\n",
    "                    # Skip very short texts\n",
    "                    if len(text.strip()) < 10:\n",
    "                        batch_results.append({\n",
    "                            \"text\": text,\n",
    "                            \"labels\": labels,\n",
    "                            \"scores\": [0] * len(labels),\n",
    "                            \"top_label\": \"N/A\",\n",
    "                            \"top_score\": 0.0\n",
    "                        })\n",
    "                        continue\n",
    "                        \n",
    "                    result = classifier(text, labels, multi_label=True)\n",
    "                    \n",
    "                    # Store results in a more accessible format\n",
    "                    batch_results.append({\n",
    "                        \"text\": text,\n",
    "                        \"labels\": result[\"labels\"],\n",
    "                        \"scores\": result[\"scores\"],\n",
    "                        \"top_label\": result[\"labels\"][0],\n",
    "                        \"top_score\": result[\"scores\"][0]\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing text: {str(e)}\")\n",
    "                    print(f\"Problematic text: {text[:100]}...\")\n",
    "                    # Add placeholder result\n",
    "                    batch_results.append({\n",
    "                        \"text\": text,\n",
    "                        \"labels\": labels,\n",
    "                        \"scores\": [0] * len(labels),\n",
    "                        \"top_label\": \"ERROR\",\n",
    "                        \"top_score\": 0.0\n",
    "                    })\n",
    "            \n",
    "            all_results.extend(batch_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing batch {batch_num}: {str(e)}\")\n",
    "            # Add placeholder results to maintain alignment\n",
    "            for text in cleaned_batch:\n",
    "                all_results.append({\n",
    "                    \"text\": text,\n",
    "                    \"labels\": labels,\n",
    "                    \"scores\": [0] * len(labels),\n",
    "                    \"top_label\": \"ERROR\",\n",
    "                    \"top_score\": 0.0\n",
    "                })\n",
    "    \n",
    "    print(\"\\nClassification complete!\")\n",
    "    \n",
    "    # Add classification results to the dataframe\n",
    "    # Create new columns for each label\n",
    "    for label in labels:\n",
    "        result_df[f\"zs_{label.replace(' ', '_')}\"] = np.nan\n",
    "    \n",
    "    # Add a column for the top label and its score\n",
    "    result_df[\"zs_top_label\"] = np.nan\n",
    "    result_df[\"zs_top_score\"] = np.nan\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    for i, idx in enumerate(texts.index):\n",
    "        if i < len(all_results):  # Safety check\n",
    "            result = all_results[i]\n",
    "            \n",
    "            # Add top label and score\n",
    "            result_df.loc[idx, \"zs_top_label\"] = result[\"top_label\"]\n",
    "            result_df.loc[idx, \"zs_top_score\"] = result[\"top_score\"]\n",
    "            \n",
    "            # Add individual scores for each label\n",
    "            for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
    "                col_name = f\"zs_{label.replace(' ', '_')}\"\n",
    "                if col_name in result_df.columns:\n",
    "                    result_df.loc[idx, col_name] = score\n",
    "    \n",
    "    # Generate visualizations if save_results is True\n",
    "    if save_results:\n",
    "        # 1. Distribution of data between labels by paper count\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        label_counts = result_df[\"zs_top_label\"].value_counts()\n",
    "        sns.barplot(x=label_counts.values, y=label_counts.index, palette=\"viridis\")\n",
    "        plt.title(f\"Distribution of Papers by Top Label for {column_name}\", fontsize=14)\n",
    "        plt.xlabel(\"Number of Papers\", fontsize=12)\n",
    "        plt.ylabel(\"Label\", fontsize=12)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, \"label_distribution_by_paper_count.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Distribution of confidence scores for top N labels\n",
    "        # Get the top N most frequent labels\n",
    "        top_n_frequent_labels = label_counts.nlargest(top_n_labels).index.tolist()\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for label in top_n_frequent_labels:\n",
    "            # Get confidence scores for papers with this label as top label\n",
    "            scores = result_df[result_df[\"zs_top_label\"] == label][\"zs_top_score\"]\n",
    "            if len(scores) > 0:  # Only plot if we have data\n",
    "                sns.kdeplot(scores, fill=True, label=f\"{label} (n={len(scores)})\")\n",
    "        \n",
    "        plt.title(f\"Confidence Score Distribution for Top Labels - {column_name}\", fontsize=14)\n",
    "        plt.xlabel(\"Confidence Score\", fontsize=12)\n",
    "        plt.ylabel(\"Density\", fontsize=12)\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, f\"top_labels_confidence_distribution.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 3a. Trend analysis by decade (if decade column exists)\n",
    "        if decade_column in result_df.columns:\n",
    "            # Filter to rows with valid decade information and top labels\n",
    "            trend_df = result_df.dropna(subset=[decade_column, \"zs_top_label\"])\n",
    "            \n",
    "            # Convert decade to numeric if it's not already\n",
    "            if not pd.api.types.is_numeric_dtype(trend_df[decade_column]):\n",
    "                try:\n",
    "                    # Try to extract year from various date formats\n",
    "                    trend_df = trend_df.copy()  # To avoid SettingWithCopyWarning\n",
    "                    trend_df[\"decade_numeric\"] = pd.to_datetime(trend_df[decade_column], errors='coerce').dt.year // 10 * 10\n",
    "                except:\n",
    "                    # If that fails, try to extract digits directly\n",
    "                    trend_df[\"decade_numeric\"] = trend_df[decade_column].astype(str).str.extract('(\\d{4})').astype(float) // 10 * 10\n",
    "            else:\n",
    "                # If it's already numeric, just floor to decade\n",
    "                trend_df[\"decade_numeric\"] = trend_df[decade_column] // 10 * 10\n",
    "            \n",
    "            # Drop rows with invalid decade values\n",
    "            trend_df = trend_df.dropna(subset=[\"decade_numeric\"])\n",
    "            \n",
    "            if len(trend_df) > 0:\n",
    "                # Count papers by decade and top label\n",
    "                decade_label_counts = pd.crosstab(\n",
    "                    trend_df[\"decade_numeric\"], \n",
    "                    trend_df[\"zs_top_label\"],\n",
    "                    normalize='index'\n",
    "                ) * 100  # Convert to percentage\n",
    "                \n",
    "                # Visualization 3a: Line chart showing trends in label prevalence\n",
    "                plt.figure(figsize=(14, 8))\n",
    "                \n",
    "                # Get the most prominent label for each decade\n",
    "                most_prominent_per_decade = decade_label_counts.idxmax(axis=1)\n",
    "                \n",
    "                # Find labels that were most prominent in at least one decade\n",
    "                prominent_labels = most_prominent_per_decade.unique()\n",
    "                \n",
    "                # Plot only prominent labels\n",
    "                for label in prominent_labels:\n",
    "                    if label in decade_label_counts.columns:\n",
    "                        plt.plot(\n",
    "                            decade_label_counts.index, \n",
    "                            decade_label_counts[label], \n",
    "                            marker='o', \n",
    "                            linewidth=2, \n",
    "                            label=label\n",
    "                        )\n",
    "                \n",
    "                plt.title(f\"Trends in Label Prevalence by Decade - {column_name}\", fontsize=14)\n",
    "                plt.xlabel(\"Decade\", fontsize=12)\n",
    "                plt.ylabel(\"Percentage of Papers (%)\", fontsize=12)\n",
    "                plt.grid(linestyle='--', alpha=0.7)\n",
    "                plt.legend(loc='best')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(save_dir, \"label_trends_by_decade.png\"), dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # Visualization 3b: Heatmap of most prominent labels per decade\n",
    "                plt.figure(figsize=(14, 10))\n",
    "                \n",
    "                # Create a binary indicator matrix where 1 indicates the most prominent label\n",
    "                top_label_matrix = pd.DataFrame(0, \n",
    "                                               index=decade_label_counts.index, \n",
    "                                               columns=decade_label_counts.columns)\n",
    "                \n",
    "                for decade in decade_label_counts.index:\n",
    "                    top_label = most_prominent_per_decade.loc[decade]\n",
    "                    if top_label in top_label_matrix.columns:\n",
    "                        top_label_matrix.loc[decade, top_label] = 1\n",
    "                \n",
    "                # Create the heatmap\n",
    "                sns.heatmap(top_label_matrix, cmap=\"YlOrRd\", cbar=False, linewidths=.5)\n",
    "                plt.title(f\"Most Prominent Label by Decade - {column_name}\", fontsize=14)\n",
    "                plt.xlabel(\"Label\", fontsize=12)\n",
    "                plt.ylabel(\"Decade\", fontsize=12)\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(save_dir, \"most_prominent_label_by_decade.png\"), dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # Visualization 3c: Alternative visualization - Stacked area chart\n",
    "                plt.figure(figsize=(14, 8))\n",
    "                decade_label_counts.plot.area(figsize=(14, 8), alpha=0.7, stacked=True)\n",
    "                plt.title(f\"Relative Distribution of Labels by Decade - {column_name}\", fontsize=14)\n",
    "                plt.xlabel(\"Decade\", fontsize=12)\n",
    "                plt.ylabel(\"Percentage of Papers (%)\", fontsize=12)\n",
    "                plt.grid(linestyle='--', alpha=0.7)\n",
    "                plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(save_dir, \"label_distribution_stacked_by_decade.png\"), dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        \n",
    "        # Additional visualization: Label correlation heatmap\n",
    "        score_cols = [col for col in result_df.columns if col.startswith(\"zs_\") and col not in [\"zs_top_label\", \"zs_top_score\"]]\n",
    "        if len(score_cols) > 1:  # Only if we have multiple labels\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            corr_matrix = result_df[score_cols].corr()\n",
    "            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "            sns.heatmap(\n",
    "                corr_matrix, \n",
    "                mask=mask, \n",
    "                cmap=\"coolwarm\", \n",
    "                annot=True, \n",
    "                fmt=\".2f\", \n",
    "                linewidths=.5,\n",
    "                vmin=-1, \n",
    "                vmax=1,\n",
    "                center=0\n",
    "            )\n",
    "            plt.title(f\"Correlation Between Label Scores - {column_name}\", fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(save_dir, \"label_score_correlation.png\"), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        # Save the results to CSV\n",
    "        result_df[[\"zs_top_label\", \"zs_top_score\"] + score_cols + [column_name]].to_csv(\n",
    "            os.path.join(save_dir, f\"{subject_type}_zero_shot_analysis.csv\"), index=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Zero-shot analysis results saved to {save_dir}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Example usage:\n",
    "results_df = run_zero_shot_analysis(\n",
    "    df, \n",
    "    \"Title\", \n",
    "    model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    ")\n",
    "results_df = run_zero_shot_analysis(\n",
    "    df, \n",
    "    \"Abstract\", \n",
    "    model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f444312-63f7-41bf-b206-a703dc8fbed0",
   "metadata": {},
   "source": [
    "## Keyword analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5721c421-1d25-4d51-90c5-1c5dfc199abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from community import best_partition  # python-louvain package for community detection\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def visualize_keyword_network(df, \n",
    "                             keywords, \n",
    "                             title_column=\"Title\", \n",
    "                             list_column=None, \n",
    "                             top_n=10, \n",
    "                             subject_main_dir=\"output\", \n",
    "                             subject_type=\"networks\",\n",
    "                             filename_prefix=\"keyword_network\"):\n",
    "    \"\"\"\n",
    "    Create and visualize a network graph where keywords connect to most common entities\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the data\n",
    "    keywords (list): List of keywords to filter titles\n",
    "    title_column (str): Name of the column containing titles to filter\n",
    "    list_column (str): Name of the column containing lists of string values\n",
    "    top_n (int): Number of top entities to include in the graph\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    subject_type (str): Subfolder name for this analysis type\n",
    "    filename_prefix (str): Prefix for saved files\n",
    "    \n",
    "    Returns:\n",
    "    nx.Graph: The created network graph\n",
    "    \"\"\"\n",
    "    # Step 1: Filter dataframe for titles containing at least one keyword\n",
    "    filtered_df = filter_by_keywords(df, keywords, title_column)\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        print(f\"No rows found with titles containing the specified keywords: {keywords}\")\n",
    "        return None\n",
    "        \n",
    "    # Step 2: Get top entities from the list column\n",
    "    if list_column is None:\n",
    "        raise ValueError(\"list_column must be specified\")\n",
    "        \n",
    "    top_entities = get_top_entities(filtered_df, list_column, top_n)\n",
    "    \n",
    "    # Step 3: Create network graph\n",
    "    G = create_network_graph(filtered_df, keywords, list_column, top_entities)\n",
    "    \n",
    "    # Step 4: Visualize and save the graph\n",
    "    visualize_graph(G, keywords, top_entities, \n",
    "                   f\"{filename_prefix}_base\", \n",
    "                   subject_main_dir, \n",
    "                   subject_type)\n",
    "    \n",
    "    # Step 5: Analyze and visualize communities\n",
    "    visualize_communities(G, keywords, top_entities, \n",
    "                         f\"{filename_prefix}_communities\", \n",
    "                         subject_main_dir, \n",
    "                         subject_type)\n",
    "    \n",
    "    # Step 6: Find and visualize cliques\n",
    "    visualize_cliques(G, keywords, top_entities, \n",
    "                     f\"{filename_prefix}_cliques\", \n",
    "                     subject_main_dir, \n",
    "                     subject_type)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def filter_by_keywords(df, keywords, title_column):\n",
    "    \"\"\"Filter dataframe for rows where title contains at least one keyword\"\"\"\n",
    "    # Create case-insensitive filter for each keyword\n",
    "    filters = []\n",
    "    for keyword in keywords:\n",
    "        filters.append(df[title_column].str.contains(keyword, case=False))\n",
    "        \n",
    "    # Combine filters with OR operation\n",
    "    combined_filter = filters[0]\n",
    "    for f in filters[1:]:\n",
    "        combined_filter = combined_filter | f\n",
    "        \n",
    "    return df[combined_filter].copy()\n",
    "\n",
    "def get_top_entities(df, list_column, top_n):\n",
    "    \"\"\"Get top n most common entities from a column of lists\"\"\"\n",
    "    # Flatten the lists and count occurrences\n",
    "    all_entities = []\n",
    "    \n",
    "    for entities_list in df[list_column]:\n",
    "        # Handle string representation of lists (if needed)\n",
    "        if isinstance(entities_list, str):\n",
    "            try:\n",
    "                entities_list = eval(entities_list)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        # Skip if not a list\n",
    "        if not isinstance(entities_list, list):\n",
    "            continue\n",
    "            \n",
    "        all_entities.extend(entities_list)\n",
    "    \n",
    "    # Count and get top n\n",
    "    counter = Counter(all_entities)\n",
    "    return [item for item, count in counter.most_common(top_n)]\n",
    "\n",
    "def create_network_graph(df, keywords, list_column, top_entities):\n",
    "    \"\"\"Create a network graph connecting keywords with entities\"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add keyword nodes\n",
    "    for keyword in keywords:\n",
    "        G.add_node(keyword, type='keyword')\n",
    "    \n",
    "    # Add entity nodes\n",
    "    for entity in top_entities:\n",
    "        G.add_node(entity, type='entity')\n",
    "    \n",
    "    # Add edges based on co-occurrences\n",
    "    for _, row in df.iterrows():\n",
    "        title = row[df.columns[0]]  # First column assumed to be title\n",
    "        \n",
    "        # Get entities for this row\n",
    "        entities = row[list_column]\n",
    "        if isinstance(entities, str):\n",
    "            try:\n",
    "                entities = eval(entities)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        if not isinstance(entities, list):\n",
    "            continue\n",
    "            \n",
    "        # Filter to only include top entities\n",
    "        row_entities = [e for e in entities if e in top_entities]\n",
    "        \n",
    "        # Connect keywords in title to entities\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in title.lower():\n",
    "                for entity in row_entities:\n",
    "                    # Add edge or increment weight if exists\n",
    "                    if G.has_edge(keyword, entity):\n",
    "                        G[keyword][entity]['weight'] += 1\n",
    "                    else:\n",
    "                        G.add_edge(keyword, entity, weight=1)\n",
    "    \n",
    "    # Calculate node centrality\n",
    "    centrality = nx.degree_centrality(G)\n",
    "    nx.set_node_attributes(G, centrality, 'centrality')\n",
    "    \n",
    "    return G\n",
    "\n",
    "def visualize_graph(G, keywords, top_entities, filename, subject_main_dir, subject_type):\n",
    "    \"\"\"Visualize and save the network graph\"\"\"\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Set node positions using spring layout\n",
    "    pos = nx.spring_layout(G, k=0.3, seed=42)\n",
    "    \n",
    "    # Get node attributes\n",
    "    node_types = nx.get_node_attributes(G, 'type')\n",
    "    centrality = nx.get_node_attributes(G, 'centrality')\n",
    "    \n",
    "    # Prepare node lists by type\n",
    "    keyword_nodes = [node for node in G.nodes() if node in keywords]\n",
    "    entity_nodes = [node for node in G.nodes() if node in top_entities]\n",
    "    \n",
    "    # Node sizes based on centrality (scaled differently for each type)\n",
    "    keyword_sizes = [centrality.get(node, 0.1) * 3000 for node in keyword_nodes]\n",
    "    entity_sizes = [centrality.get(node, 0.1) * 2000 for node in entity_nodes]\n",
    "    \n",
    "    # Edge weights for line thickness and color\n",
    "    edges = G.edges()\n",
    "    weights = [G[u][v]['weight'] for u, v in edges]\n",
    "    max_weight = max(weights) if weights else 1\n",
    "    normalized_weights = [w/max_weight for w in weights]\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=keyword_nodes, \n",
    "                          node_size=keyword_sizes, \n",
    "                          node_color='red', \n",
    "                          node_shape='o', \n",
    "                          alpha=0.8)\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=entity_nodes, \n",
    "                          node_size=entity_sizes, \n",
    "                          node_color='blue', \n",
    "                          node_shape='s', \n",
    "                          alpha=0.6)\n",
    "    \n",
    "    # Draw edges with varying thickness\n",
    "    for (u, v, data) in G.edges(data=True):\n",
    "        width = data['weight'] * 2 / max_weight\n",
    "        alpha = 0.3 + (0.7 * data['weight'] / max_weight)\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], width=width, alpha=alpha, \n",
    "                              edge_color='gray')\n",
    "    \n",
    "    # Draw labels with varying sizes\n",
    "    keyword_labels = {node: node for node in keyword_nodes}\n",
    "    entity_labels = {node: node for node in entity_nodes}\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, labels=keyword_labels, font_size=12, font_weight='bold')\n",
    "    nx.draw_networkx_labels(G, pos, labels=entity_labels, font_size=10)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.plot([0], [0], 'ro', markersize=10, label='Keywords')\n",
    "    plt.plot([0], [0], 'bs', markersize=10, label='Entities')\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    plt.title(f\"Network of Keywords and Top {len(top_entities)} Entities\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, f\"{filename}.png\", subject_main_dir, subject_type)\n",
    "\n",
    "def visualize_communities(G, keywords, top_entities, filename, subject_main_dir, subject_type):\n",
    "    \"\"\"Detect and visualize communities in the network\"\"\"\n",
    "    # Detect communities using Louvain algorithm\n",
    "    partition = best_partition(G)\n",
    "    communities = {}\n",
    "    \n",
    "    # Group nodes by community\n",
    "    for node, community_id in partition.items():\n",
    "        if community_id not in communities:\n",
    "            communities[community_id] = []\n",
    "        communities[community_id].append(node)\n",
    "    \n",
    "    # Count communities\n",
    "    num_communities = len(communities)\n",
    "    \n",
    "    if num_communities <= 1:\n",
    "        print(\"Only one community detected, skipping community visualization\")\n",
    "        return\n",
    "    \n",
    "    # Create a colormap for communities\n",
    "    cmap = plt.cm.get_cmap('tab20', num_communities)\n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Set node positions using spring layout\n",
    "    pos = nx.spring_layout(G, k=0.3, seed=42)\n",
    "    \n",
    "    # Get centrality for node sizes\n",
    "    centrality = nx.get_node_attributes(G, 'centrality')\n",
    "    \n",
    "    # Draw nodes colored by community\n",
    "    for i, (community_id, nodes) in enumerate(communities.items()):\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=nodes, \n",
    "                              node_size=[centrality.get(node, 0.1) * 2500 for node in nodes],\n",
    "                              node_color=[cmap(i)], \n",
    "                              label=f'Community {community_id}')\n",
    "    \n",
    "    # Draw edges with varying thickness\n",
    "    for (u, v, data) in G.edges(data=True):\n",
    "        width = data['weight'] * 1.5 / max([G[u][v]['weight'] for u, v in G.edges()])\n",
    "        alpha = 0.2 + (0.6 * data['weight'] / max([G[u][v]['weight'] for u, v in G.edges()]))\n",
    "        \n",
    "        # If nodes are in the same community, use community color\n",
    "        if partition[u] == partition[v]:\n",
    "            edge_color = cmap(partition[u])\n",
    "        else:\n",
    "            edge_color = 'gray'\n",
    "            \n",
    "        nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], width=width, alpha=alpha, \n",
    "                              edge_color=edge_color)\n",
    "    \n",
    "    # Draw labels with appropriate sizes based on node type\n",
    "    keyword_labels = {node: node for node in keywords if node in G.nodes()}\n",
    "    entity_labels = {node: node for node in top_entities if node in G.nodes()}\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, labels=keyword_labels, font_size=12, font_weight='bold')\n",
    "    nx.draw_networkx_labels(G, pos, labels=entity_labels, font_size=10)\n",
    "    \n",
    "    plt.title(f\"Community Structure in Keyword-Entity Network (Detected: {num_communities} communities)\")\n",
    "    plt.axis('off')\n",
    "    plt.legend(scatterpoints=1, loc='upper right')\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, f\"{filename}.png\", subject_main_dir, subject_type)\n",
    "    \n",
    "    # Create community composition analysis plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    community_sizes = [len(nodes) for nodes in communities.values()]\n",
    "    \n",
    "    # Count keyword and entity nodes in each community\n",
    "    keyword_counts = []\n",
    "    entity_counts = []\n",
    "    \n",
    "    for community_nodes in communities.values():\n",
    "        keyword_count = sum(1 for node in community_nodes if node in keywords)\n",
    "        entity_count = sum(1 for node in community_nodes if node in top_entities)\n",
    "        keyword_counts.append(keyword_count)\n",
    "        entity_counts.append(entity_count)\n",
    "    \n",
    "    # Create a stacked bar chart\n",
    "    community_ids = list(communities.keys())\n",
    "    \n",
    "    plt.bar(community_ids, keyword_counts, label='Keywords')\n",
    "    plt.bar(community_ids, entity_counts, bottom=keyword_counts, label='Entities')\n",
    "    \n",
    "    plt.xlabel('Community ID')\n",
    "    plt.ylabel('Number of Nodes')\n",
    "    plt.title('Composition of Communities')\n",
    "    plt.legend()\n",
    "    plt.xticks(community_ids)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the composition plot\n",
    "    save_plot(plt, f\"{filename}_composition.png\", subject_main_dir, subject_type)\n",
    "\n",
    "def visualize_cliques(G, keywords, top_entities, filename, subject_main_dir, subject_type):\n",
    "    \"\"\"Find and visualize cliques in the network\"\"\"\n",
    "    # Find all maximal cliques\n",
    "    cliques = list(nx.find_cliques(G))\n",
    "    \n",
    "    # Filter for cliques with at least 3 nodes\n",
    "    significant_cliques = [c for c in cliques if len(c) >= 3]\n",
    "    \n",
    "    if not significant_cliques:\n",
    "        print(\"No significant cliques (size >= 3) found in the network\")\n",
    "        return\n",
    "    \n",
    "    # Sort cliques by size (largest first)\n",
    "    significant_cliques.sort(key=len, reverse=True)\n",
    "    \n",
    "    # Limit to top 6 largest cliques for visualization\n",
    "    cliques_to_plot = significant_cliques[:min(6, len(significant_cliques))]\n",
    "    num_cliques = len(cliques_to_plot)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    if num_cliques <= 3:\n",
    "        rows, cols = 1, num_cliques\n",
    "    else:\n",
    "        rows, cols = 2, 3\n",
    "    \n",
    "    # Set node positions for the full graph (to keep consistent positioning)\n",
    "    pos = nx.spring_layout(G, k=0.3, seed=42)\n",
    "    \n",
    "    # Create a colormap\n",
    "    cmap = plt.cm.get_cmap('tab10', num_cliques)\n",
    "    \n",
    "    # Iterate through cliques to plot each one\n",
    "    for i, clique in enumerate(cliques_to_plot):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        \n",
    "        # Create subgraph for this clique\n",
    "        subgraph = G.subgraph(clique)\n",
    "        \n",
    "        # Get centrality for node sizes\n",
    "        centrality = nx.degree_centrality(subgraph)\n",
    "        \n",
    "        # Identify node types in this clique\n",
    "        keyword_nodes = [node for node in clique if node in keywords]\n",
    "        entity_nodes = [node for node in clique if node in top_entities]\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(subgraph, pos, nodelist=keyword_nodes, \n",
    "                              node_size=[centrality.get(node, 0.1) * 2000 for node in keyword_nodes],\n",
    "                              node_color='red', \n",
    "                              node_shape='o', \n",
    "                              alpha=0.8)\n",
    "        \n",
    "        nx.draw_networkx_nodes(subgraph, pos, nodelist=entity_nodes, \n",
    "                              node_size=[centrality.get(node, 0.1) * 1500 for node in entity_nodes],\n",
    "                              node_color='blue', \n",
    "                              node_shape='s', \n",
    "                              alpha=0.6)\n",
    "        \n",
    "        # Draw edges\n",
    "        edge_weights = [subgraph[u][v]['weight'] for u, v in subgraph.edges()]\n",
    "        max_weight = max(edge_weights) if edge_weights else 1\n",
    "        \n",
    "        for (u, v, data) in subgraph.edges(data=True):\n",
    "            width = data['weight'] * 2 / max_weight\n",
    "            alpha = 0.4 + (0.6 * data['weight'] / max_weight)\n",
    "            nx.draw_networkx_edges(subgraph, pos, edgelist=[(u, v)], width=width, alpha=alpha, \n",
    "                                  edge_color='gray')\n",
    "        \n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(subgraph, pos, font_size=9, font_weight='bold')\n",
    "        \n",
    "        plt.title(f\"Clique {i+1}: {len(clique)} nodes\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Top {num_cliques} Cliques in the Network\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for the suptitle\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, f\"{filename}.png\", subject_main_dir, subject_type)\n",
    "    \n",
    "    # Create clique composition summary\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    clique_sizes = [len(c) for c in cliques_to_plot]\n",
    "    clique_labels = [f\"Clique {i+1}\" for i in range(num_cliques)]\n",
    "    \n",
    "    # Count keywords and entities in each clique\n",
    "    keyword_counts = []\n",
    "    entity_counts = []\n",
    "    \n",
    "    for clique in cliques_to_plot:\n",
    "        keyword_count = sum(1 for node in clique if node in keywords)\n",
    "        entity_count = sum(1 for node in clique if node in top_entities)\n",
    "        keyword_counts.append(keyword_count)\n",
    "        entity_counts.append(entity_count)\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    plt.bar(clique_labels, keyword_counts, label='Keywords')\n",
    "    plt.bar(clique_labels, entity_counts, bottom=keyword_counts, label='Entities')\n",
    "    \n",
    "    plt.xlabel('Clique')\n",
    "    plt.ylabel('Number of Nodes')\n",
    "    plt.title('Composition of Top Cliques')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the composition plot\n",
    "    save_plot(plt, f\"{filename}_composition.png\", subject_main_dir, subject_type)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Define keywords\n",
    "keywords = ['antisemitism', 'antizionist', 'anti-zionism', 'jew', 'jews','jewish','holocaust', 'nazi', 'nazism']\n",
    "\n",
    "# Visualize network\n",
    "G = visualize_keyword_network(\n",
    "    df=df,\n",
    "    keywords=keywords,\n",
    "    title_column=\"Title\",\n",
    "    list_column=\"Topics\",  # Column containing lists of strings\n",
    "    top_n=15,               # Top 15 most common authors\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    subject_type=\"keyword_netword\",\n",
    "    filename_prefix=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22564b72-22ea-4073-a1c6-ade64d180a81",
   "metadata": {},
   "source": [
    "# Titles Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8586c0e6-b08c-4523-9b29-cca96d60022f",
   "metadata": {},
   "source": [
    "## Title general analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34288cae-8b6f-4496-9735-5f37de173199",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, processed_df=analyze_text_column(df, \"Title\", subject_main_dir,\n",
    "                                          proc_df_out_path=filtered_titles_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58132da-f72d-4394-a570-e3f1048a9cda",
   "metadata": {},
   "source": [
    "## Title Entity recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63fb269-a8c5-48c5-84f5-745d0cc373cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_entity_analysis = analyze_entities_in_column(\n",
    "    df=processed_df,\n",
    "    column_name='Title',\n",
    "    subject_main_dir=subject_main_dir, \n",
    "    proc_df_out_path=filtered_titles_df_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16403543-ee45-4c38-97e4-2e1429bf483e",
   "metadata": {},
   "source": [
    "## Title Bert-Topic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb49a0-a198-4b38-8a5e-3cd162b4e3e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_df, lstat=preprocess_dataframe_text_col(df, text_column=\"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3dae1-f6c4-4019-ac71-aa5262d551d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_topic_modeling(processed_df, \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd29871-fe91-4198-a74c-2f94a5adb0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertopic_results = run_bertopic_analysis(\n",
    "    df=processed_df,                          # Your dataframe\n",
    "    column_name=\"Title\",         # Text column to analyze\n",
    "    decade_column=\"decade\",         # Column with decade information (for trend analysis)\n",
    "    subject_main_dir=subject_main_dir, # Main directory for saving results\n",
    "    language=\"english\",             # Text language\n",
    "    nr_topics=15,                   # Number of topics (or \"auto\")\n",
    "    min_topic_size=10,              # Min documents per topic\n",
    "    top_n_topics=8                  # Number of top topics to visualize\n",
    ")\n",
    "\n",
    "# Check for errors\n",
    "if \"error\" in bertopic_results:\n",
    "    print(f\"Error: {bertopic_results['error']}\")\n",
    "else:\n",
    "    print(f\"Analysis complete with {bertopic_results['topic_count']} topics\")\n",
    "    print(\"Created visualizations:\")\n",
    "    for viz in bertopic_results['visualizations']:\n",
    "        print(f\"- {viz}\")\n",
    "    \n",
    "    # Access the processed dataframe with topic assignments\n",
    "    topic_df = bertopic_results['processed_df']\n",
    "    print(f\"Documents with topic assignments: {len(topic_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d615257-8d61-402f-ba45-054dea122e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example of analyzing document titles with BERTopic using decade-based temporal analysis\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def analyze_column_by_decade(df, column_name='Title', output_dir='results/title_analysis'):\n",
    "    \"\"\"\n",
    "    Analyze document titles with BERTopic, including decade-based temporal analysis\n",
    "    \n",
    "    Parameters:\n",
    "    df_path (str): Path to the CSV file containing the data\n",
    "    output_dir (str): Directory where analysis results will be saved\n",
    "    \n",
    "    Returns:\n",
    "    dict: Analysis results\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    print(f\"Starting title analysis with {len(df)} documents...\")\n",
    "    \n",
    "    # Run comprehensive BERTopic analysis\n",
    "    results = run_comprehensive_bertopic_analysis(\n",
    "        df=df,\n",
    "        column_name=column_name,            # Analyze the Title column\n",
    "        time_column='decade',           # Use decade column for temporal analysis\n",
    "        class_column='Type',    # Optional: if you have a document type column\n",
    "        subject_main_dir=output_dir,    # Output directory\n",
    "        language='english',             # Language for stopwords\n",
    "        nr_topics='auto',               # Let BERTopic determine the optimal number of topics\n",
    "        min_topic_size=15,              # Minimum size for each topic\n",
    "        top_n_topics=15,                # Show top 15 topics in visualizations\n",
    "        visualize_documents=True,       # Create document-topic visualizations\n",
    "        sample_documents=1000           # Sample 1000 documents for visualizations\n",
    "    )\n",
    "    \n",
    "    if \"error\" in results:\n",
    "        print(f\"Error during analysis: {results['error']}\")\n",
    "        return results\n",
    "    \n",
    "    print(f\"Analysis complete! Found {results['topic_count']} topics.\")\n",
    "    print(f\"Summary visualization available at: {results['summary']}\")\n",
    "    \n",
    "    # Return results for further analysis if needed\n",
    "    return results\n",
    "\n",
    "def analyze_multiple_columns(df, columns=['Title', 'Abstract'], output_dir='results/topic_analysis'):\n",
    "    \"\"\"\n",
    "    Analyze multiple text columns with BERTopic\n",
    "    \n",
    "    Parameters:\n",
    "    df_path (str): Path to the CSV file containing the data\n",
    "    columns (list): List of column names to analyze\n",
    "    output_dir (str): Directory where analysis results will be saved\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary of analysis results for each column\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {df_path}...\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    # Store results for each column\n",
    "    all_results = {}\n",
    "    \n",
    "    for column in columns:\n",
    "        print(f\"\\n--- Starting analysis of {column} column ---\")\n",
    "        \n",
    "        # Define column-specific output directory\n",
    "        column_output_dir = f\"{output_dir}/{column.lower()}\"\n",
    "        \n",
    "        # Run analysis for this column\n",
    "        all_results[column] = run_comprehensive_bertopic_analysis(\n",
    "            df=df,\n",
    "            column_name=column,           # Analyze this specific column\n",
    "            time_column='decade',         # Use decade column for temporal analysis\n",
    "            class_column='Type',  # Optional: if you have a document type column\n",
    "            subject_main_dir=column_output_dir,\n",
    "            language='english',\n",
    "            nr_topics='auto',\n",
    "            min_topic_size=15,\n",
    "            top_n_topics=15\n",
    "        )\n",
    "        \n",
    "        if \"error\" in all_results[column]:\n",
    "            print(f\"Error analyzing {column}: {all_results[column]['error']}\")\n",
    "        else:\n",
    "            print(f\"Analysis of {column} complete! Found {all_results[column]['topic_count']} topics.\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    abs_processed_df = pd.read_csv('datasets/filtered_abstracts_dataset.csv')\n",
    "    # Analyze just titles\n",
    "    abs_results = analyze_column_by_decade(abs_processed_df, column_name='Abstract')\n",
    "    \n",
    "    # Or analyze multiple columns\n",
    "    # all_results = analyze_multiple_columns(df_path, columns=['Title', 'Abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7df10-d5c1-485d-905b-d50602d94a86",
   "metadata": {},
   "source": [
    "## Title semantic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f694b-70a9-4a03-9827-573ca50e662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = run_sentiment_analysis(df, \"Title\", max_samples=1000, model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a55ec1f-0f04-4b5f-b489-27766321bd9e",
   "metadata": {},
   "source": [
    "# Abstract analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be2f22-e6d9-490e-8304-86c51fde13a1",
   "metadata": {},
   "source": [
    "## General anlysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7475a3b2-068f-41d1-902b-f22dbb337b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, abs_processed_df=analyze_text_column(df, \"Abstract\", subject_main_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca1e25e-163d-4089-8b23-68106b48ac8f",
   "metadata": {},
   "source": [
    "## Abstract Entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e33faa-6936-47bc-b025-502ef08f0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_entity_analysis = analyze_entities_in_column(\n",
    "    df=abs_processed_df,\n",
    "    column_name='Abstract',\n",
    "    subject_main_dir=subject_main_dir, \n",
    "    proc_df_out_path=filtered_abstracts_df_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34900e0-5ecc-4acb-ab95-328bc1f13d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertopic_results = run_bertopic_analysis(\n",
    "    df=abs_processed_df,                          # Your dataframe\n",
    "    column_name=\"Abstract\",         # Text column to analyze\n",
    "    decade_column=\"decade\",         # Column with decade information (for trend analysis)\n",
    "    subject_main_dir=subject_main_dir, # Main directory for saving results\n",
    "    language=\"english\",             # Text language\n",
    "    nr_topics=15,                   # Number of topics (or \"auto\")\n",
    "    min_topic_size=10,              # Min documents per topic\n",
    "    top_n_topics=8                  # Number of top topics to visualize\n",
    ")\n",
    "\n",
    "# Check for errors\n",
    "if \"error\" in bertopic_results:\n",
    "    print(f\"Error: {bertopic_results['error']}\")\n",
    "else:\n",
    "    print(f\"Analysis complete with {bertopic_results['topic_count']} topics\")\n",
    "    print(\"Created visualizations:\")\n",
    "    for viz in bertopic_results['visualizations']:\n",
    "        print(f\"- {viz}\")\n",
    "    \n",
    "    # Access the processed dataframe with topic assignments\n",
    "    topic_df = bertopic_results['processed_df']\n",
    "    print(f\"Documents with topic assignments: {len(topic_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e943df-c987-4317-921a-7d1d9e2529d9",
   "metadata": {},
   "source": [
    "## Abstract BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e39a1bc-c6a8-476a-8d4c-2146afaf6f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertopic_results = run_bertopic_analysis(\n",
    "    df=processed_df,                          # Your dataframe\n",
    "    column_name=\"Abstract\",         # Text column to analyze\n",
    "    decade_column=\"decade\",         # Column with decade information (for trend analysis)\n",
    "    subject_main_dir=subject_main_dir, # Main directory for saving results\n",
    "    language=\"english\",             # Text language\n",
    "    nr_topics=15,                   # Number of topics (or \"auto\")\n",
    "    min_topic_size=10,              # Min documents per topic\n",
    "    top_n_topics=8                  # Number of top topics to visualize\n",
    ")\n",
    "\n",
    "# Check for errors\n",
    "if \"error\" in bertopic_results:\n",
    "    print(f\"Error: {bertopic_results['error']}\")\n",
    "else:\n",
    "    print(f\"Analysis complete with {bertopic_results['topic_count']} topics\")\n",
    "    print(\"Created visualizations:\")\n",
    "    for viz in bertopic_results['visualizations']:\n",
    "        print(f\"- {viz}\")\n",
    "    \n",
    "    # Access the processed dataframe with topic assignments\n",
    "    topic_df = bertopic_results['processed_df']\n",
    "    print(f\"Documents with topic assignments: {len(topic_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed900a-6bdc-4ece-ab1c-b47b60955b81",
   "metadata": {},
   "source": [
    "## Abstract semantic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac036d48-ec65-4d8f-b71f-c9408d10a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = run_sentiment_analysis(df, \"Abstract\", max_samples=1000)\n",
    "sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f6f94-8195-45a6-a7f3-b696e71f90bf",
   "metadata": {},
   "source": [
    "# Categorical column analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582229a0-32b8-4fd9-b518-a1e47f4bdf46",
   "metadata": {},
   "source": [
    "## Categorical utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a6ecd-3690-4f3b-9194-f2ce8936e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_categorical(df, column_name):\n",
    "    \"\"\"\n",
    "    Preprocess a categorical column by removing NA values and printing statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe containing the data\n",
    "    column_name (str): The name of the categorical column to process\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A new dataframe with NA values removed for the specified column\n",
    "    \"\"\"\n",
    "    # Record the original number of records\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Drop rows where the specified column is NA\n",
    "    processed_df = processed_df.dropna(subset=[column_name])\n",
    "    \n",
    "    # Record the new count\n",
    "    new_count = len(processed_df)\n",
    "    \n",
    "    # Calculate and print statistics\n",
    "    dropped_count = original_count - new_count\n",
    "    dropped_percentage = (dropped_count / original_count) * 100 if original_count > 0 else 0\n",
    "    \n",
    "    print(f\"Preprocessing statistics for column '{column_name}':\")\n",
    "    print(f\"- Original record count: {original_count}\")\n",
    "    print(f\"- Records after dropping NA values: {new_count}\")\n",
    "    print(f\"- Dropped records: {dropped_count} ({dropped_percentage:.2f}%)\")\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d95a73-2342-46ed-87ca-1805e2b04318",
   "metadata": {},
   "source": [
    "## Linear trends over decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7353d-a568-43d2-a1bd-9d0e8245fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_categorical_by_decade_percentage(df, column_name, decade_column, subject_main_dir, top_n=10, title_prefix=\"Distribution of\"):\n",
    "    \"\"\"\n",
    "    Visualize the percentage distribution of categorical values across decades.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe containing the data\n",
    "    column_name (str): Name of the categorical column to analyze\n",
    "    decade_column (str): Name of the column containing decade information\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis (e.g., 'domains', 'fields')\n",
    "    top_n (int): Number of top categories to display (others will be grouped)\n",
    "    title_prefix (str): Prefix for the plot title\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: The created figure\n",
    "    \"\"\"\n",
    "    subject_type=column_name.lower()\n",
    "    \n",
    "    # Preprocess data\n",
    "    processed_df = preprocess_categorical(df, column_name)\n",
    "    \n",
    "    # Get value counts for the top N categories\n",
    "    top_categories = processed_df[column_name].value_counts().nlargest(top_n).index.tolist()\n",
    "    \n",
    "    # Create a copy of the dataframe with 'Other' for categories not in top N\n",
    "    plot_df = processed_df.copy()\n",
    "    plot_df.loc[~plot_df[column_name].isin(top_categories), column_name] = 'Other'\n",
    "    \n",
    "    # Group by decade and category, calculate percentages\n",
    "    decade_category_counts = plot_df.groupby([decade_column, column_name]).size().unstack(fill_value=0)\n",
    "    decade_percentages = decade_category_counts.div(decade_category_counts.sum(axis=1), axis=0)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot each category as a line\n",
    "    for category in decade_percentages.columns:\n",
    "        if category in top_categories:  # Skip 'Other' if we want to focus on top categories\n",
    "            plt.plot(decade_percentages.index, decade_percentages[category], marker='o', label=category)\n",
    "    \n",
    "    # Configure the plot\n",
    "    plt.title(f\"{title_prefix} {column_name} Over Time\", fontsize=14)\n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel(f'Percentage of {column_name}', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"{column_name}_percentage_by_decade.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def visualize_categorical_by_decade_counts(df, column_name, decade_column, subject_main_dir, top_n=10, title_prefix=\"Frequency of\"):\n",
    "    \"\"\"\n",
    "    Visualize the count distribution of categorical values across decades.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe containing the data\n",
    "    column_name (str): Name of the categorical column to analyze\n",
    "    decade_column (str): Name of the column containing decade information\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis (e.g., 'domains', 'fields')\n",
    "    top_n (int): Number of top categories to display (others will be grouped)\n",
    "    title_prefix (str): Prefix for the plot title\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: The created figure\n",
    "    \"\"\"\n",
    "    subject_type=column_name.lower()\n",
    "    # Preprocess data\n",
    "    processed_df = preprocess_categorical(df, column_name)\n",
    "    \n",
    "    # Get value counts for the top N categories\n",
    "    top_categories = processed_df[column_name].value_counts().nlargest(top_n).index.tolist()\n",
    "    \n",
    "    # Create a copy of the dataframe with 'Other' for categories not in top N\n",
    "    plot_df = processed_df.copy()\n",
    "    plot_df.loc[~plot_df[column_name].isin(top_categories), column_name] = 'Other'\n",
    "    \n",
    "    # Group by decade and category, get counts\n",
    "    decade_category_counts = plot_df.groupby([decade_column, column_name]).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot each category as a line\n",
    "    for category in decade_category_counts.columns:\n",
    "        if category in top_categories:  # Skip 'Other' if we want to focus on top categories\n",
    "            plt.plot(decade_category_counts.index, decade_category_counts[category], marker='o', label=category)\n",
    "    \n",
    "    # Configure the plot\n",
    "    plt.title(f\"{title_prefix} {column_name} Over Time\", fontsize=14)\n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel(f'Number of Records with {column_name}', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"{column_name}_counts_by_decade.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35ea0c-7866-48b6-be3d-b76b44a64150",
   "metadata": {},
   "source": [
    "### Peaks analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e611f-581e-4dd7-8052-8935b27b1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_peak_years(df, column_name, year_column, decade_column=None, focus_period=None):\n",
    "    \"\"\"\n",
    "    Identify the years where each category in a column reaches its maximum value.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe containing the data\n",
    "    column_name (str): Name of the categorical column to analyze\n",
    "    year_column (str): Name of the column containing year information\n",
    "    decade_column (str, optional): Name of decade column if available\n",
    "    focus_period (tuple, optional): Tuple of (start_year, end_year) to focus analysis\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with categories as keys and information about their peaks as values\n",
    "    pandas.DataFrame: Summary DataFrame of peak information\n",
    "    \"\"\"\n",
    "    # Preprocess data\n",
    "    processed_df = preprocess_categorical(df, column_name)\n",
    "    \n",
    "    # Filter by focus period if provided\n",
    "    if focus_period:\n",
    "        start_year, end_year = focus_period\n",
    "        processed_df = processed_df[\n",
    "            (processed_df[year_column] >= start_year) & \n",
    "            (processed_df[year_column] <= end_year)\n",
    "        ]\n",
    "        print(f\"Focusing on period: {start_year} to {end_year}\")\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    peak_info = {}\n",
    "    \n",
    "    # Get unique categories\n",
    "    categories = processed_df[column_name].unique()\n",
    "    \n",
    "    # For each category, find the year with maximum count\n",
    "    for category in categories:\n",
    "        category_data = processed_df[processed_df[column_name] == category]\n",
    "        \n",
    "        # Group by year and count records\n",
    "        yearly_counts = category_data.groupby(year_column).size()\n",
    "        \n",
    "        if not yearly_counts.empty:\n",
    "            # Find the year(s) with maximum count\n",
    "            max_count = yearly_counts.max()\n",
    "            max_years = yearly_counts[yearly_counts == max_count].index.tolist()\n",
    "            \n",
    "            # Store the information\n",
    "            peak_info[category] = {\n",
    "                'peak_years': max_years,\n",
    "                'peak_count': max_count,\n",
    "                'total_records': len(category_data)\n",
    "            }\n",
    "    \n",
    "    # Create a summary DataFrame\n",
    "    summary_data = []\n",
    "    for category, info in peak_info.items():\n",
    "        summary_data.append({\n",
    "            'Category': category,\n",
    "            'Peak_Year(s)': ', '.join(map(str, info['peak_years'])),\n",
    "            'Peak_Count': info['peak_count'],\n",
    "            'Total_Records': info['total_records'],\n",
    "            'Percentage_at_Peak': (info['peak_count'] / info['total_records']) * 100 if info['total_records'] > 0 else 0\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df = summary_df.sort_values('Peak_Count', ascending=False)\n",
    "    \n",
    "    return peak_info, summary_df\n",
    "\n",
    "def visualize_peak_distribution(peak_info, year_column, subject_main_dir, subject_type, focus_period=None):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of peak years across categories.\n",
    "    \n",
    "    Parameters:\n",
    "    peak_info (dict): Dictionary with peak information from identify_peak_years\n",
    "    year_column (str): Name of the column containing year information \n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    focus_period (tuple, optional): Tuple of (start_year, end_year) of focus period\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: The created figure\n",
    "    \"\"\"\n",
    "    # Extract all peak years with their frequencies\n",
    "    all_peak_years = []\n",
    "    for category, info in peak_info.items():\n",
    "        all_peak_years.extend(info['peak_years'])\n",
    "    \n",
    "    # Count frequency of each peak year\n",
    "    year_counts = pd.Series(all_peak_years).value_counts().sort_index()\n",
    "    \n",
    "    # Create the histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(year_counts.index, year_counts.values, color='steelblue', alpha=0.7)\n",
    "    \n",
    "    # Add data labels on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "    # Configure the plot\n",
    "    title = \"Distribution of Peak Years Across Categories\"\n",
    "    if focus_period:\n",
    "        title += f\" ({focus_period[0]}-{focus_period[1]})\"\n",
    "        \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.ylabel('Number of Categories with Peak', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"peak_years_distribution_{year_column}.png\"\n",
    "    if focus_period:\n",
    "        filename = f\"peak_years_distribution_{focus_period[0]}_{focus_period[1]}.png\"\n",
    "        \n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad865ab-0869-4846-92ee-54d4539815f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_peaks(df, column_name, year_column, subject_main_dir, focus_period=None):\n",
    "    \"\"\"\n",
    "    Analyze when categories reach their peak values and visualize the results.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe containing the data\n",
    "    column_name (str): Name of the categorical column to analyze\n",
    "    year_column (str): Name of the column containing year information\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    focus_period (tuple, optional): Tuple of (start_year, end_year) to focus analysis\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Summary of peak information\n",
    "    \"\"\"\n",
    "    subject_type=column_name.lower()\n",
    "    # Identify peaks for each category\n",
    "    peak_info, summary_df = identify_peak_years(\n",
    "        df, \n",
    "        column_name, \n",
    "        year_column, \n",
    "        focus_period=focus_period\n",
    "    )\n",
    "    \n",
    "    # Print summary information\n",
    "    print(f\"\\nPeak Analysis for {column_name}:\")\n",
    "    print(f\"Total categories analyzed: {len(peak_info)}\")\n",
    "    \n",
    "    if focus_period:\n",
    "        print(f\"Focus period: {focus_period[0]} to {focus_period[1]}\")\n",
    "    \n",
    "    # Display the top categories by peak count\n",
    "    print(\"\\nTop categories by peak count:\")\n",
    "    display_df = summary_df.head(10)  # Show top 10\n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Visualize the distribution of peak years\n",
    "    visualize_peak_distribution(\n",
    "        peak_info, \n",
    "        year_column, \n",
    "        subject_main_dir, \n",
    "        subject_type,\n",
    "        focus_period\n",
    "    )\n",
    "    \n",
    "    # Save the summary to CSV\n",
    "    csv_filename = f\"{column_name}_peak_analysis.csv\"\n",
    "    csv_path = Path(subject_main_dir) / subject_type / csv_filename\n",
    "    summary_df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nSummary saved to: {csv_path}\")\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ebe3d-31d7-4353-9651-01f4d2471485",
   "metadata": {},
   "source": [
    "#### Peak analysis - top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db31b459-2b93-4cf7-9e3f-faffad226b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_peak_years_top_categories(df, column_name, year_column, top_n=10, focus_period=None):\n",
    "    \"\"\"\n",
    "    Identify the years where the top N categories in a column reach their maximum values.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe containing the data\n",
    "    column_name (str): Name of the categorical column to analyze\n",
    "    year_column (str): Name of the column containing year information\n",
    "    top_n (int): Number of top categories to analyze\n",
    "    focus_period (tuple, optional): Tuple of (start_year, end_year) to focus analysis\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with categories as keys and information about their peaks as values\n",
    "    pandas.DataFrame: Summary DataFrame of peak information\n",
    "    list: List of top categories analyzed\n",
    "    \"\"\"\n",
    "    # Set subject_type based on column_name\n",
    "    subject_type = column_name.lower()\n",
    "    \n",
    "    # Preprocess data\n",
    "    processed_df = preprocess_categorical(df, column_name)\n",
    "    \n",
    "    # Filter by focus period if provided\n",
    "    if focus_period:\n",
    "        start_year, end_year = focus_period\n",
    "        processed_df = processed_df[\n",
    "            (processed_df[year_column] >= start_year) & \n",
    "            (processed_df[year_column] <= end_year)\n",
    "        ]\n",
    "        print(f\"Focusing on period: {start_year} to {end_year}\")\n",
    "    \n",
    "    # Get the top N categories\n",
    "    top_categories = processed_df[column_name].value_counts().nlargest(top_n).index.tolist()\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    peak_info = {}\n",
    "    \n",
    "    # For each top category, find the year with maximum count\n",
    "    for category in top_categories:\n",
    "        category_data = processed_df[processed_df[column_name] == category]\n",
    "        \n",
    "        # Group by year and count records\n",
    "        yearly_counts = category_data.groupby(year_column).size()\n",
    "        \n",
    "        if not yearly_counts.empty:\n",
    "            # Find the year(s) with maximum count\n",
    "            max_count = yearly_counts.max()\n",
    "            max_years = yearly_counts[yearly_counts == max_count].index.tolist()\n",
    "            \n",
    "            # Store the information\n",
    "            peak_info[category] = {\n",
    "                'peak_years': max_years,\n",
    "                'peak_count': max_count,\n",
    "                'yearly_counts': yearly_counts,\n",
    "                'total_records': len(category_data)\n",
    "            }\n",
    "    \n",
    "    # Create a summary DataFrame\n",
    "    summary_data = []\n",
    "    for category, info in peak_info.items():\n",
    "        summary_data.append({\n",
    "            f'{column_name}': category,  # Use column_name as the column header\n",
    "            'Peak_Year(s)': ', '.join(map(str, info['peak_years'])),\n",
    "            'Peak_Count': info['peak_count'],\n",
    "            'Total_Records': info['total_records'],\n",
    "            'Percentage_at_Peak': (info['peak_count'] / info['total_records']) * 100 if info['total_records'] > 0 else 0\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df = summary_df.sort_values('Peak_Count', ascending=False)\n",
    "    \n",
    "    return peak_info, summary_df, top_categories\n",
    "\n",
    "def visualize_peak_distribution_top_categories(peak_info, column_name, year_column, subject_main_dir, focus_period=None):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of peak years across top categories.\n",
    "    \n",
    "    Parameters:\n",
    "    peak_info (dict): Dictionary with peak information from identify_peak_years_top_categories\n",
    "    column_name (str): Name of the categorical column analyzed\n",
    "    year_column (str): Name of the column containing year information \n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    focus_period (tuple, optional): Tuple of (start_year, end_year) of focus period\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: The created figure\n",
    "    \"\"\"\n",
    "    # Set subject_type based on column_name\n",
    "    subject_type = column_name.lower()\n",
    "    \n",
    "    # Capital case the first letter for display\n",
    "    display_name = column_name[0].upper() + column_name[1:]\n",
    "    \n",
    "    # Extract all peak years with their frequencies\n",
    "    all_peak_years = []\n",
    "    for category, info in peak_info.items():\n",
    "        all_peak_years.extend(info['peak_years'])\n",
    "    \n",
    "    # Count frequency of each peak year\n",
    "    year_counts = pd.Series(all_peak_years).value_counts().sort_index()\n",
    "    \n",
    "    # Create the histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(year_counts.index, year_counts.values, color='steelblue', alpha=0.7)\n",
    "    \n",
    "    # Add data labels on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "    # Configure the plot\n",
    "    title = f\"Distribution of Peak Years Across Top {display_name}\"\n",
    "    if focus_period:\n",
    "        title += f\" ({focus_period[0]}-{focus_period[1]})\"\n",
    "        \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.ylabel(f'Number of {display_name} with Peak', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"peak_years_distribution_top_{column_name}.png\"\n",
    "    if focus_period:\n",
    "        filename = f\"peak_years_distribution_top_{column_name}_{focus_period[0]}_{focus_period[1]}.png\"\n",
    "        \n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def visualize_peak_trends_top_categories(peak_info, column_name, year_column, subject_main_dir, plot_type='counts', focus_period=None):\n",
    "    \"\"\"\n",
    "    Visualize trends over time for top categories with their peak years highlighted.\n",
    "    \n",
    "    Parameters:\n",
    "    peak_info (dict): Dictionary with peak information from identify_peak_years_top_categories\n",
    "    column_name (str): Name of the categorical column analyzed\n",
    "    year_column (str): Name of the column containing year information\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    plot_type (str): Type of plot - 'counts' or 'percentage'\n",
    "    focus_period (tuple, optional): Tuple of (start_year, end_year) of focus period\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: The created figure\n",
    "    \"\"\"\n",
    "    # Set subject_type based on column_name\n",
    "    subject_type = column_name.lower()\n",
    "    \n",
    "    # Capital case the first letter for display\n",
    "    display_name = column_name[0].upper() + column_name[1:]\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Get all unique years across all categories\n",
    "    all_years = set()\n",
    "    for category, info in peak_info.items():\n",
    "        all_years.update(info['yearly_counts'].index)\n",
    "    all_years = sorted(list(all_years))\n",
    "    \n",
    "    # If plotting percentages, we need total counts per year\n",
    "    if plot_type == 'percentage':\n",
    "        total_by_year = {year: 0 for year in all_years}\n",
    "        for category, info in peak_info.items():\n",
    "            for year in info['yearly_counts'].index:\n",
    "                total_by_year[year] += info['yearly_counts'][year]\n",
    "    \n",
    "    # Plot each category\n",
    "    for category, info in peak_info.items():\n",
    "        years = info['yearly_counts'].index.tolist()\n",
    "        \n",
    "        if plot_type == 'counts':\n",
    "            values = info['yearly_counts'].values\n",
    "            plt.plot(years, values, marker='o', label=category)\n",
    "            \n",
    "            # Highlight peak point(s)\n",
    "            for peak_year in info['peak_years']:\n",
    "                peak_value = info['yearly_counts'][peak_year]\n",
    "                plt.plot(peak_year, peak_value, 'o', markersize=10, \n",
    "                         markerfacecolor='none', markeredgecolor='red', markeredgewidth=2)\n",
    "        \n",
    "        else:  # percentage\n",
    "            percentages = [info['yearly_counts'][year] / total_by_year[year] * 100 \n",
    "                          if total_by_year[year] > 0 else 0 \n",
    "                          for year in years]\n",
    "            plt.plot(years, percentages, marker='o', label=category)\n",
    "            \n",
    "            # Highlight peak point(s)\n",
    "            for peak_year in info['peak_years']:\n",
    "                if peak_year in years:\n",
    "                    idx = years.index(peak_year)\n",
    "                    peak_value = percentages[idx]\n",
    "                    plt.plot(peak_year, peak_value, 'o', markersize=10, \n",
    "                             markerfacecolor='none', markeredgecolor='red', markeredgewidth=2)\n",
    "    \n",
    "    # Configure the plot\n",
    "    y_label = f\"Number of Records with {display_name}\" if plot_type == 'counts' else f\"Percentage of {display_name}\"\n",
    "    title_prefix = f\"Frequency of {display_name}\" if plot_type == 'counts' else f\"Distribution of {display_name}\"\n",
    "    \n",
    "    title = f\"{title_prefix} Over Time with Peak Years\"\n",
    "    if focus_period:\n",
    "        title += f\" ({focus_period[0]}-{focus_period[1]})\"\n",
    "    \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.ylabel(y_label, fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_type_str = 'counts' if plot_type == 'counts' else 'percentage'\n",
    "    filename = f\"{column_name}_peaks_{plot_type_str}.png\"\n",
    "    if focus_period:\n",
    "        filename = f\"{column_name}_peaks_{plot_type_str}_{focus_period[0]}_{focus_period[1]}.png\"\n",
    "    \n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def analyze_top_categorical_peaks(df, column_name, year_column, subject_main_dir, top_n=10, focus_period=None):\n",
    "    \"\"\"\n",
    "    Analyze when top categories reach their peak values and visualize the results.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe containing the data\n",
    "    column_name (str): Name of the categorical column to analyze\n",
    "    year_column (str): Name of the column containing year information\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    top_n (int): Number of top categories to analyze\n",
    "    focus_period (tuple, optional): Tuple of (start_year, end_year) to focus analysis\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Summary of peak information\n",
    "    \"\"\"\n",
    "    # Set subject_type based on column_name\n",
    "    subject_type = column_name.lower()\n",
    "    \n",
    "    # Capital case the first letter for display\n",
    "    display_name = column_name[0].upper() + column_name[1:]\n",
    "    \n",
    "    # Identify peaks for the top categories\n",
    "    peak_info, summary_df, top_categories = identify_peak_years_top_categories(\n",
    "        df, \n",
    "        column_name, \n",
    "        year_column,\n",
    "        top_n=top_n,\n",
    "        focus_period=focus_period\n",
    "    )\n",
    "    \n",
    "    # Print summary information\n",
    "    print(f\"\\nPeak Analysis for top {top_n} {display_name}:\")\n",
    "    print(f\"Total categories analyzed: {len(peak_info)}\")\n",
    "    \n",
    "    if focus_period:\n",
    "        print(f\"Focus period: {focus_period[0]} to {focus_period[1]}\")\n",
    "    \n",
    "    # Display the summary\n",
    "    print(\"\\nCategories by peak count:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Visualize the distribution of peak years\n",
    "    visualize_peak_distribution_top_categories(\n",
    "        peak_info, \n",
    "        column_name,\n",
    "        year_column, \n",
    "        subject_main_dir,\n",
    "        focus_period\n",
    "    )\n",
    "    \n",
    "    # Visualize trends with peaks highlighted - Counts\n",
    "    visualize_peak_trends_top_categories(\n",
    "        peak_info,\n",
    "        column_name,\n",
    "        year_column,\n",
    "        subject_main_dir,\n",
    "        plot_type='counts',\n",
    "        focus_period=focus_period\n",
    "    )\n",
    "    \n",
    "    # Visualize trends with peaks highlighted - Percentages\n",
    "    visualize_peak_trends_top_categories(\n",
    "        peak_info,\n",
    "        column_name,\n",
    "        year_column,\n",
    "        subject_main_dir,\n",
    "        plot_type='percentage',\n",
    "        focus_period=focus_period\n",
    "    )\n",
    "    \n",
    "    # Save the summary to CSV\n",
    "    csv_filename = f\"{column_name}_top_{top_n}_peak_analysis.csv\"\n",
    "    csv_path = Path(subject_main_dir) / subject_type / csv_filename\n",
    "    summary_df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nSummary saved to: {csv_path}\")\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b3f5b-5ac5-4faf-ad07-c63f8f06cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_peak_content(df, column_name='Journal', column_to_analyze='Title', n=5, \n",
    "                        year_range=(2010, 2020), year_column='Publication Year', \n",
    "                        subject_main_dir=None, text_analysis=True, entity_analysis=True,\n",
    "                        min_entity_count=2, max_entities=20, language='english'):\n",
    "    \"\"\"\n",
    "    Analyze the content of records during peak years for top categories.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe containing the data\n",
    "    column_name (str): Name of the categorical column to analyze (default: 'Journal')\n",
    "    column_to_analyze (str): Name of the text column to analyze (default: 'Title')\n",
    "    n (int): Number of top categories to analyze (default: 5)\n",
    "    year_range (tuple): Range of years to focus on (default: 2010-2020)\n",
    "    year_column (str): Name of the column containing year information\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    text_analysis (bool): Whether to perform general text analysis\n",
    "    entity_analysis (bool): Whether to perform entity analysis\n",
    "    min_entity_count (int): Minimum count for an entity to be included in analysis\n",
    "    max_entities (int): Maximum number of entities to display\n",
    "    language (str): Language for text analysis\n",
    "    \n",
    "    Returns:\n",
    "    dict: Results of the analysis including peak information and textual insights\n",
    "    \"\"\"\n",
    "    # Setting up the subject type\n",
    "    subject_type = f\"{column_name.lower()}_peak_{column_to_analyze.lower()}\"\n",
    "    \n",
    "    # Create the appropriate directory\n",
    "    if subject_main_dir:\n",
    "        analysis_dir = Path(subject_main_dir) / subject_type\n",
    "        analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Filter by year range\n",
    "    start_year, end_year = year_range\n",
    "    year_filtered_df = df[(df[year_column] >= start_year) & (df[year_column] <= end_year)]\n",
    "    \n",
    "    print(f\"Analyzing peak content for top {n} {column_name} between {start_year}-{end_year}\")\n",
    "    print(f\"Original dataset size: {len(df)}\")\n",
    "    print(f\"Dataset size after year filtering: {len(year_filtered_df)}\")\n",
    "    \n",
    "    # Get the top N categories in this period\n",
    "    top_categories = year_filtered_df[column_name].value_counts().nlargest(n).index.tolist()\n",
    "    \n",
    "    # Results dictionary\n",
    "    results = {\n",
    "        'peak_info': {},\n",
    "        'text_analysis': {},\n",
    "        'entity_analysis': {}\n",
    "    }\n",
    "    \n",
    "    # Process each top category\n",
    "    for category in top_categories:\n",
    "        # Filter to just this category\n",
    "        category_df = year_filtered_df[year_filtered_df[column_name] == category]\n",
    "        \n",
    "        if len(category_df) == 0:\n",
    "            print(f\"No data found for {category} in the specified year range.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nAnalyzing peak content for {category} ({len(category_df)} records)\")\n",
    "        \n",
    "        # Find the peak year(s) for this category\n",
    "        yearly_counts = category_df.groupby(year_column).size()\n",
    "        max_count = yearly_counts.max()\n",
    "        peak_years = yearly_counts[yearly_counts == max_count].index.tolist()\n",
    "        \n",
    "        # Store peak information\n",
    "        results['peak_info'][category] = {\n",
    "            'total_records': len(category_df),\n",
    "            'peak_years': peak_years,\n",
    "            'peak_count': max_count,\n",
    "            'yearly_distribution': yearly_counts.to_dict()\n",
    "        }\n",
    "        \n",
    "        print(f\"Peak year(s) for {category}: {peak_years} with {max_count} records\")\n",
    "        \n",
    "        # Filter to just the peak years for this category\n",
    "        peak_df = category_df[category_df[year_column].isin(peak_years)]\n",
    "        \n",
    "        # Check if we have content to analyze\n",
    "        if len(peak_df) == 0:\n",
    "            print(f\"No records found for {category} in peak years.\")\n",
    "            continue\n",
    "            \n",
    "        if column_to_analyze not in peak_df.columns:\n",
    "            print(f\"Column '{column_to_analyze}' not found in the dataset.\")\n",
    "            continue\n",
    "            \n",
    "        # Check if the column has non-null values\n",
    "        if peak_df[column_to_analyze].isna().all():\n",
    "            print(f\"No non-null values found in '{column_to_analyze}' for {category} in peak years.\")\n",
    "            continue\n",
    "            \n",
    "        # Perform text analysis if requested\n",
    "        if text_analysis:\n",
    "            print(f\"Performing text analysis on {column_to_analyze} for {category} peak years...\")\n",
    "            \n",
    "            # Create a category-specific subfolder for this analysis\n",
    "            category_subject_type = f\"{subject_type}/{category.replace('/', '_')}\"\n",
    "            \n",
    "            try:\n",
    "                text_results = analyze_text_column(\n",
    "                    df=peak_df,\n",
    "                    column_name=column_to_analyze,\n",
    "                    subject_main_dir=subject_main_dir,\n",
    "                    filter_language=True,\n",
    "                    target_language='en',\n",
    "                    remove_stopwords=True,\n",
    "                    language=language, \n",
    "                    subject_type=subject_type,\n",
    "                )\n",
    "                results['text_analysis'][category] = text_results\n",
    "            except Exception as e:\n",
    "                print(f\"Error in text analysis for {category}: {str(e)}\")\n",
    "                results['text_analysis'][category] = {\"error\": str(e)}\n",
    "        \n",
    "        # Perform entity analysis if requested\n",
    "        if entity_analysis:\n",
    "            print(f\"Performing entity analysis on {column_to_analyze} for {category} peak years...\")\n",
    "            \n",
    "            try:\n",
    "                entity_results = analyze_entities_in_column(\n",
    "                    df=peak_df,\n",
    "                    column_name=column_to_analyze,\n",
    "                    subject_main_dir=subject_main_dir,\n",
    "                    min_count=min_entity_count,\n",
    "                    max_entities=max_entities,\n",
    "                    subject_type=subject_type,\n",
    "                )\n",
    "                results['entity_analysis'][category] = entity_results\n",
    "            except Exception as e:\n",
    "                print(f\"Error in entity analysis for {category}: {str(e)}\")\n",
    "                results['entity_analysis'][category] = {\"error\": str(e)}\n",
    "    \n",
    "    # Generate summary dataframe of peak information\n",
    "    summary_data = []\n",
    "    for category, info in results['peak_info'].items():\n",
    "        summary_data.append({\n",
    "            f'{column_name}': category,\n",
    "            'Peak_Year(s)': ', '.join(map(str, info['peak_years'])),\n",
    "            'Peak_Count': info['peak_count'],\n",
    "            'Total_Records': info['total_records']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    if not summary_df.empty:\n",
    "        summary_df = summary_df.sort_values('Peak_Count', ascending=False)\n",
    "        \n",
    "        # Save the summary\n",
    "        if subject_main_dir:\n",
    "            csv_filename = f\"{column_name.lower()}_top_{n}_peak_content_summary.csv\"\n",
    "            csv_path = Path(subject_main_dir) / subject_type / csv_filename\n",
    "            summary_df.to_csv(csv_path, index=False)\n",
    "            print(f\"\\nSummary saved to: {csv_path}\")\n",
    "    \n",
    "    # Create a visualization of common themes across peak content\n",
    "    visualize_peak_content_summary(results, column_name, column_to_analyze, subject_main_dir, subject_type)\n",
    "    \n",
    "    return results, summary_df\n",
    "\n",
    "def visualize_peak_content_summary(results, column_name, column_to_analyze, subject_main_dir, subject_type):\n",
    "    \"\"\"\n",
    "    Visualize a summary of common themes across peak content.\n",
    "    \n",
    "    Parameters:\n",
    "    results (dict): Results from the analyze_peak_content function\n",
    "    column_name (str): Name of the categorical column analyzed\n",
    "    column_to_analyze (str): Name of the text column analyzed\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    subject_type (str): Type of subject analysis\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if we have text analysis results\n",
    "    if not results['text_analysis'] or all(isinstance(v, dict) and 'error' in v for v in results['text_analysis'].values()):\n",
    "        print(\"No valid text analysis results to visualize.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Extract common words from all categories\n",
    "        all_words = {}\n",
    "        categories = list(results['text_analysis'].keys())\n",
    "        \n",
    "        for category in categories:\n",
    "            if 'error' in results['text_analysis'][category]:\n",
    "                continue\n",
    "                \n",
    "            if 'word_frequencies' in results['text_analysis'][category]:\n",
    "                for word, count in results['text_analysis'][category]['word_frequencies'].items():\n",
    "                    if word not in all_words:\n",
    "                        all_words[word] = {}\n",
    "                    all_words[word][category] = count\n",
    "        \n",
    "        if not all_words:\n",
    "            print(\"No word frequencies found in the text analysis results.\")\n",
    "            return\n",
    "            \n",
    "        # Find words that appear in multiple categories\n",
    "        common_words = {word: counts for word, counts in all_words.items() \n",
    "                      if len(counts) > 1}\n",
    "        \n",
    "        if not common_words:\n",
    "            print(\"No common words found across categories.\")\n",
    "            return\n",
    "            \n",
    "        # Sort by total frequency across categories\n",
    "        sorted_words = sorted(common_words.items(), \n",
    "                             key=lambda x: sum(x[1].values()), \n",
    "                             reverse=True)[:20]  # Top 20 common words\n",
    "        \n",
    "        # Prepare data for visualization\n",
    "        words = [word for word, _ in sorted_words]\n",
    "        categories_data = {category: [] for category in categories}\n",
    "        \n",
    "        for word, counts in sorted_words:\n",
    "            for category in categories:\n",
    "                categories_data[category].append(counts.get(category, 0))\n",
    "        \n",
    "        # Create the visualization\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Define width of bars and positions\n",
    "        width = 0.8 / len(categories)\n",
    "        positions = np.arange(len(words))\n",
    "        \n",
    "        # Plot bars for each category\n",
    "        for i, (category, values) in enumerate(categories_data.items()):\n",
    "            plt.bar(positions + i * width, values, width=width, label=category)\n",
    "        \n",
    "        # Set labels and title\n",
    "        plt.xlabel('Common Words', fontsize=12)\n",
    "        plt.ylabel('Frequency', fontsize=12)\n",
    "        plt.title(f'Common Words in {column_to_analyze} During Peak Years for Top {column_name}', fontsize=14)\n",
    "        \n",
    "        # Set x-axis ticks\n",
    "        plt.xticks(positions + width * (len(categories) - 1) / 2, words, rotation=45, ha='right')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        if subject_main_dir:\n",
    "            filename = f\"common_words_peak_years_{column_name.lower()}_{column_to_analyze.lower()}.png\"\n",
    "            save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating peak content summary visualization: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01589a34-9d74-4221-9310-1ef1914e93be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_combined_peak_content(df, column_name='Journal', column_to_analyze='Title', n=5, \n",
    "                        year_range=(2010, 2020), year_column='Publication Year', \n",
    "                        subject_main_dir=None, text_analysis=True, entity_analysis=True,\n",
    "                        min_entity_count=2, max_entities=20, language='english'):\n",
    "    \"\"\"\n",
    "    Analyze the combined content of records from peak years for top categories.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe containing the data\n",
    "    column_name (str): Name of the categorical column to analyze (default: 'Journal')\n",
    "    column_to_analyze (str): Name of the text column to analyze (default: 'Title')\n",
    "    n (int): Number of top categories to analyze (default: 5)\n",
    "    year_range (tuple): Range of years to focus on (default: 2010-2020)\n",
    "    year_column (str): Name of the column containing year information\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    text_analysis (bool): Whether to perform general text analysis\n",
    "    entity_analysis (bool): Whether to perform entity analysis\n",
    "    min_entity_count (int): Minimum count for an entity to be included in analysis\n",
    "    max_entities (int): Maximum number of entities to display\n",
    "    language (str): Language for text analysis\n",
    "    \n",
    "    Returns:\n",
    "    dict: Results of the analysis including peak information and textual insights\n",
    "    \"\"\"\n",
    "    # Setting up the subject type\n",
    "    subject_type = f\"{column_name.lower()}_peak_{column_to_analyze.lower()}\"\n",
    "    \n",
    "    # Create the appropriate directory\n",
    "    if subject_main_dir:\n",
    "        analysis_dir = Path(subject_main_dir) / subject_type\n",
    "        analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Filter by year range\n",
    "    start_year, end_year = year_range\n",
    "    year_filtered_df = df[(df[year_column] >= start_year) & (df[year_column] <= end_year)]\n",
    "    \n",
    "    print(f\"Analyzing combined peak content for top {n} {column_name} between {start_year}-{end_year}\")\n",
    "    print(f\"Original dataset size: {len(df)}\")\n",
    "    print(f\"Dataset size after year filtering: {len(year_filtered_df)}\")\n",
    "    \n",
    "    # Get the top N categories in this period\n",
    "    top_categories = year_filtered_df[column_name].value_counts().nlargest(n).index.tolist()\n",
    "    \n",
    "    # Results dictionary\n",
    "    results = {\n",
    "        'peak_info': {},\n",
    "        'combined_analysis': {}\n",
    "    }\n",
    "    \n",
    "    # Collect all records from peak years across all categories\n",
    "    all_peak_records = []\n",
    "    \n",
    "    # Process each top category to find peak years\n",
    "    for category in top_categories:\n",
    "        # Filter to just this category\n",
    "        category_df = year_filtered_df[year_filtered_df[column_name] == category]\n",
    "        \n",
    "        if len(category_df) == 0:\n",
    "            print(f\"No data found for {category} in the specified year range.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nIdentifying peak years for {category} ({len(category_df)} records)\")\n",
    "        \n",
    "        # Find the peak year(s) for this category\n",
    "        yearly_counts = category_df.groupby(year_column).size()\n",
    "        max_count = yearly_counts.max()\n",
    "        peak_years = yearly_counts[yearly_counts == max_count].index.tolist()\n",
    "        \n",
    "        # Store peak information\n",
    "        results['peak_info'][category] = {\n",
    "            'total_records': len(category_df),\n",
    "            'peak_years': peak_years,\n",
    "            'peak_count': max_count,\n",
    "            'yearly_distribution': yearly_counts.to_dict()\n",
    "        }\n",
    "        \n",
    "        print(f\"Peak year(s) for {category}: {peak_years} with {max_count} records\")\n",
    "        \n",
    "        # Filter to just the peak years for this category and add to our collection\n",
    "        peak_df = category_df[category_df[year_column].isin(peak_years)]\n",
    "        all_peak_records.append(peak_df)\n",
    "    \n",
    "    # Combine all peak records into a single DataFrame\n",
    "    if all_peak_records:\n",
    "        combined_peak_df = pd.concat(all_peak_records, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No peak data found for any category.\")\n",
    "        return results, pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nAnalyzing combined peak data with {len(combined_peak_df)} records...\")\n",
    "    \n",
    "    # Check if we have content to analyze\n",
    "    if column_to_analyze not in combined_peak_df.columns:\n",
    "        print(f\"Column '{column_to_analyze}' not found in the dataset.\")\n",
    "        return results, pd.DataFrame()\n",
    "        \n",
    "    # Check if the column has non-null values\n",
    "    if combined_peak_df[column_to_analyze].isna().all():\n",
    "        print(f\"No non-null values found in '{column_to_analyze}' in peak years.\")\n",
    "        return results, pd.DataFrame()\n",
    "    \n",
    "    # Perform text analysis if requested\n",
    "    if text_analysis:\n",
    "        print(f\"Performing text analysis on combined {column_to_analyze} from peak years...\")\n",
    "        \n",
    "        try:\n",
    "            text_results = analyze_text_column(\n",
    "                df=combined_peak_df,\n",
    "                column_name=column_to_analyze,\n",
    "                subject_main_dir=subject_main_dir,\n",
    "                filter_language=True,\n",
    "                target_language='en',\n",
    "                remove_stopwords=True,\n",
    "                language=language, \n",
    "                subject_type=subject_type,\n",
    "            )\n",
    "            results['combined_analysis']['text_analysis'] = text_results\n",
    "        except Exception as e:\n",
    "            print(f\"Error in combined text analysis: {str(e)}\")\n",
    "            results['combined_analysis']['text_analysis'] = {\"error\": str(e)}\n",
    "    \n",
    "    # Perform entity analysis if requested\n",
    "    if entity_analysis:\n",
    "        print(f\"Performing entity analysis on combined {column_to_analyze} from peak years...\")\n",
    "        \n",
    "        try:\n",
    "            entity_results = analyze_entities_in_column(\n",
    "                df=combined_peak_df,\n",
    "                column_name=column_to_analyze,\n",
    "                subject_main_dir=subject_main_dir,\n",
    "                min_count=min_entity_count,\n",
    "                max_entities=max_entities,\n",
    "                subject_type=subject_type,\n",
    "            )\n",
    "            results['combined_analysis']['entity_analysis'] = entity_results\n",
    "        except Exception as e:\n",
    "            print(f\"Error in combined entity analysis: {str(e)}\")\n",
    "            results['combined_analysis']['entity_analysis'] = {\"error\": str(e)}\n",
    "    \n",
    "    # Generate summary dataframe of peak information\n",
    "    summary_data = []\n",
    "    for category, info in results['peak_info'].items():\n",
    "        summary_data.append({\n",
    "            f'{column_name}': category,\n",
    "            'Peak_Year(s)': ', '.join(map(str, info['peak_years'])),\n",
    "            'Peak_Count': info['peak_count'],\n",
    "            'Total_Records': info['total_records']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    if not summary_df.empty:\n",
    "        summary_df = summary_df.sort_values('Peak_Count', ascending=False)\n",
    "        \n",
    "        # Save the summary\n",
    "        if subject_main_dir:\n",
    "            csv_filename = f\"{column_name.lower()}_top_{n}_peak_content_summary.csv\"\n",
    "            csv_path = Path(subject_main_dir) / subject_type / csv_filename\n",
    "            summary_df.to_csv(csv_path, index=False)\n",
    "            print(f\"\\nSummary saved to: {csv_path}\")\n",
    "    \n",
    "    return results, summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce30f6-68e2-4967-a1fb-9ec0723259da",
   "metadata": {},
   "source": [
    "# Journal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983948c7-47c0-41a0-bf84-1f4f64f657f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_categorical_by_decade_percentage(\n",
    "    df=df,\n",
    "    column_name='Journal',  # Your categorical column\n",
    "    decade_column='decade',  # Column containing decade information\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    top_n=10,  # Number of top categories to show\n",
    ")\n",
    "\n",
    "# For count distribution\n",
    "visualize_categorical_by_decade_counts(\n",
    "    df=df,\n",
    "    column_name='Journal',\n",
    "    decade_column='decade',\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    top_n=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca7faa-bbf4-41fb-b17f-dbe5caf2edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze peaks for the 'journal' column in the 2000-2025 period\n",
    "journal_peaks = analyze_categorical_peaks(\n",
    "    df=df,\n",
    "    column_name='Journal',  # Your categorical column\n",
    "    year_column='Publication Year',     # Column with year information\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    focus_period=(2000, 2025)  # Focus on 2000-2025 period\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e37e8-54de-4ff1-8b13-39300d26ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze peaks for the top 10 Journals in the 2000-2025 period\n",
    "journal_peaks = analyze_top_categorical_peaks(\n",
    "    df=df,\n",
    "    column_name='Journal',\n",
    "    year_column='Publication Year',\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    top_n=5,\n",
    "    focus_period=(2000, 2025)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddae3d6-497b-4d34-809b-9e812ce9cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_peak_content = analyze_combined_peak_content(\n",
    "    df=df,\n",
    "    column_name='Journal',\n",
    "    column_to_analyze='Abstract',\n",
    "    n=5,\n",
    "    year_range=(2010, 2020),\n",
    "    year_column='Publication Year',\n",
    "    subject_main_dir=subject_main_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cff75b-b671-4abf-9de6-b43b52621618",
   "metadata": {},
   "source": [
    "# Publisher analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92845c-d933-4c9d-82b5-0912aff4568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_categorical_by_decade_percentage(\n",
    "    df=df,\n",
    "    column_name='Publisher',  # Your categorical column\n",
    "    decade_column='decade',  # Column containing decade information\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    top_n=10,  # Number of top categories to show\n",
    ")\n",
    "\n",
    "# For count distribution\n",
    "visualize_categorical_by_decade_counts(\n",
    "    df=df,\n",
    "    column_name='Publisher',\n",
    "    decade_column='decade',\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    top_n=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b54ee7-14ec-425a-9325-288ec39427ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze peaks for the top 10 Journals in the 2000-2025 period\n",
    "journal_peaks = analyze_top_categorical_peaks(\n",
    "    df=df,\n",
    "    column_name='Publisher',\n",
    "    year_column='Publication Year',\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    top_n=5,\n",
    "    focus_period=(2000, 2025)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6fbc0b-0bf2-414a-a9b8-b60a6a0f992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_peak_content = analyze_combined_peak_content(\n",
    "    df=df,\n",
    "    column_name='Publisher',\n",
    "    column_to_analyze='Title',\n",
    "    n=5,\n",
    "    year_range=(2010, 2020),\n",
    "    year_column='Publication Year',\n",
    "    subject_main_dir=subject_main_dir\n",
    ")\n",
    "publisher_peak_content = analyze_combined_peak_content(\n",
    "    df=df,\n",
    "    column_name='Publisher',\n",
    "    column_to_analyze='Abstract',\n",
    "    n=5,\n",
    "    year_range=(2010, 2020),\n",
    "    year_column='Publication Year',\n",
    "    subject_main_dir=subject_main_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965f91d-c42e-4544-89ee-07dd2e4173e8",
   "metadata": {},
   "source": [
    "# Type analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce2f2a-b5fc-45fd-b412-eeb4b3a740aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_categorical_by_decade_percentage(\n",
    "    df=df,\n",
    "    column_name='Type',  # Your categorical column\n",
    "    decade_column='decade',  # Column containing decade information\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    top_n=10,  # Number of top categories to show\n",
    ")\n",
    "\n",
    "# For count distribution\n",
    "visualize_categorical_by_decade_counts(\n",
    "    df=df,\n",
    "    column_name='Type',\n",
    "    decade_column='decade',\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    top_n=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec042eb-3666-4bf3-a772-415dcaf71477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze peaks for the top 10 Journals in the 2000-2025 period\n",
    "journal_peaks = analyze_top_categorical_peaks(\n",
    "    df=df,\n",
    "    column_name='Type',\n",
    "    year_column='Publication Year',\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    top_n=10,\n",
    "    focus_period=(2000, 2025)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae51c7-a19c-4bfa-9c2d-88eb1dc46eb6",
   "metadata": {},
   "source": [
    "# Citation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa7334-4330-4c0f-a3ae-f0803c2926c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_citation_by_time(df, subject_main_dir, time_column='decade', citation_column='Citation Count', \n",
    "                            plot_title='Mean Citation Count by Decade', \n",
    "                            filename='mean_citation_by_decade.png',\n",
    "                            exclude_zero_citations=False):\n",
    "    \"\"\"\n",
    "    Analyze and visualize mean citation counts over time periods (e.g., decades)\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the data\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    time_column (str): Column name for the time period (e.g., 'decade')\n",
    "    citation_column (str): Column name for citation counts\n",
    "    plot_title (str): Title for the plot\n",
    "    filename (str): Filename for saving the plot\n",
    "    \"\"\"\n",
    "    # Subject type is derived from the citation column name\n",
    "    subject_type = citation_column.lower().replace(' ', '_')\n",
    "    \n",
    "    # Filter out zero citations if requested\n",
    "    analysis_df = df.copy()\n",
    "    # Filter out NaN and non-numeric values in the citation column\n",
    "    analysis_df = analysis_df[pd.to_numeric(analysis_df[citation_column], errors='coerce').notna()]\n",
    "    if exclude_zero_citations:\n",
    "        analysis_df = analysis_df[analysis_df[citation_column] > 0]\n",
    "        # Update filename to indicate filtering\n",
    "        filename = filename.replace('.png', '_non_zero.png')\n",
    "        plot_title += ' (Excluding Zero Citations)'\n",
    "    \n",
    "    # Group by time period and calculate mean citation count\n",
    "    time_citation_df = analysis_df.groupby(time_column)[citation_column].mean().reset_index()\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Line plot with markers\n",
    "    sns.lineplot(x=time_column, y=citation_column, data=time_citation_df, marker='o', linewidth=2, markersize=8)\n",
    "    \n",
    "    # Add linear trend line\n",
    "    sns.regplot(x=time_column, y=citation_column, data=time_citation_df, \n",
    "                scatter=False, line_kws={\"color\":\"red\", \"linestyle\":\"--\"})\n",
    "    \n",
    "    # Enhance the plot\n",
    "    plt.title(plot_title, fontsize=16)\n",
    "    plt.xlabel(f'{time_column.capitalize()}', fontsize=14)\n",
    "    plt.ylabel('Mean Citation Count', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(time_citation_df[time_column], rotation=45 if len(time_citation_df) > 10 else 0)\n",
    "    \n",
    "    # # Add value labels\n",
    "    # for x, y in zip(time_citation_df[time_column], time_citation_df[citation_column]):\n",
    "    #     plt.text(x, y + max(time_citation_df[citation_column])*0.02, f'{y:.2f}', \n",
    "    #             ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot using the utility function\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    # Return the aggregated data for further analysis if needed\n",
    "    return time_citation_df\n",
    "\n",
    "# Example usage (assuming df and subject_main_dir are already defined):\n",
    "# With all citations:\n",
    "# citation_by_decade = analyze_citation_by_time(df, subject_main_dir)\n",
    "# \n",
    "# Only considering non-zero citations:\n",
    "citation_by_decade_non_zero = analyze_citation_by_time(df, subject_main_dir, exclude_zero_citations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674a9fd-a5a2-4e80-82d0-c5afba42e340",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "def analyze_citation_distribution(df, subject_main_dir, citation_column='Citation Count', \n",
    "                                 exclude_zero_citations=False, log_scale=True,\n",
    "                                 percentile_cutoff=99.5):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the distribution of citation counts\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the data\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    citation_column (str): Column name for citation counts\n",
    "    exclude_zero_citations (bool): Whether to exclude zero citations\n",
    "    log_scale (bool): Whether to use log scale for certain visualizations\n",
    "    percentile_cutoff (float): Percentile cutoff for trimming extreme values in some plots\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing statistical summary and filtered dataframe\n",
    "    \"\"\"\n",
    "    # Subject type is derived from the citation column name\n",
    "    subject_type = citation_column.lower().replace(' ', '_')\n",
    "    \n",
    "    # Filter out NaN and non-numeric values\n",
    "    analysis_df = df.copy()\n",
    "    analysis_df = analysis_df[pd.to_numeric(analysis_df[citation_column], errors='coerce').notna()]\n",
    "    \n",
    "    # Optionally exclude zero citations\n",
    "    if exclude_zero_citations:\n",
    "        analysis_df = analysis_df[analysis_df[citation_column] > 0]\n",
    "        plot_suffix = '_non_zero'\n",
    "    else:\n",
    "        plot_suffix = ''\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    stats = {\n",
    "        'count': len(analysis_df),\n",
    "        'mean': analysis_df[citation_column].mean(),\n",
    "        'median': analysis_df[citation_column].median(),\n",
    "        'std': analysis_df[citation_column].std(),\n",
    "        'min': analysis_df[citation_column].min(),\n",
    "        'max': analysis_df[citation_column].max(),\n",
    "        'papers_with_1000plus_citations': len(analysis_df[analysis_df[citation_column] >= 1000]),\n",
    "        'papers_with_500plus_citations': len(analysis_df[analysis_df[citation_column] >= 500]),\n",
    "        'papers_with_100plus_citations': len(analysis_df[analysis_df[citation_column] >= 100]),\n",
    "        'papers_with_10plus_citations': len(analysis_df[analysis_df[citation_column] >= 10]),\n",
    "        'papers_with_zero_citations': len(analysis_df[analysis_df[citation_column] == 0]),\n",
    "        'percentiles': {\n",
    "            '25%': analysis_df[citation_column].quantile(0.25),\n",
    "            '50%': analysis_df[citation_column].quantile(0.5),\n",
    "            '75%': analysis_df[citation_column].quantile(0.75),\n",
    "            '90%': analysis_df[citation_column].quantile(0.9),\n",
    "            '95%': analysis_df[citation_column].quantile(0.95),\n",
    "            '99%': analysis_df[citation_column].quantile(0.99),\n",
    "            '99.9%': analysis_df[citation_column].quantile(0.999)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 1. Create histogram with distribution\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Get the cutoff value for trimming extreme values\n",
    "    cutoff_value = analysis_df[citation_column].quantile(percentile_cutoff/100)\n",
    "    trimmed_df = analysis_df[analysis_df[citation_column] <= cutoff_value]\n",
    "    \n",
    "    # Determine appropriate number of bins\n",
    "    bin_count = min(50, int(math.sqrt(len(trimmed_df))))\n",
    "    \n",
    "    # Create histogram\n",
    "    sns.histplot(trimmed_df[citation_column], bins=bin_count, kde=True)\n",
    "    plt.title(f'Distribution of Citation Counts (≤ {percentile_cutoff}th percentile)', fontsize=15)\n",
    "    plt.xlabel('Citation Count', fontsize=12)\n",
    "    plt.ylabel('Number of Papers', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add summary statistics as text\n",
    "    stats_text = (f\"Mean: {stats['mean']:.2f}\\n\"\n",
    "                 f\"Median: {stats['median']:.1f}\\n\"\n",
    "                 f\"Max: {stats['max']}\\n\"\n",
    "                 f\"Std Dev: {stats['std']:.2f}\\n\"\n",
    "                 f\"Papers ≥1000 citations: {stats['papers_with_1000plus_citations']}\\n\"\n",
    "                 f\"Papers ≥500 citations: {stats['papers_with_500plus_citations']}\")\n",
    "    plt.figtext(0.75, 0.70, stats_text, fontsize=10, \n",
    "                bbox=dict(facecolor='white', alpha=0.8, boxstyle='round'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, f'citation_distribution_histogram{plot_suffix}.png', subject_main_dir, subject_type)\n",
    "    \n",
    "    # 2. Create log-scale histogram if requested\n",
    "    if log_scale:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        # Add 1 to all values to handle zeros when taking log\n",
    "        log_values = np.log1p(analysis_df[citation_column])\n",
    "        sns.histplot(log_values, bins=50, kde=True)\n",
    "        plt.title('Distribution of Citation Counts (Log Scale)', fontsize=15)\n",
    "        plt.xlabel('Log(Citation Count + 1)', fontsize=12)\n",
    "        plt.ylabel('Number of Papers', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        save_plot(plt, f'citation_distribution_log_scale{plot_suffix}.png', subject_main_dir, subject_type)\n",
    "    \n",
    "    # 3. Create boxplot to show the distribution and outliers\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(y=analysis_df[citation_column])\n",
    "    plt.title('Boxplot of Citation Counts', fontsize=15)\n",
    "    plt.ylabel('Citation Count', fontsize=12)\n",
    "    plt.grid(True, axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, f'citation_distribution_boxplot{plot_suffix}.png', subject_main_dir, subject_type)\n",
    "    \n",
    "    # 4. Create CDF (Cumulative Distribution Function) plot\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Sort values for CDF\n",
    "    sorted_citations = np.sort(analysis_df[citation_column])\n",
    "    # Calculate cumulative probabilities\n",
    "    cumulative_prob = np.arange(1, len(sorted_citations) + 1) / len(sorted_citations)\n",
    "    \n",
    "    plt.plot(sorted_citations, cumulative_prob)\n",
    "    plt.title('Cumulative Distribution Function of Citation Counts', fontsize=15)\n",
    "    plt.xlabel('Citation Count', fontsize=12)\n",
    "    plt.ylabel('Cumulative Probability', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark interesting percentiles\n",
    "    for perc, label in [(0.5, '50%'), (0.75, '75%'), (0.9, '90%'), (0.95, '95%')]:\n",
    "        idx = np.searchsorted(cumulative_prob, perc)\n",
    "        if idx < len(sorted_citations):\n",
    "            x_val = sorted_citations[idx]\n",
    "            plt.axhline(y=perc, color='r', linestyle='--', alpha=0.3)\n",
    "            plt.axvline(x=x_val, color='r', linestyle='--', alpha=0.3)\n",
    "            plt.text(x_val + 0.5, perc + 0.01, f'{label}: {x_val:.1f}', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, f'citation_distribution_cdf{plot_suffix}.png', subject_main_dir, subject_type)\n",
    "    \n",
    "    # 5. Create a table of citation count ranges\n",
    "    citation_ranges = [\n",
    "        (0, 0, 'Zero citations'),\n",
    "        (1, 9, '1-9 citations'),\n",
    "        (10, 49, '10-49 citations'),\n",
    "        (50, 99, '50-99 citations'),\n",
    "        (100, 499, '100-499 citations'),\n",
    "        (500, 999, '500-999 citations'),\n",
    "        (1000, float('inf'), '1000+ citations')\n",
    "    ]\n",
    "    \n",
    "    range_counts = []\n",
    "    for lower, upper, label in citation_ranges:\n",
    "        if upper == float('inf'):\n",
    "            count = len(analysis_df[analysis_df[citation_column] >= lower])\n",
    "        else:\n",
    "            count = len(analysis_df[(analysis_df[citation_column] >= lower) & \n",
    "                                   (analysis_df[citation_column] <= upper)])\n",
    "        percentage = count / len(analysis_df) * 100\n",
    "        range_counts.append({'Range': label, 'Count': count, 'Percentage': percentage})\n",
    "    \n",
    "    range_df = pd.DataFrame(range_counts)\n",
    "    \n",
    "    # Create bar chart of citation ranges\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(x='Range', y='Percentage', data=range_df)\n",
    "    plt.title('Distribution of Papers by Citation Count Ranges', fontsize=15)\n",
    "    plt.xlabel('Citation Count Range', fontsize=12)\n",
    "    plt.ylabel('Percentage of Papers (%)', fontsize=12)\n",
    "    \n",
    "    # Add count labels on top of bars\n",
    "    for i, row in enumerate(range_df.itertuples()):\n",
    "        plt.text(i, row.Percentage + 0.5, f'{row.Count:,}\\n({row.Percentage:.1f}%)', \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, f'citation_distribution_ranges{plot_suffix}.png', subject_main_dir, subject_type)\n",
    "    \n",
    "    return {'stats': stats, 'filtered_df': analysis_df, 'range_distribution': range_df}\n",
    "\n",
    "# Example usage (assuming df and subject_main_dir are already defined):\n",
    "citation_stats = analyze_citation_distribution(df, subject_main_dir)\n",
    "# \n",
    "# # With filtering for non-zero citations:\n",
    "citation_stats_non_zero = analyze_citation_distribution(df, subject_main_dir, exclude_zero_citations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8919fc-7fba-47e4-a890-432d3b22cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_high_citations_by_category(df, subject_main_dir, category_column='Journal', \n",
    "                                      citation_column='Citation Count', min_citation_count=500,\n",
    "                                      top_n=10, min_papers=3, exclude_na=True,\n",
    "                                      plot_title=None, filename=None):\n",
    "    \"\"\"\n",
    "    Analyze and visualize top categories (e.g., journals, publishers) by citation metrics\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the data\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    category_column (str): Column name for the category to analyze (e.g., 'Journal', 'Publisher', 'Type')\n",
    "    citation_column (str): Column name for citation counts\n",
    "    min_citation_count (int): Minimum citation count threshold for \"highly cited\" papers\n",
    "    top_n (int): Number of top categories to display\n",
    "    min_papers (int): Minimum number of papers a category must have to be included\n",
    "    exclude_na (bool): Whether to exclude NA/null values in the category column\n",
    "    plot_title (str): Custom plot title (if None, will be auto-generated)\n",
    "    filename (str): Custom filename (if None, will be auto-generated)\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    # Generate the subject type\n",
    "    subject_type = f\"citation_{category_column.lower().replace(' ', '_')}\"\n",
    "    \n",
    "    # Filter out NaN and non-numeric values in citation column\n",
    "    analysis_df = df.copy()\n",
    "    analysis_df = analysis_df[pd.to_numeric(analysis_df[citation_column], errors='coerce').notna()]\n",
    "    \n",
    "    # Exclude NA values in category column if requested\n",
    "    if exclude_na:\n",
    "        analysis_df = analysis_df[analysis_df[category_column].notna()]\n",
    "    \n",
    "    # Generate auto filename and title if not provided\n",
    "    if filename is None:\n",
    "        filename = f\"top_{top_n}_{category_column.lower().replace(' ', '_')}_by_citations.png\"\n",
    "    \n",
    "    if plot_title is None:\n",
    "        plot_title = f\"Top {top_n} {category_column}s by Citation Metrics\"\n",
    "    \n",
    "    # 1. Identify highly cited papers\n",
    "    highly_cited = analysis_df[analysis_df[citation_column] >= min_citation_count].copy()\n",
    "    \n",
    "    # 2. Calculate statistics by category\n",
    "    category_stats = []\n",
    "    \n",
    "    for category, group in analysis_df.groupby(category_column):\n",
    "        # Skip categories with too few papers\n",
    "        if len(group) < min_papers:\n",
    "            continue\n",
    "            \n",
    "        # Calculate metrics\n",
    "        highly_cited_count = len(highly_cited[highly_cited[category_column] == category])\n",
    "        total_papers = len(group)\n",
    "        percent_highly_cited = (highly_cited_count / total_papers) * 100 if total_papers > 0 else 0\n",
    "        mean_citations = group[citation_column].mean()\n",
    "        median_citations = group[citation_column].median()\n",
    "        max_citations = group[citation_column].max()\n",
    "        \n",
    "        category_stats.append({\n",
    "            'Category': category,\n",
    "            'Total_Papers': total_papers,\n",
    "            'Highly_Cited_Papers': highly_cited_count,\n",
    "            'Percent_Highly_Cited': percent_highly_cited,\n",
    "            'Mean_Citations': mean_citations,\n",
    "            'Median_Citations': median_citations,\n",
    "            'Max_Citations': max_citations\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    stats_df = pd.DataFrame(category_stats)\n",
    "    \n",
    "    # If we have no data, return early\n",
    "    if len(stats_df) == 0:\n",
    "        print(f\"No categories with at least {min_papers} papers found.\")\n",
    "        return {\n",
    "            'stats_df': stats_df,\n",
    "            'error': 'No categories with minimum paper count found'\n",
    "        }\n",
    "    \n",
    "    # Sort by different metrics and get top N\n",
    "    top_by_percent = stats_df.sort_values('Percent_Highly_Cited', ascending=False).head(top_n)\n",
    "    top_by_count = stats_df.sort_values('Highly_Cited_Papers', ascending=False).head(top_n)\n",
    "    top_by_mean = stats_df.sort_values('Mean_Citations', ascending=False).head(top_n)\n",
    "    \n",
    "    # 3. Create visualizations\n",
    "    \n",
    "    # 3.1 Top categories by percentage of highly cited papers\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = sns.barplot(y='Category', x='Percent_Highly_Cited', data=top_by_percent)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, (_, row) in enumerate(top_by_percent.iterrows()):\n",
    "        plt.text(row['Percent_Highly_Cited'] + 0.5, i, \n",
    "                f\"{row['Highly_Cited_Papers']}/{row['Total_Papers']}\", \n",
    "                va='center', fontsize=9)\n",
    "    \n",
    "    plt.title(f\"Top {top_n} {category_column}s by Percentage of Highly Cited Papers (≥{min_citation_count} citations)\", \n",
    "              fontsize=15)\n",
    "    plt.xlabel('Percentage of Highly Cited Papers (%)', fontsize=12)\n",
    "    plt.ylabel(category_column, fontsize=12)\n",
    "    plt.grid(True, axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, f\"top_{category_column.lower().replace(' ', '_')}_by_percent_highly_cited.png\", \n",
    "              subject_main_dir, subject_type)\n",
    "    \n",
    "    # 3.2 Top categories by count of highly cited papers\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = sns.barplot(y='Category', x='Highly_Cited_Papers', data=top_by_count)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (_, row) in enumerate(top_by_count.iterrows()):\n",
    "        plt.text(row['Highly_Cited_Papers'] + 0.5, i, \n",
    "                f\"{row['Percent_Highly_Cited']:.1f}% of {row['Total_Papers']}\", \n",
    "                va='center', fontsize=9)\n",
    "    \n",
    "    plt.title(f\"Top {top_n} {category_column}s by Number of Highly Cited Papers (≥{min_citation_count} citations)\", \n",
    "              fontsize=15)\n",
    "    plt.xlabel('Number of Highly Cited Papers', fontsize=12)\n",
    "    plt.ylabel(category_column, fontsize=12)\n",
    "    plt.grid(True, axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, f\"top_{category_column.lower().replace(' ', '_')}_by_count_highly_cited.png\", \n",
    "              subject_main_dir, subject_type)\n",
    "    \n",
    "    # 3.3 Top categories by mean citation count\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = sns.barplot(y='Category', x='Mean_Citations', data=top_by_mean)\n",
    "    \n",
    "    # Add paper count labels\n",
    "    for i, (_, row) in enumerate(top_by_mean.iterrows()):\n",
    "        plt.text(row['Mean_Citations'] + 1, i, \n",
    "                f\"{row['Total_Papers']} papers\", \n",
    "                va='center', fontsize=9)\n",
    "    \n",
    "    plt.title(f\"Top {top_n} {category_column}s by Mean Citation Count\", fontsize=15)\n",
    "    plt.xlabel('Mean Citation Count', fontsize=12)\n",
    "    plt.ylabel(category_column, fontsize=12)\n",
    "    plt.grid(True, axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, f\"top_{category_column.lower().replace(' ', '_')}_by_mean_citations.png\", \n",
    "              subject_main_dir, subject_type)\n",
    "    \n",
    "    # 3.4 Create a combined visualization showing multiple metrics\n",
    "    # Combine the top categories from all metrics\n",
    "    all_top_categories = pd.concat([\n",
    "        top_by_percent['Category'], \n",
    "        top_by_count['Category'], \n",
    "        top_by_mean['Category']\n",
    "    ]).unique()\n",
    "    \n",
    "    # Get stats for all these categories\n",
    "    combined_stats = stats_df[stats_df['Category'].isin(all_top_categories)].sort_values('Highly_Cited_Papers', ascending=False)\n",
    "    \n",
    "    # Check if we have any data for the combined chart\n",
    "    if len(combined_stats) > 0:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Prepare for grouped bar chart with 3 metrics\n",
    "        categories = combined_stats['Category'].tolist()\n",
    "        x = np.arange(len(categories))\n",
    "        width = 0.25\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        \n",
    "        # Create bars for each metric\n",
    "        ax.bar(x - width, combined_stats['Highly_Cited_Papers'], width, label='Highly Cited Papers')\n",
    "        ax.bar(x, combined_stats['Mean_Citations'], width, label='Mean Citations')\n",
    "        ax.bar(x + width, combined_stats['Percent_Highly_Cited'], width, label='% Highly Cited')\n",
    "        \n",
    "        # Add custom y-axis for percentage\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.set_ylabel('Percentage', fontsize=12)\n",
    "        ax2.set_ylim(0, max(combined_stats['Percent_Highly_Cited']) * 1.2)\n",
    "        \n",
    "        # Set labels and legend\n",
    "        ax.set_ylabel('Count / Mean', fontsize=12)\n",
    "        ax.set_title(f'Multi-metric Comparison of Top {category_column}s', fontsize=15)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(categories, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_plot(plt, f\"top_{category_column.lower().replace(' ', '_')}_multi_metric.png\", \n",
    "                subject_main_dir, subject_type)\n",
    "    \n",
    "    # 4. Return results for further analysis\n",
    "    return {\n",
    "        'stats_df': stats_df,\n",
    "        'top_by_percent': top_by_percent,\n",
    "        'top_by_count': top_by_count,\n",
    "        'top_by_mean': top_by_mean,\n",
    "        'highly_cited_threshold': min_citation_count,\n",
    "        'highly_cited_papers': highly_cited\n",
    "    }\n",
    "\n",
    "# Example usage (assuming df and subject_main_dir are already defined):\n",
    "# Journal analysis\n",
    "journal_analysis = analyze_high_citations_by_category(df, subject_main_dir, category_column='Journal', min_citation_count=1000, top_n=10)\n",
    "\n",
    "# Publisher analysis\n",
    "publisher_analysis = analyze_high_citations_by_category(df, subject_main_dir, category_column='Publisher', min_citation_count=1000, top_n=10)\n",
    "\n",
    "# Type analysis\n",
    "type_analysis = analyze_high_citations_by_category(df, subject_main_dir, category_column='Type', min_citation_count=1000, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b2bd9e-3214-44f8-b147-37da29f7f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def analyze_citations_by_list_elements(df, subject_main_dir, list_columns=['Topics'], \n",
    "                                      citation_column='Citation Count', \n",
    "                                      min_citation_count=500, top_n=15,\n",
    "                                      min_occurrences=10, exclude_na=True):\n",
    "    \"\"\"\n",
    "    Analyze and visualize citation patterns across list-based columns (e.g., Topics, Domains, Fields)\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the data\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    list_columns (list): List of column names containing list-like data (string representations of lists or dictionaries)\n",
    "    citation_column (str): Column name for citation counts\n",
    "    min_citation_count (int): Minimum citation count threshold for \"highly cited\" papers\n",
    "    top_n (int): Number of top elements to display\n",
    "    min_occurrences (int): Minimum number of occurrences an element must have to be included\n",
    "    exclude_na (bool): Whether to exclude NA/null values in the analyzed columns\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    # Generate the subject type from all column names\n",
    "    combined_names = '_'.join([col.lower().replace(' ', '_') for col in list_columns])\n",
    "    subject_type = f\"citation_{combined_names}\"\n",
    "    \n",
    "    # Filter out NaN and non-numeric values in citation column\n",
    "    analysis_df = df.copy()\n",
    "    analysis_df = analysis_df[pd.to_numeric(analysis_df[citation_column], errors='coerce').notna()]\n",
    "    \n",
    "    # Identify highly cited papers\n",
    "    highly_cited = analysis_df[analysis_df[citation_column] >= min_citation_count].copy()\n",
    "    \n",
    "    # Function to safely convert string representations of lists to actual lists\n",
    "    def safe_convert_to_list(value):\n",
    "        if pd.isna(value):\n",
    "            return []\n",
    "        if isinstance(value, str):\n",
    "            try:\n",
    "                # If it's a string representation of a list\n",
    "                converted = ast.literal_eval(value)\n",
    "                if isinstance(converted, list):\n",
    "                    return converted\n",
    "                elif isinstance(converted, dict):\n",
    "                    # If it's a dictionary, extract keys\n",
    "                    return list(converted.keys())\n",
    "                else:\n",
    "                    return [converted]  # Handle scalar values\n",
    "            except (ValueError, SyntaxError):\n",
    "                # If not a valid literal, treat as a single string item\n",
    "                return [value]\n",
    "        elif isinstance(value, dict):\n",
    "            # If it's already a dictionary\n",
    "            return list(value.keys())\n",
    "        elif isinstance(value, list):\n",
    "            # If it's already a list\n",
    "            return value\n",
    "        else:\n",
    "            # For scalar values\n",
    "            return [value]\n",
    "    \n",
    "    # Process each paper and collect elements from specified columns\n",
    "    all_elements = []\n",
    "    highly_cited_elements = []\n",
    "    \n",
    "    # This will store counts of total papers and highly cited papers per element\n",
    "    element_counts = defaultdict(lambda: {'total': 0, 'highly_cited': 0})\n",
    "    \n",
    "    # Process all papers\n",
    "    for _, row in analysis_df.iterrows():\n",
    "        paper_elements = []\n",
    "        \n",
    "        # Skip if any required column is missing\n",
    "        if exclude_na and any(pd.isna(row[col]) for col in list_columns):\n",
    "            continue\n",
    "            \n",
    "        # Process each column and extract elements\n",
    "        for col in list_columns:\n",
    "            elements = safe_convert_to_list(row[col])\n",
    "            paper_elements.extend(elements)\n",
    "        \n",
    "        # Count each element for this paper\n",
    "        for element in set(paper_elements):  # use set to count each element once per paper\n",
    "            element_counts[element]['total'] += 1\n",
    "            all_elements.append(element)\n",
    "            \n",
    "            # Check if this is a highly cited paper\n",
    "            if row[citation_column] >= min_citation_count:\n",
    "                element_counts[element]['highly_cited'] += 1\n",
    "                highly_cited_elements.append(element)\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    elements_df = pd.DataFrame([\n",
    "        {\n",
    "            'Element': element,\n",
    "            'Total_Papers': counts['total'],\n",
    "            'Highly_Cited_Papers': counts['highly_cited'],\n",
    "            'Percent_Highly_Cited': (counts['highly_cited'] / counts['total'] * 100) if counts['total'] > 0 else 0,\n",
    "            'Total_Frequency': all_elements.count(element),\n",
    "            'Highly_Cited_Frequency': highly_cited_elements.count(element)\n",
    "        }\n",
    "        for element, counts in element_counts.items()\n",
    "        if counts['total'] >= min_occurrences  # Filter by minimum occurrences\n",
    "    ])\n",
    "    \n",
    "    # If no data, return early\n",
    "    if len(elements_df) == 0:\n",
    "        print(f\"No elements with at least {min_occurrences} occurrences found.\")\n",
    "        return {\n",
    "            'elements_df': pd.DataFrame(),\n",
    "            'error': 'No elements with minimum occurrences found'\n",
    "        }\n",
    "    \n",
    "    # Sort by different metrics and get top N\n",
    "    top_by_percent = elements_df.sort_values('Percent_Highly_Cited', ascending=False).head(top_n)\n",
    "    top_by_count = elements_df.sort_values('Highly_Cited_Papers', ascending=False).head(top_n)\n",
    "    \n",
    "    # Create visualizations\n",
    "    \n",
    "    # 1. Top elements by percentage of highly cited papers\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    bars = sns.barplot(y='Element', x='Percent_Highly_Cited', data=top_by_percent)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, (_, row) in enumerate(top_by_percent.iterrows()):\n",
    "        plt.text(row['Percent_Highly_Cited'] + 0.5, i, \n",
    "                f\"{row['Highly_Cited_Papers']}/{row['Total_Papers']}\", \n",
    "                va='center', fontsize=9)\n",
    "    \n",
    "    column_names = ', '.join(list_columns)\n",
    "    plt.title(f\"Top {top_n} Elements from {column_names} by Percentage of Highly Cited Papers\", \n",
    "              fontsize=15)\n",
    "    plt.xlabel('Percentage of Highly Cited Papers (%)', fontsize=12)\n",
    "    plt.ylabel('Element', fontsize=12)\n",
    "    plt.grid(True, axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, f\"top_elements_by_percent_highly_cited.png\", subject_main_dir, subject_type)\n",
    "    \n",
    "    # 2. Top elements by count of highly cited papers\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    bars = sns.barplot(y='Element', x='Highly_Cited_Papers', data=top_by_count)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (_, row) in enumerate(top_by_count.iterrows()):\n",
    "        plt.text(row['Highly_Cited_Papers'] + 0.5, i, \n",
    "                f\"{row['Percent_Highly_Cited']:.1f}% of {row['Total_Papers']}\", \n",
    "                va='center', fontsize=9)\n",
    "    \n",
    "    plt.title(f\"Top {top_n} Elements from {column_names} by Number of Highly Cited Papers\", \n",
    "              fontsize=15)\n",
    "    plt.xlabel('Number of Highly Cited Papers', fontsize=12)\n",
    "    plt.ylabel('Element', fontsize=12)\n",
    "    plt.grid(True, axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, f\"top_elements_by_count_highly_cited.png\", subject_main_dir, subject_type)\n",
    "    \n",
    "    # 3. Create a scatter plot of total papers vs highly cited papers\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Calculate point sizes based on total papers (normalized)\n",
    "    max_size = 300\n",
    "    min_size = 30\n",
    "    sizes = min_size + (elements_df['Total_Papers'] / elements_df['Total_Papers'].max()) * (max_size - min_size)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = plt.scatter(\n",
    "        elements_df['Total_Papers'], \n",
    "        elements_df['Highly_Cited_Papers'],\n",
    "        s=sizes,\n",
    "        alpha=0.6,\n",
    "        c=elements_df['Percent_Highly_Cited'],\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('Percentage of Highly Cited Papers', fontsize=12)\n",
    "    \n",
    "    # Add labels for top elements\n",
    "    top_combined = pd.concat([top_by_percent, top_by_count]).drop_duplicates()\n",
    "    for _, row in top_combined.iterrows():\n",
    "        plt.annotate(\n",
    "            row['Element'],\n",
    "            (row['Total_Papers'], row['Highly_Cited_Papers']),\n",
    "            fontsize=9,\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points'\n",
    "        )\n",
    "    \n",
    "    plt.title(f\"Relationship Between Total Papers and Highly Cited Papers by {column_names} Elements\", \n",
    "              fontsize=15)\n",
    "    plt.xlabel('Total Number of Papers', fontsize=12)\n",
    "    plt.ylabel('Number of Highly Cited Papers', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, f\"elements_citation_scatter.png\", subject_main_dir, subject_type)\n",
    "    \n",
    "    # Return results for further analysis\n",
    "    return {\n",
    "        'elements_df': elements_df,\n",
    "        'top_by_percent': top_by_percent,\n",
    "        'top_by_count': top_by_count,\n",
    "        'highly_cited_threshold': min_citation_count\n",
    "    }\n",
    "\n",
    "# Example usage (assuming df and subject_main_dir are already defined):\n",
    "# \n",
    "# # Single list column analysis\n",
    "topics_analysis = analyze_citations_by_list_elements(df, subject_main_dir, \n",
    "                                                   list_columns=['Domains'],\n",
    "                                                   min_citation_count=1000)\n",
    "\n",
    "# Multiple list columns combined\n",
    "domains_fields_analysis = analyze_citations_by_list_elements(df, subject_main_dir, \n",
    "                                                           list_columns=['Topics'],\n",
    "                                                           min_citation_count=1000)\n",
    "\n",
    "# # Analysis with concept dictionary\n",
    "concepts_analysis = analyze_citations_by_list_elements(df, subject_main_dir, \n",
    "                                                     list_columns=['concept_dict'],\n",
    "                                                     min_citation_count=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e824f17-5116-4276-b759-654bd359e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_high_citation_text(df, subject_main_dir, text_column='Abstract', citation_column='Citation Count',\n",
    "                             min_citation_count=500, analysis_type='general', comparison=True,\n",
    "                             filter_language=True, target_language='en', remove_stopwords=True,\n",
    "                             language='english', ner_model='en_core_web_sm', \n",
    "                             min_entity_count=2, max_entities=20):\n",
    "    \"\"\"\n",
    "    Analyze textual content specifically for highly cited papers and compare with regular papers\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the data\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    text_column (str): Column name containing text to analyze\n",
    "    citation_column (str): Column name for citation counts\n",
    "    min_citation_count (int): Minimum citation count threshold for \"highly cited\" papers\n",
    "    analysis_type (str): Type of analysis to perform - 'general', 'entities', or 'both'\n",
    "    comparison (bool): Whether to compare high-citation papers with regular papers\n",
    "    filter_language (bool): Whether to filter by language\n",
    "    target_language (str): Target language code to keep\n",
    "    remove_stopwords (bool): Whether to remove stopwords in preprocessing\n",
    "    language (str): Language for stopwords removal\n",
    "    ner_model (str): spaCy model to use for NER\n",
    "    min_entity_count (int): Minimum count for an entity to be included\n",
    "    max_entities (int): Maximum number of entities to show in visualizations\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    # Generate the subject type\n",
    "    subject_type = f\"high_citation_{text_column.lower().replace(' ', '_')}\"\n",
    "    \n",
    "    # Filter out NaN and non-numeric values in citation column\n",
    "    analysis_df = df.copy()\n",
    "    analysis_df = analysis_df[pd.to_numeric(analysis_df[citation_column], errors='coerce').notna()]\n",
    "    \n",
    "    # Filter out rows with missing text\n",
    "    analysis_df = analysis_df[analysis_df[text_column].notna()]\n",
    "    \n",
    "    # Split into highly cited and regular papers\n",
    "    highly_cited_df = analysis_df[analysis_df[citation_column] >= min_citation_count].copy()\n",
    "    regular_papers_df = analysis_df[analysis_df[citation_column] < min_citation_count].copy()\n",
    "    \n",
    "    # Create paths for intermediate processing results if needed\n",
    "    highly_cited_proc_path = Path(subject_main_dir) / subject_type / f\"highly_cited_processed.csv\"\n",
    "    regular_papers_proc_path = Path(subject_main_dir) / subject_type / f\"regular_papers_processed.csv\"\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    (Path(subject_main_dir) / subject_type).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    results = {\n",
    "        'highly_cited_count': len(highly_cited_df),\n",
    "        'regular_papers_count': len(regular_papers_df),\n",
    "        'analysis_type': analysis_type\n",
    "    }\n",
    "    \n",
    "    # Run appropriate analyses based on the selected type\n",
    "    if analysis_type in ['general', 'both']:\n",
    "        print(f\"Running general text analysis on {len(highly_cited_df)} highly cited papers...\")\n",
    "        highly_cited_text_results = analyze_text_column(\n",
    "            highly_cited_df, \n",
    "            text_column, \n",
    "            subject_main_dir,\n",
    "            filter_language=filter_language,\n",
    "            target_language=target_language,\n",
    "            remove_stopwords=remove_stopwords,\n",
    "            language=language,\n",
    "            subject_type=subject_type\n",
    "        )\n",
    "        results['highly_cited_text_analysis'] = highly_cited_text_results\n",
    "        \n",
    "        # Compare with regular papers if requested\n",
    "        if comparison:\n",
    "            # Take a random sample of regular papers of similar size to highly cited papers\n",
    "            sample_size = min(len(highly_cited_df) * 2, len(regular_papers_df))\n",
    "            regular_sample_df = regular_papers_df.sample(sample_size, random_state=42)\n",
    "            \n",
    "            print(f\"Running general text analysis on {len(regular_sample_df)} regular papers for comparison...\")\n",
    "            regular_text_results = analyze_text_column(\n",
    "                regular_sample_df, \n",
    "                text_column, \n",
    "                subject_main_dir,\n",
    "                filter_language=filter_language,\n",
    "                target_language=target_language,\n",
    "                remove_stopwords=remove_stopwords,\n",
    "                language=language,\n",
    "                subject_type=f'{subject_type}_compared}'\n",
    "            )\n",
    "            results['regular_papers_text_analysis'] = regular_text_results\n",
    "            \n",
    "            # Compare word clouds or other text statistics here\n",
    "            # This would depend on what analyze_text_column returns\n",
    "    \n",
    "    if analysis_type in ['entities', 'both']:\n",
    "        print(f\"Running entity analysis on {len(highly_cited_df)} highly cited papers...\")\n",
    "        highly_cited_entity_results = analyze_entities_in_column(\n",
    "            highly_cited_df,\n",
    "            text_column,\n",
    "            subject_main_dir,\n",
    "            min_count=min_entity_count,\n",
    "            max_entities=max_entities,\n",
    "            model=ner_model,\n",
    "            preprocess=True,\n",
    "            subject_type=subject_type\n",
    "        )\n",
    "        results['highly_cited_entity_analysis'] = highly_cited_entity_results\n",
    "        \n",
    "        # Compare with regular papers if requested\n",
    "        if comparison:\n",
    "            # Take a random sample of regular papers of similar size to highly cited papers\n",
    "            sample_size = min(len(highly_cited_df) * 2, len(regular_papers_df))\n",
    "            regular_sample_df = regular_papers_df.sample(sample_size, random_state=42)\n",
    "            \n",
    "            print(f\"Running entity analysis on {len(regular_sample_df)} regular papers for comparison...\")\n",
    "            regular_entity_results = analyze_entities_in_column(\n",
    "                regular_sample_df,\n",
    "                text_column,\n",
    "                subject_main_dir,\n",
    "                min_count=min_entity_count,\n",
    "                max_entities=max_entities,\n",
    "                model=ner_model,\n",
    "                preprocess=True,\n",
    "                subject_type=f'{subject_type}_compared}'\n",
    "            )\n",
    "            results['regular_papers_entity_analysis'] = regular_entity_results\n",
    "    \n",
    "    # Create comparison visualizations if we have both sets of results\n",
    "    if comparison and 'highly_cited_text_analysis' in results and 'regular_papers_text_analysis' in results:\n",
    "        print(\"Creating comparison visualizations...\")\n",
    "        \n",
    "        # Example: Compare document lengths\n",
    "        # (This is just an example - modify based on what your analyze_text_column function returns)\n",
    "        try:\n",
    "            high_lengths = results['highly_cited_text_analysis'].get('document_lengths', [])\n",
    "            regular_lengths = results['regular_papers_text_analysis'].get('document_lengths', [])\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.histplot(high_lengths, kde=True, color='blue', alpha=0.5, label=f'Highly Cited Papers (≥{min_citation_count} citations)')\n",
    "            sns.histplot(regular_lengths, kde=True, color='red', alpha=0.5, label='Regular Papers')\n",
    "            plt.title(f'Document Length Comparison for {text_column}', fontsize=15)\n",
    "            plt.xlabel('Document Length (characters)', fontsize=12)\n",
    "            plt.ylabel('Count', fontsize=12)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            save_plot(plt, f\"document_length_comparison.png\", subject_main_dir, subject_type)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Couldn't create document length comparison: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage (assuming df and subject_main_dir are already defined):\n",
    "# \n",
    "# Analyze abstracts of highly cited papers\n",
    "abstract_analysis = analyze_high_citation_text(\n",
    "    df, subject_main_dir, text_column='Abstract', min_citation_count=500,\n",
    "    analysis_type='general', comparison=True\n",
    ")\n",
    "\n",
    "# Analyze titles of highly cited papers\n",
    "title_analysis = analyze_high_citation_text(\n",
    "    df, subject_main_dir, text_column='Title', min_citation_count=500,\n",
    "    analysis_type='general', comparison=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4b13c7-9f05-46bf-a0f3-d3eb5625e694",
   "metadata": {},
   "source": [
    "# Domain analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab03fdee-b678-4402-b431-fbae44cf991d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Top 20 domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b357f-9232-4f62-b7c3-33b498b638bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_domains = [domain for sublist in df['Domains'] for domain in sublist]\n",
    "domain_counts = Counter(all_domains)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3594980b-3ac7-4e47-9192-dde27a60a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_counts_df = (\n",
    "    pd.DataFrame\n",
    "    .from_dict(domain_counts, orient='index', columns=['count'])\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'domain'})\n",
    "    .sort_values('count', ascending=False)\n",
    ")\n",
    "\n",
    "# 5. Visualize the top 20 domains\n",
    "top_n = 20\n",
    "top_domains = domain_counts_df.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_domains['domain'], top_domains['count'], color='skyblue')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(f\"Top {top_n} Domains\")\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 6. Save the figure\n",
    "analysis_filename = \"domain_frequency_top20.png\"\n",
    "domain_plot_path = os.path.join(processing_domain_dir, analysis_filename)\n",
    "plt.savefig(domain_plot_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1e27d-e4a6-4f60-918a-5a89f9d1df88",
   "metadata": {},
   "source": [
    "## Most common domain over decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044a374-6402-42f4-bd40-bbf174999e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_domain(group):\n",
    "    # Flatten the list of domain lists for the group (decade)\n",
    "    all_domains = [domain for domains in group['Domains'] for domain in domains]\n",
    "    if all_domains:\n",
    "        top_domain, count = Counter(all_domains).most_common(1)[0]\n",
    "        return pd.Series({'top_domain': top_domain, 'count': count})\n",
    "    else:\n",
    "        return pd.Series({'top_domain': None, 'count': 0})\n",
    "\n",
    "# Group by decade and get the top domain (and its count) for each decade.\n",
    "decade_top = df.groupby('decade').apply(get_top_domain).reset_index()\n",
    "decade_top = decade_top.sort_values('decade')\n",
    "print(decade_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b104f4ab-8dc9-45e2-9753-9e34680da23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(decade_top['decade'], decade_top['count'], marker='o', linestyle='-', color='blue')\n",
    "# Annotate each point with the top domain name.\n",
    "for idx, row in decade_top.iterrows():\n",
    "    plt.annotate(row['top_domain'],\n",
    "                 (row['decade'], row['count']),\n",
    "                 textcoords=\"offset points\",\n",
    "                 xytext=(0,10),\n",
    "                 ha='center')\n",
    "plt.title(\"Most Common Domain by Decade (Line Chart)\")\n",
    "plt.xlabel(\"Decade\")\n",
    "plt.ylabel(\"Count for Top Domain\")\n",
    "plt.xticks(decade_top['decade'], rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "line_chart_filename = \"top_domain_by_decade_line.png\"\n",
    "line_chart_path = os.path.join(processing_domain_dir, line_chart_filename)\n",
    "plt.savefig(line_chart_path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da6a8d-9105-4ef2-8009-b67f5f3814ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_top_domains = list(dict.fromkeys(decade_top['top_domain']))\n",
    "mapping = {domain: i for i, domain in enumerate(unique_top_domains)}\n",
    "decade_top['domain_code'] = decade_top['top_domain'].map(mapping)\n",
    "\n",
    "# =========================\n",
    "#       LINE PLOT\n",
    "# =========================\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(decade_top['decade'], decade_top['domain_code'], marker='o', linestyle='-', color='blue')\n",
    "plt.xticks(decade_top['decade'], rotation=45)\n",
    "# Set y-axis ticks using the numeric codes and corresponding domain names.\n",
    "plt.yticks(list(mapping.values()), list(mapping.keys()))\n",
    "plt.xlabel(\"Decade\")\n",
    "plt.ylabel(\"Top Domain\")\n",
    "plt.title(\"Trend of Most Common Domain by Decade\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and display the figure.\n",
    "line_plot_filename = \"top_domain_trend_line_plot.png\"\n",
    "line_plot_path = os.path.join(processing_domain_dir, line_plot_filename)\n",
    "plt.savefig(line_plot_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d6971d-ad9a-419e-abb2-da90784a8fc6",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ce8d7-f6e8-4f41-ad73-c3a256e4a867",
   "metadata": {},
   "source": [
    "### Main domains co-appearnces graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f16321-60c6-4a80-91a8-b637689c29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "\n",
    "def create_domain_network(df, subject_main_dir):\n",
    "    \"\"\"\n",
    "    Create and save a network visualization of domain co-occurrences\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The preprocessed DataFrame\n",
    "    subject_main_dir (str/Path): Directory to save the visualization\n",
    "    \"\"\"\n",
    "    # First preprocess the DataFrame\n",
    "    df = preprocess_dataframe(df)\n",
    "    \n",
    "    # Create co-occurrence dictionary\n",
    "    cooccurrence_dict = {}\n",
    "    # Count total occurrences of each domain for node sizing\n",
    "    domain_counts = Counter()\n",
    "    \n",
    "    # Process each paper's domains\n",
    "    for domains in df['Domains']:\n",
    "        # Count individual domains\n",
    "        domain_counts.update(domains)\n",
    "        \n",
    "        # Count co-occurrences\n",
    "        for d1, d2 in combinations(sorted(set(domains)), 2):\n",
    "            pair = tuple(sorted([d1, d2]))\n",
    "            cooccurrence_dict[pair] = cooccurrence_dict.get(pair, 0) + 1\n",
    "    \n",
    "    # Create network\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes with size based on frequency\n",
    "    max_count = max(domain_counts.values())\n",
    "    for domain, count in domain_counts.items():\n",
    "        # Normalize node size between 1000 and 5000\n",
    "        node_size = 1000 + (count / max_count) * 4000\n",
    "        G.add_node(domain, size=node_size)\n",
    "    \n",
    "    # Add edges with weights\n",
    "    edge_weights = []\n",
    "    for (d1, d2), weight in cooccurrence_dict.items():\n",
    "        G.add_edge(d1, d2, weight=weight)\n",
    "        edge_weights.append(weight)\n",
    "    \n",
    "    # Calculate edge width and color ranges\n",
    "    max_weight = max(edge_weights)\n",
    "    min_weight = min(edge_weights)\n",
    "    \n",
    "    # Create color map with shifted scale to make lower values more visible\n",
    "    edge_colors = []\n",
    "    edge_widths = []\n",
    "    \n",
    "    for (d1, d2) in G.edges():\n",
    "        weight = G[d1][d2]['weight']\n",
    "        # Shift the color scale to make lower co-occurrences darker\n",
    "        # Using logarithmic scale to enhance visibility of lower values\n",
    "        if max_weight > min_weight:\n",
    "            # Using a non-linear transformation to enhance lower values\n",
    "            color_val = 0.4 + 0.6 * (np.log1p(weight - min_weight) / np.log1p(max_weight - min_weight))\n",
    "        else:\n",
    "            color_val = 0.5\n",
    "        edge_colors.append(color_val)\n",
    "        \n",
    "        # Normalize width (1 to 5 to maintain clarity)\n",
    "        width = 1 + 4 * (weight - min_weight) / (max_weight - min_weight) if max_weight > min_weight else 3\n",
    "        edge_widths.append(width)\n",
    "    \n",
    "    # Set up the figure with GridSpec\n",
    "    fig = plt.figure(figsize=(24, 22), facecolor='white')  # Slightly larger figure\n",
    "    gs = gridspec.GridSpec(1, 20)  # 1 row, 20 columns for fine control\n",
    "    \n",
    "    # Main plot area (using 19 columns)\n",
    "    ax_main = fig.add_subplot(gs[0, :19])\n",
    "    \n",
    "    # Colorbar area (using 1 column)\n",
    "    ax_cbar = fig.add_subplot(gs[0, 19])\n",
    "    \n",
    "    # Set custom style parameters\n",
    "    plt.rcParams['figure.facecolor'] = 'white'\n",
    "    plt.rcParams['axes.facecolor'] = 'white'\n",
    "    plt.rcParams['axes.grid'] = True\n",
    "    plt.rcParams['grid.alpha'] = 0.3\n",
    "    \n",
    "    # Create layout with more spread\n",
    "    pos = nx.spring_layout(G, k=1.5, iterations=100, seed=42)  # Increased k and iterations\n",
    "    \n",
    "    # Draw edges\n",
    "    edges = nx.draw_networkx_edges(G, pos, \n",
    "                                 edge_color=edge_colors, \n",
    "                                 width=edge_widths,\n",
    "                                 edge_cmap=plt.cm.Blues,\n",
    "                                 alpha=0.8,  # Slightly increased alpha\n",
    "                                 ax=ax_main)\n",
    "    \n",
    "    # Draw nodes\n",
    "    node_sizes = [G.nodes[node]['size'] for node in G.nodes()]\n",
    "    nodes = nx.draw_networkx_nodes(G, pos,\n",
    "                                 node_color='lightblue',\n",
    "                                 node_size=node_sizes,\n",
    "                                 alpha=0.7,\n",
    "                                 linewidths=2,\n",
    "                                 edgecolors='white',\n",
    "                                 ax=ax_main)\n",
    "    \n",
    "    # Add labels with slightly reduced size to prevent overlap\n",
    "    label_sizes = {node: np.sqrt(size/1000) * 9 for node, size in zip(G.nodes(), node_sizes)}  # Reduced multiplier from 10 to 9\n",
    "    nx.draw_networkx_labels(G, pos, font_size=label_sizes, font_weight='bold', ax=ax_main)\n",
    "    \n",
    "    # Add title\n",
    "    ax_main.set_title('Domain Co-occurrence Network', fontsize=24, pad=20, fontweight='bold')\n",
    "    ax_main.axis('off')\n",
    "    \n",
    "    # Add colorbar with shifted color scale\n",
    "    # Create custom normalization to better show the relationship\n",
    "    from matplotlib.colors import Normalize\n",
    "    \n",
    "    # Create custom tick positions and labels for the colorbar\n",
    "    tick_positions = np.linspace(0, 1, 5)\n",
    "    if max_weight > min_weight:\n",
    "        # Calculate corresponding values using the inverse of our transformation\n",
    "        tick_values = [min_weight + (np.exp(p * np.log1p(max_weight - min_weight)) - 1) for p in tick_positions]\n",
    "        tick_labels = [f\"{int(v)}\" for v in tick_values]\n",
    "    else:\n",
    "        tick_values = [min_weight] * 5\n",
    "        tick_labels = [f\"{int(min_weight)}\"] * 5\n",
    "    \n",
    "    norm = plt.Normalize(0, 1)  # We'll use our custom mapping\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, cax=ax_cbar, ticks=tick_positions)\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "    cbar.set_label('Number of Co-occurrences', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, \"domain_cooccurrence_network.png\", subject_main_dir, \"domains\")\n",
    "    \n",
    "    # Return the graph object for potential further analysis\n",
    "    return G\n",
    "\n",
    "# Function to analyze and print network statistics\n",
    "def print_network_stats(G):\n",
    "    \"\"\"\n",
    "    Print basic network statistics\n",
    "    \"\"\"\n",
    "    print(\"\\nNetwork Statistics:\")\n",
    "    print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "    print(f\"Network density: {nx.density(G):.3f}\")\n",
    "    print(\"\\nTop 5 domains by degree centrality:\")\n",
    "    degree_cent = nx.degree_centrality(G)\n",
    "    for node, centrality in sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"{node}: {centrality:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694765dc-cc02-4569-bf9e-a7c953aabfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = create_domain_network(df, subject_main_dir)\n",
    "print_network_stats(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3161ec2-2cfc-4345-8541-70486f3097d7",
   "metadata": {},
   "source": [
    "### Community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac2252-982e-46f6-bdf4-777c4940cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "import community.community_louvain as community_louvain  # Updated import statement\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def analyze_domain_communities(df, subject_main_dir):\n",
    "    \"\"\"\n",
    "    Analyze and visualize domain communities and cliques\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The preprocessed DataFrame\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    \"\"\"\n",
    "    # First get the network using our previous approach\n",
    "    G = create_domain_network(df, subject_main_dir)\n",
    "    \n",
    "    # Detect communities using Louvain method\n",
    "    communities = community_louvain.best_partition(G)\n",
    "    \n",
    "    # Find all maximal cliques of size 3 or larger\n",
    "    cliques = list(nx.find_cliques(G))\n",
    "    significant_cliques = [c for c in cliques if len(c) >= 3]\n",
    "    \n",
    "    # Print community and clique statistics\n",
    "    print(\"\\nCommunity Statistics:\")\n",
    "    print(f\"Number of communities: {len(set(communities.values()))}\")\n",
    "    \n",
    "    print(\"\\nLargest communities:\")\n",
    "    community_counts = Counter(communities.values())\n",
    "    for comm_id, count in sorted(community_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        members = [node for node, c_id in communities.items() if c_id == comm_id]\n",
    "        print(f\"\\nCommunity {comm_id} (Size: {count}):\")\n",
    "        print(f\"Members: {', '.join(members)}\")\n",
    "    \n",
    "    print(\"\\nClique Analysis:\")\n",
    "    print(f\"Number of maximal cliques (size ≥ 3): {len(significant_cliques)}\")\n",
    "    print(\"\\nLargest cliques:\")\n",
    "    for clique in sorted(significant_cliques, key=len, reverse=True)[:5]:\n",
    "        print(f\"Size {len(clique)}: {', '.join(clique)}\")\n",
    "    \n",
    "    # Create visualization with communities\n",
    "    # Set up the figure with GridSpec\n",
    "    fig = plt.figure(figsize=(24, 22), facecolor='white')\n",
    "    gs = gridspec.GridSpec(1, 20)\n",
    "    ax_main = fig.add_subplot(gs[0, :19])\n",
    "    ax_cbar = fig.add_subplot(gs[0, 19])\n",
    "    \n",
    "    # Generate colors for communities\n",
    "    n_communities = len(set(communities.values()))\n",
    "    community_colors = plt.cm.tab20(np.linspace(0, 1, n_communities))\n",
    "    \n",
    "    # Create layout with more spread\n",
    "    pos = nx.spring_layout(G, k=1.5, iterations=100, seed=42)\n",
    "    \n",
    "    # Draw edges with previous style but slightly transparent\n",
    "    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    max_weight = max(edge_weights)\n",
    "    min_weight = min(edge_weights)\n",
    "    \n",
    "    edge_colors = []\n",
    "    edge_widths = []\n",
    "    for weight in edge_weights:\n",
    "        color_val = 0.4 + 0.6 * (np.log1p(weight - min_weight) / np.log1p(max_weight - min_weight))\n",
    "        edge_colors.append(color_val)\n",
    "        width = 1 + 4 * (weight - min_weight) / (max_weight - min_weight)\n",
    "        edge_widths.append(width)\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos,\n",
    "                          edge_color=edge_colors,\n",
    "                          width=edge_widths,\n",
    "                          edge_cmap=plt.cm.Blues,\n",
    "                          alpha=0.5,\n",
    "                          ax=ax_main)\n",
    "    \n",
    "    # Draw nodes colored by community\n",
    "    node_colors = [community_colors[communities[node]] for node in G.nodes()]\n",
    "    node_sizes = [G.nodes[node]['size'] for node in G.nodes()]\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos,\n",
    "                          node_color=node_colors,\n",
    "                          node_size=node_sizes,\n",
    "                          alpha=0.7,\n",
    "                          linewidths=2,\n",
    "                          edgecolors='white',\n",
    "                          ax=ax_main)\n",
    "    \n",
    "    # Add labels\n",
    "    label_sizes = {node: np.sqrt(size/1000) * 9 for node, size in zip(G.nodes(), node_sizes)}\n",
    "    nx.draw_networkx_labels(G, pos, font_size=label_sizes, font_weight='bold', ax=ax_main)\n",
    "    \n",
    "    # Add title with community information\n",
    "    ax_main.set_title('Domain Co-occurrence Network\\nColored by Communities', \n",
    "                      fontsize=24, pad=20, fontweight='bold')\n",
    "    ax_main.axis('off')\n",
    "    \n",
    "    # Add colorbar for edge weights\n",
    "    norm = plt.Normalize(0, 1)\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=norm)\n",
    "    sm.set_array([])\n",
    "    \n",
    "    tick_positions = np.linspace(0, 1, 5)\n",
    "    tick_values = [min_weight + (np.exp(p * np.log1p(max_weight - min_weight)) - 1) \n",
    "                  for p in tick_positions]\n",
    "    tick_labels = [f\"{int(v)}\" for v in tick_values]\n",
    "    \n",
    "    cbar = plt.colorbar(sm, cax=ax_cbar, ticks=tick_positions)\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "    cbar.set_label('Number of Co-occurrences', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the community visualization\n",
    "    save_plot(plt, \"domain_communities_network.png\", subject_main_dir, \"domains\")\n",
    "    \n",
    "    return G, communities, significant_cliques\n",
    "\n",
    "# Additional helper function for detailed community analysis\n",
    "def analyze_community_characteristics(G, communities):\n",
    "    \"\"\"\n",
    "    Analyze characteristics of each community\n",
    "    \"\"\"\n",
    "    community_stats = {}\n",
    "    for node, comm_id in communities.items():\n",
    "        if comm_id not in community_stats:\n",
    "            community_stats[comm_id] = {\n",
    "                'nodes': [],\n",
    "                'internal_edges': 0,\n",
    "                'external_edges': 0,\n",
    "                'total_weight': 0\n",
    "            }\n",
    "        community_stats[comm_id]['nodes'].append(node)\n",
    "    \n",
    "    # Calculate edge statistics\n",
    "    for (u, v, w) in G.edges(data='weight'):\n",
    "        comm_u = communities[u]\n",
    "        comm_v = communities[v]\n",
    "        if comm_u == comm_v:\n",
    "            community_stats[comm_u]['internal_edges'] += 1\n",
    "            community_stats[comm_u]['total_weight'] += w\n",
    "        else:\n",
    "            community_stats[comm_u]['external_edges'] += 1\n",
    "            community_stats[comm_v]['external_edges'] += 1\n",
    "    \n",
    "    return community_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c897cb0-b7e2-46a4-91d6-030a53ae3062",
   "metadata": {},
   "outputs": [],
   "source": [
    "G, communities, significant_cliques = analyze_domain_communities(df, subject_main_dir)\n",
    "community_stats = analyze_community_characteristics(G, communities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c7be73-067c-4c2d-a904-85fc61019470",
   "metadata": {},
   "source": [
    "### Temporal graph analysis- domains over decades trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa2b0a-4caf-4727-a4a9-f8905c793b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "import community.community_louvain as community_louvain\n",
    "\n",
    "def analyze_temporal_domains(df, subject_main_dir):\n",
    "    \"\"\"\n",
    "    Analyze and visualize domain relationships across different decades\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The preprocessed DataFrame\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    \"\"\"\n",
    "    # First preprocess the DataFrame\n",
    "    df = preprocess_dataframe(df)\n",
    "    \n",
    "    # Create decade bins\n",
    "    df['Decade'] = (df['Publication Year'] // 10) * 10\n",
    "    decades = sorted(df['Decade'].unique())\n",
    "    \n",
    "    # Store network metrics for each decade\n",
    "    temporal_metrics = {\n",
    "        'n_nodes': [],\n",
    "        'n_edges': [],\n",
    "        'density': [],\n",
    "        'avg_clustering': [],\n",
    "        'top_domains': []\n",
    "    }\n",
    "    \n",
    "    # Create subplots for network visualization\n",
    "    n_decades = len(decades)\n",
    "    n_cols = min(3, n_decades)\n",
    "    n_rows = (n_decades + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig = plt.figure(figsize=(8*n_cols, 8*n_rows))\n",
    "    \n",
    "    # Process each decade\n",
    "    for idx, decade in enumerate(decades):\n",
    "        decade_data = df[df['Decade'] == decade]\n",
    "        \n",
    "        # Create co-occurrence dictionary for this decade\n",
    "        cooccurrence_dict = {}\n",
    "        domain_counts = Counter()\n",
    "        \n",
    "        # Process each paper's domains in this decade\n",
    "        for domains in decade_data['Domains']:\n",
    "            domain_counts.update(domains)\n",
    "            for d1, d2 in combinations(sorted(set(domains)), 2):\n",
    "                pair = tuple(sorted([d1, d2]))\n",
    "                cooccurrence_dict[pair] = cooccurrence_dict.get(pair, 0) + 1\n",
    "        \n",
    "        # Create network\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes with size based on frequency\n",
    "        max_count = max(domain_counts.values()) if domain_counts else 1\n",
    "        for domain, count in domain_counts.items():\n",
    "            G.add_node(domain, size=1000 + (count / max_count) * 4000)\n",
    "        \n",
    "        # Add edges with weights\n",
    "        for (d1, d2), weight in cooccurrence_dict.items():\n",
    "            G.add_edge(d1, d2, weight=weight)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        temporal_metrics['n_nodes'].append(G.number_of_nodes())\n",
    "        temporal_metrics['n_edges'].append(G.number_of_edges())\n",
    "        temporal_metrics['density'].append(nx.density(G))\n",
    "        temporal_metrics['avg_clustering'].append(nx.average_clustering(G))\n",
    "        \n",
    "        # Get top domains by degree centrality\n",
    "        deg_cent = nx.degree_centrality(G)\n",
    "        top_domains = sorted(deg_cent.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        temporal_metrics['top_domains'].append(top_domains)\n",
    "        \n",
    "        # Create subplot for this decade\n",
    "        ax = plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        \n",
    "        # Draw network\n",
    "        pos = nx.spring_layout(G, k=1.5, iterations=50)\n",
    "        \n",
    "        # Draw edges with weight-based colors\n",
    "        edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "        if edge_weights:\n",
    "            max_weight = max(edge_weights)\n",
    "            min_weight = min(edge_weights)\n",
    "            edge_colors = [0.4 + 0.6 * (np.log1p(w - min_weight) / np.log1p(max_weight - min_weight))\n",
    "                         for w in edge_weights]\n",
    "        else:\n",
    "            edge_colors = []\n",
    "        \n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "                             edge_color=edge_colors,\n",
    "                             edge_cmap=plt.cm.Blues,\n",
    "                             alpha=0.5,\n",
    "                             width=1)\n",
    "        \n",
    "        # Draw nodes\n",
    "        node_sizes = [G.nodes[node]['size'] for node in G.nodes()]\n",
    "        nx.draw_networkx_nodes(G, pos,\n",
    "                             node_color='lightblue',\n",
    "                             node_size=[s/4 for s in node_sizes],  # Smaller nodes for subplot\n",
    "                             alpha=0.7,\n",
    "                             linewidths=1,\n",
    "                             edgecolors='white')\n",
    "        \n",
    "        # Add minimal labels (only for top domains)\n",
    "        top_domain_names = [d[0] for d in top_domains[:3]]  # Show only top 3 for clarity\n",
    "        labels = {node: node if node in top_domain_names else '' for node in G.nodes()}\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold')\n",
    "        \n",
    "        plt.title(f'{decade}s\\nNodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, \"temporal_domain_networks.png\", subject_main_dir, \"domains\")\n",
    "    \n",
    "    # Create trend visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "    \n",
    "    # Plot network metrics trends\n",
    "    ax1.plot(decades, temporal_metrics['n_nodes'], 'o-', label='Number of Domains')\n",
    "    ax1.plot(decades, temporal_metrics['n_edges'], 's-', label='Number of Co-Appearnces')\n",
    "    ax1.set_xlabel('Decade')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Network Size Evolution')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot network characteristics trends\n",
    "    ax2.plot(decades, temporal_metrics['density'], 'o-', label='Network Density')\n",
    "    ax2.plot(decades, temporal_metrics['avg_clustering'], 's-', label='Average Clustering')\n",
    "    ax2.set_xlabel('Decade')\n",
    "    ax2.set_ylabel('Value')\n",
    "    ax2.set_title('Network Characteristics Evolution')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, \"temporal_domain_metrics.png\", subject_main_dir, \"domains\")\n",
    "    \n",
    "    return temporal_metrics\n",
    "\n",
    "def print_temporal_analysis(temporal_metrics, decades):\n",
    "    \"\"\"Print detailed analysis of temporal trends\"\"\"\n",
    "    print(\"\\nTemporal Analysis of Domain Networks:\")\n",
    "    print(\"\\nNetwork Size Evolution:\")\n",
    "    for decade, nodes, edges in zip(decades, temporal_metrics['n_nodes'], temporal_metrics['n_edges']):\n",
    "        print(f\"\\n{decade}s:\")\n",
    "        print(f\"  Nodes: {nodes}\")\n",
    "        print(f\"  Edges: {edges}\")\n",
    "        print(f\"  Density: {temporal_metrics['density'][decades.index(decade)]:.3f}\")\n",
    "        print(f\"  Avg Clustering: {temporal_metrics['avg_clustering'][decades.index(decade)]:.3f}\")\n",
    "        print(\"  Top Domains:\")\n",
    "        for domain, centrality in temporal_metrics['top_domains'][decades.index(decade)][:3]:\n",
    "            print(f\"    - {domain}: {centrality:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef51b6-de78-412e-94fd-382f8b75640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_metrics = analyze_temporal_domains(df, subject_main_dir)\n",
    "decades = sorted(df['Publication Year'].apply(lambda x: (x // 10) * 10).unique())\n",
    "print_temporal_analysis(temporal_metrics, decades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a9f65-d236-4811-a6f0-96c3476ae4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import community.community_louvain as community_louvain\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "def safe_centrality_calculation(G, calc_function, default_value=None):\n",
    "    \"\"\"\n",
    "    Safely calculate centrality metrics with error handling\n",
    "    \"\"\"\n",
    "    if G.number_of_edges() == 0:\n",
    "        return {node: 0 for node in G.nodes()}\n",
    "    try:\n",
    "        return calc_function(G)\n",
    "    except:\n",
    "        if default_value is None:\n",
    "            return {node: 0 for node in G.nodes()}\n",
    "        return default_value\n",
    "\n",
    "def analyze_temporal_centrality_communities(df, subject_main_dir):\n",
    "    \"\"\"\n",
    "    Analyze central domains and community evolution over decades with edge case handling\n",
    "    \"\"\"\n",
    "    df = preprocess_dataframe(df)\n",
    "    df['Decade'] = (df['Publication Year'] // 10) * 10\n",
    "    decades = sorted(df['Decade'].unique())\n",
    "    \n",
    "    # Store metrics\n",
    "    centrality_evolution = defaultdict(list)\n",
    "    community_evolution = defaultdict(dict)\n",
    "    persistent_communities = defaultdict(list)\n",
    "    all_top_domains = set()\n",
    "    centrality_scores = defaultdict(dict)\n",
    "    \n",
    "    # Create figures\n",
    "    fig_centrality = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Analyze each decade\n",
    "    for decade in decades:\n",
    "        print(f\"\\nProcessing decade: {decade}s\")  # Debug information\n",
    "        decade_data = df[df['Decade'] == decade]\n",
    "        \n",
    "        # Create network\n",
    "        G = nx.Graph()\n",
    "        cooccurrence_dict = {}\n",
    "        domain_counts = Counter()\n",
    "        \n",
    "        # Build network\n",
    "        for domains in decade_data['Domains']:\n",
    "            domain_counts.update(domains)\n",
    "            for d1, d2 in combinations(sorted(set(domains)), 2):\n",
    "                pair = tuple(sorted([d1, d2]))\n",
    "                cooccurrence_dict[pair] = cooccurrence_dict.get(pair, 0) + 1\n",
    "        \n",
    "        # Add nodes and edges\n",
    "        for domain, count in domain_counts.items():\n",
    "            G.add_node(domain)\n",
    "        \n",
    "        for (d1, d2), weight in cooccurrence_dict.items():\n",
    "            G.add_edge(d1, d2, weight=weight)\n",
    "        \n",
    "        print(f\"Network for {decade}s - Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
    "        \n",
    "        # Skip empty or disconnected graphs\n",
    "        if G.number_of_nodes() == 0:\n",
    "            print(f\"Warning: Empty graph for decade {decade}\")\n",
    "            community_evolution[decade] = {\n",
    "                'communities': {},\n",
    "                'modularity': 0,\n",
    "                'num_communities': 0,\n",
    "                'sizes': Counter()\n",
    "            }\n",
    "            continue\n",
    "            \n",
    "        # Calculate centrality metrics with error handling\n",
    "        degree_cent = nx.degree_centrality(G)\n",
    "        betweenness_cent = safe_centrality_calculation(G, nx.betweenness_centrality)\n",
    "        eigenvector_cent = safe_centrality_calculation(\n",
    "            G, \n",
    "            lambda g: nx.eigenvector_centrality(g, max_iter=1000)\n",
    "        )\n",
    "        \n",
    "        # Store top domains\n",
    "        top_domains = sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        all_top_domains.update(domain for domain, _ in top_domains)\n",
    "        \n",
    "        # Store centrality scores\n",
    "        for domain in G.nodes():\n",
    "            centrality_scores[domain][decade] = {\n",
    "                'degree': degree_cent.get(domain, 0),\n",
    "                'betweenness': betweenness_cent.get(domain, 0),\n",
    "                'eigenvector': eigenvector_cent.get(domain, 0)\n",
    "            }\n",
    "        \n",
    "        # Community detection with error handling\n",
    "        try:\n",
    "            if G.number_of_edges() > 0:\n",
    "                communities = community_louvain.best_partition(G)\n",
    "                modularity = community_louvain.modularity(communities, G)\n",
    "            else:\n",
    "                # For graphs without edges, each node is its own community\n",
    "                communities = {node: idx for idx, node in enumerate(G.nodes())}\n",
    "                modularity = 0\n",
    "                print(f\"Warning: No edges in graph for decade {decade}, setting default community values\")\n",
    "            \n",
    "            community_evolution[decade] = {\n",
    "                'communities': communities,\n",
    "                'modularity': modularity,\n",
    "                'num_communities': len(set(communities.values())),\n",
    "                'sizes': Counter(communities.values())\n",
    "            }\n",
    "            \n",
    "            # Track community membership\n",
    "            for domain, comm_id in communities.items():\n",
    "                persistent_communities[domain].append((decade, comm_id))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Community detection failed for decade {decade}: {str(e)}\")\n",
    "            community_evolution[decade] = {\n",
    "                'communities': {node: 0 for node in G.nodes()},\n",
    "                'modularity': 0,\n",
    "                'num_communities': 1,\n",
    "                'sizes': Counter([0] * G.number_of_nodes())\n",
    "            }\n",
    "    \n",
    "    # Visualize centrality evolution for top domains\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for domain in list(all_top_domains)[:10]:  # Plot top 10 domains\n",
    "        decades_present = []\n",
    "        centrality_values = []\n",
    "        for decade in decades:\n",
    "            if domain in centrality_scores and decade in centrality_scores[domain]:\n",
    "                decades_present.append(decade)\n",
    "                centrality_values.append(centrality_scores[domain][decade]['degree'])\n",
    "        if decades_present:\n",
    "            plt.plot(decades_present, centrality_values, 'o-', label=domain)\n",
    "    \n",
    "    plt.title('Evolution of Domain Centrality Over Time')\n",
    "    plt.xlabel('Decade')\n",
    "    plt.ylabel('Degree Centrality')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    save_plot(plt, \"domain_centrality_evolution.png\", subject_main_dir, \"domains\")\n",
    "    \n",
    "    # Visualize community evolution (only for decades with valid data)\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    valid_decades = []\n",
    "    modularity_values = []\n",
    "    num_communities = []\n",
    "    \n",
    "    for decade in decades:\n",
    "        if community_evolution[decade]['num_communities'] > 0:\n",
    "            valid_decades.append(decade)\n",
    "            modularity_values.append(community_evolution[decade]['modularity'])\n",
    "            num_communities.append(community_evolution[decade]['num_communities'])\n",
    "    \n",
    "    if valid_decades:  # Only plot if we have valid data\n",
    "        plt.plot(valid_decades, modularity_values, 'o-', label='Modularity')\n",
    "        plt.plot(valid_decades, \n",
    "                [n/max(num_communities) for n in num_communities], \n",
    "                's-', \n",
    "                label='Normalized Number of Communities')\n",
    "        plt.title('Evolution of Community Structure')\n",
    "        plt.xlabel('Decade')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        save_plot(plt, \"community_evolution.png\", subject_main_dir, \"domains\")\n",
    "    \n",
    "    return centrality_scores, community_evolution, persistent_communities\n",
    "\n",
    "def print_temporal_centrality_analysis(centrality_scores, community_evolution, persistent_communities, decades):\n",
    "    \"\"\"Print detailed analysis of temporal patterns with edge case handling\"\"\"\n",
    "    print(\"\\nTemporal Analysis of Domain Centrality and Communities:\")\n",
    "    \n",
    "    if not centrality_scores:\n",
    "        print(\"No valid centrality scores found for any decade.\")\n",
    "        return\n",
    "    \n",
    "    # Analyze domain persistence\n",
    "    print(\"\\nMost Persistent Central Domains:\")\n",
    "    domain_persistence = {}\n",
    "    for domain in centrality_scores:\n",
    "        appearances = len(centrality_scores[domain])\n",
    "        if appearances > 0:  # Only include domains that appear in at least one decade\n",
    "            avg_centrality = np.mean([scores['degree'] for scores in centrality_scores[domain].values()])\n",
    "            domain_persistence[domain] = (appearances, avg_centrality)\n",
    "    \n",
    "    if domain_persistence:\n",
    "        for domain, (appearances, avg_cent) in sorted(\n",
    "            domain_persistence.items(), \n",
    "            key=lambda x: (x[1][0], x[1][1]), \n",
    "            reverse=True\n",
    "        )[:10]:\n",
    "            print(f\"{domain}: Present in {appearances} decades, Avg. Centrality: {avg_cent:.3f}\")\n",
    "    else:\n",
    "        print(\"No persistent domains found.\")\n",
    "    \n",
    "    # Analyze community stability\n",
    "    print(\"\\nCommunity Structure Evolution:\")\n",
    "    for decade in decades:\n",
    "        info = community_evolution[decade]\n",
    "        print(f\"\\n{decade}s:\")\n",
    "        print(f\"Number of communities: {info['num_communities']}\")\n",
    "        if info['num_communities'] > 0:\n",
    "            print(f\"Modularity: {info['modularity']:.3f}\")\n",
    "            \n",
    "            # Find largest communities\n",
    "            top_communities = sorted(info['sizes'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            print(\"Largest communities sizes:\", [size for _, size in top_communities])\n",
    "        else:\n",
    "            print(\"No valid community structure for this decade\")\n",
    "    \n",
    "    # Analyze stable domain groups\n",
    "    if persistent_communities:\n",
    "        print(\"\\nStable Domain Groups (domains that frequently appear together in communities):\")\n",
    "        stable_groups = defaultdict(list)\n",
    "        for domain, history in persistent_communities.items():\n",
    "            if len(history) >= len(decades) * 0.5:  # Present in at least half of the decades\n",
    "                community_pattern = tuple(comm_id for _, comm_id in history)\n",
    "                stable_groups[community_pattern].append(domain)\n",
    "        \n",
    "        if stable_groups:\n",
    "            for pattern, domains in sorted(stable_groups.items(), key=lambda x: len(x[1]), reverse=True)[:5]:\n",
    "                print(f\"\\nStable group with {len(domains)} domains:\")\n",
    "                print(\", \".join(domains))\n",
    "        else:\n",
    "            print(\"No stable domain groups found\")\n",
    "    else:\n",
    "        print(\"\\nNo persistent communities found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a29b0f-92f2-40fa-8a8e-b5dcad455d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_scores, community_evolution, persistent_communities = analyze_temporal_centrality_communities(df, subject_main_dir)\n",
    "decades = sorted(df['Publication Year'].apply(lambda x: (x // 10) * 10).unique())\n",
    "print_temporal_centrality_analysis(centrality_scores, community_evolution, persistent_communities, decades)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf531c-2ee6-4e4d-8dd5-ff6eee43c5b7",
   "metadata": {},
   "source": [
    "# Graph analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19188b-fcec-41e4-9185-847b548d319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "import ast\n",
    "\n",
    "def preprocess_column(df, column_name):\n",
    "    \"\"\"\n",
    "    Preprocess a column to ensure it contains lists.\n",
    "    For concept_dict, extract keys as a list.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame\n",
    "    column_name (str): Name of the column to process\n",
    "    \n",
    "    Returns:\n",
    "    pandas.Series: Series with list values\n",
    "    \"\"\"\n",
    "    processed_series = pd.Series(index=df.index, dtype=object)\n",
    "    \n",
    "    for idx, value in df[column_name].items():\n",
    "        try:\n",
    "            # Handle concept_dict specially - extract keys\n",
    "            if column_name == 'concept_dict':\n",
    "                if isinstance(value, str):\n",
    "                    concept_dict = ast.literal_eval(value)\n",
    "                else:\n",
    "                    concept_dict = value\n",
    "                processed_series[idx] = list(concept_dict.keys())\n",
    "            \n",
    "            # Handle string representations of lists\n",
    "            elif isinstance(value, str):\n",
    "                if value.startswith('[') and value.endswith(']'):\n",
    "                    processed_series[idx] = ast.literal_eval(value)\n",
    "                else:\n",
    "                    # Handle comma-separated strings\n",
    "                    processed_series[idx] = [item.strip() for item in value.split(',')]\n",
    "            \n",
    "            # Handle already-list values\n",
    "            elif isinstance(value, list):\n",
    "                processed_series[idx] = value\n",
    "            \n",
    "            # Handle other cases\n",
    "            else:\n",
    "                processed_series[idx] = [str(value)]\n",
    "                \n",
    "        except (ValueError, SyntaxError, TypeError):\n",
    "            # Default to empty list for problematic entries\n",
    "            processed_series[idx] = []\n",
    "    \n",
    "    return processed_series\n",
    "\n",
    "def create_network_visualization(df, column_name, subject_main_dir, top_n=None, min_occurrences=2):\n",
    "    \"\"\"\n",
    "    Create and save a network visualization of item co-occurrences\n",
    "    for any list column (including concept_dict)\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame\n",
    "    column_name (str): Name of the column to visualize\n",
    "    subject_main_dir (str/Path): Directory to save the visualization\n",
    "    top_n (int, optional): Limit to top N most frequent items\n",
    "    min_occurrences (int): Minimum number of occurrences to include an item\n",
    "    \n",
    "    Returns:\n",
    "    networkx.Graph: The network graph object\n",
    "    \"\"\"\n",
    "    # Get subject type from column name\n",
    "    subject_type = column_name.lower().split('_')[0]\n",
    "    if subject_type == 'concept':\n",
    "        subject_type = 'concepts'\n",
    "    elif not subject_type.endswith('s'):\n",
    "        subject_type = f\"{subject_type}s\"\n",
    "    \n",
    "    # Preprocess the column to get lists\n",
    "    items_series = preprocess_column(df, column_name)\n",
    "    \n",
    "    # Create co-occurrence dictionary\n",
    "    cooccurrence_dict = {}\n",
    "    # Count total occurrences of each item for node sizing\n",
    "    item_counts = Counter()\n",
    "    \n",
    "    # Process each record's items\n",
    "    for items in items_series:\n",
    "        if not items:  # Skip empty lists\n",
    "            continue\n",
    "            \n",
    "        # Count individual items\n",
    "        item_counts.update(items)\n",
    "        \n",
    "        # Count co-occurrences\n",
    "        for i1, i2 in combinations(sorted(set(items)), 2):\n",
    "            pair = tuple(sorted([i1, i2]))\n",
    "            cooccurrence_dict[pair] = cooccurrence_dict.get(pair, 0) + 1\n",
    "    \n",
    "    # Filter items by minimum occurrences\n",
    "    item_counts = {item: count for item, count in item_counts.items() \n",
    "                  if count >= min_occurrences}\n",
    "    \n",
    "    # Optionally filter to top N items\n",
    "    if top_n is not None and len(item_counts) > top_n:\n",
    "        top_items = set(dict(Counter(item_counts).most_common(top_n)).keys())\n",
    "        item_counts = {item: count for item, count in item_counts.items() \n",
    "                      if item in top_items}\n",
    "        \n",
    "        # Filter co-occurrences to only include top items\n",
    "        cooccurrence_dict = {(i1, i2): count for (i1, i2), count in cooccurrence_dict.items() \n",
    "                            if i1 in top_items and i2 in top_items}\n",
    "    \n",
    "    # Create network\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes with size based on frequency\n",
    "    max_count = max(item_counts.values()) if item_counts else 1\n",
    "    for item, count in item_counts.items():\n",
    "        # Normalize node size between 1000 and 5000\n",
    "        node_size = 1000 + (count / max_count) * 4000\n",
    "        G.add_node(item, size=node_size, count=count)\n",
    "    \n",
    "    # Add edges with weights\n",
    "    edge_weights = []\n",
    "    for (i1, i2), weight in cooccurrence_dict.items():\n",
    "        # Only add edges between nodes that exist in our filtered graph\n",
    "        if i1 in G.nodes and i2 in G.nodes:\n",
    "            G.add_edge(i1, i2, weight=weight)\n",
    "            edge_weights.append(weight)\n",
    "    \n",
    "    # Calculate edge width and color ranges\n",
    "    if edge_weights:\n",
    "        max_weight = max(edge_weights)\n",
    "        min_weight = min(edge_weights)\n",
    "    else:\n",
    "        max_weight = min_weight = 1\n",
    "    \n",
    "    # Create color map with shifted scale to make lower values more visible\n",
    "    edge_colors = []\n",
    "    edge_widths = []\n",
    "    \n",
    "    for (i1, i2) in G.edges():\n",
    "        weight = G[i1][i2]['weight']\n",
    "        # Shift the color scale to make lower co-occurrences darker\n",
    "        # Using logarithmic scale to enhance visibility of lower values\n",
    "        if max_weight > min_weight:\n",
    "            # Using a non-linear transformation to enhance lower values\n",
    "            color_val = 0.4 + 0.6 * (np.log1p(weight - min_weight) / np.log1p(max_weight - min_weight))\n",
    "        else:\n",
    "            color_val = 0.5\n",
    "        edge_colors.append(color_val)\n",
    "        \n",
    "        # Normalize width (1 to 5 to maintain clarity)\n",
    "        width = 1 + 4 * (weight - min_weight) / (max_weight - min_weight) if max_weight > min_weight else 3\n",
    "        edge_widths.append(width)\n",
    "    \n",
    "    # Set up the figure with GridSpec\n",
    "    fig = plt.figure(figsize=(24, 22), facecolor='white')\n",
    "    gs = gridspec.GridSpec(1, 20)  # 1 row, 20 columns for fine control\n",
    "    \n",
    "    # Main plot area (using 19 columns)\n",
    "    ax_main = fig.add_subplot(gs[0, :19])\n",
    "    \n",
    "    # Colorbar area (using 1 column)\n",
    "    ax_cbar = fig.add_subplot(gs[0, 19])\n",
    "    \n",
    "    # Set custom style parameters\n",
    "    plt.rcParams['figure.facecolor'] = 'white'\n",
    "    plt.rcParams['axes.facecolor'] = 'white'\n",
    "    plt.rcParams['axes.grid'] = True\n",
    "    plt.rcParams['grid.alpha'] = 0.3\n",
    "    \n",
    "    # Create layout with more spread\n",
    "    pos = nx.spring_layout(G, k=1.5, iterations=100, seed=42)\n",
    "    \n",
    "    # Draw edges\n",
    "    edges = nx.draw_networkx_edges(G, pos, \n",
    "                                 edge_color=edge_colors, \n",
    "                                 width=edge_widths,\n",
    "                                 edge_cmap=plt.cm.Blues,\n",
    "                                 alpha=0.8,\n",
    "                                 ax=ax_main)\n",
    "    \n",
    "    # Draw nodes\n",
    "    node_sizes = [G.nodes[node]['size'] for node in G.nodes()]\n",
    "    nodes = nx.draw_networkx_nodes(G, pos,\n",
    "                                 node_color='lightblue',\n",
    "                                 node_size=node_sizes,\n",
    "                                 alpha=0.7,\n",
    "                                 linewidths=2,\n",
    "                                 edgecolors='white',\n",
    "                                 ax=ax_main)\n",
    "    \n",
    "    # Add labels with slightly reduced size to prevent overlap\n",
    "    label_sizes = {node: np.sqrt(size/1000) * 9 for node, size in zip(G.nodes(), node_sizes)}\n",
    "    nx.draw_networkx_labels(G, pos, font_size=label_sizes, font_weight='bold', ax=ax_main)\n",
    "    \n",
    "    # Add title - make it more descriptive based on column\n",
    "    title = f\"{column_name.replace('_', ' ').title()} Co-occurrence Network\"\n",
    "    if top_n:\n",
    "        title += f\" (Top {top_n})\"\n",
    "    ax_main.set_title(title, fontsize=24, pad=20, fontweight='bold')\n",
    "    ax_main.axis('off')\n",
    "    \n",
    "    # Add colorbar with shifted color scale\n",
    "    # Create custom normalization to better show the relationship\n",
    "    from matplotlib.colors import Normalize\n",
    "    \n",
    "    # Create custom tick positions and labels for the colorbar\n",
    "    tick_positions = np.linspace(0, 1, 5)\n",
    "    if max_weight > min_weight:\n",
    "        # Calculate corresponding values using the inverse of our transformation\n",
    "        tick_values = [min_weight + (np.exp(p * np.log1p(max_weight - min_weight)) - 1) for p in tick_positions]\n",
    "        tick_labels = [f\"{int(v)}\" for v in tick_values]\n",
    "    else:\n",
    "        tick_values = [min_weight] * 5\n",
    "        tick_labels = [f\"{int(min_weight)}\"] * 5\n",
    "    \n",
    "    norm = plt.Normalize(0, 1)  # We'll use our custom mapping\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, cax=ax_cbar, ticks=tick_positions)\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "    cbar.set_label('Number of Co-occurrences', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create filename from column name\n",
    "    filename = f\"{column_name.lower().replace('_', '_')}_cooccurrence_network\"\n",
    "    if top_n:\n",
    "        filename += f\"_top_{top_n}\"\n",
    "    filename += \".png\"\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    # Return the graph object for potential further analysis\n",
    "    return G\n",
    "\n",
    "def get_network_stats(G):\n",
    "    \"\"\"\n",
    "    Calculate and return basic network statistics\n",
    "    \n",
    "    Parameters:\n",
    "    G (networkx.Graph): The network graph object\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary of network statistics\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        \"nodes\": G.number_of_nodes(),\n",
    "        \"edges\": G.number_of_edges(),\n",
    "        \"density\": nx.density(G),\n",
    "        \"top_by_degree\": []\n",
    "    }\n",
    "    \n",
    "    # Get top 5 nodes by degree centrality\n",
    "    degree_cent = nx.degree_centrality(G)\n",
    "    stats[\"top_by_degree\"] = [\n",
    "        {\"node\": node, \"centrality\": centrality}\n",
    "        for node, centrality in sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    ]\n",
    "    \n",
    "    # Add average degree\n",
    "    if G.number_of_nodes() > 0:\n",
    "        stats[\"avg_degree\"] = 2 * G.number_of_edges() / G.number_of_nodes()\n",
    "    else:\n",
    "        stats[\"avg_degree\"] = 0\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def print_network_stats(G):\n",
    "    \"\"\"\n",
    "    Print basic network statistics\n",
    "    \n",
    "    Parameters:\n",
    "    G (networkx.Graph): The network graph object\n",
    "    \"\"\"\n",
    "    stats = get_network_stats(G)\n",
    "    \n",
    "    print(\"\\nNetwork Statistics:\")\n",
    "    print(f\"Number of nodes: {stats['nodes']}\")\n",
    "    print(f\"Number of edges: {stats['edges']}\")\n",
    "    print(f\"Network density: {stats['density']:.3f}\")\n",
    "    print(f\"Average degree: {stats['avg_degree']:.3f}\")\n",
    "    \n",
    "    print(\"\\nTop 5 items by degree centrality:\")\n",
    "    for item in stats[\"top_by_degree\"]:\n",
    "        print(f\"{item['node']}: {item['centrality']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256f689-167a-40f8-979f-b76646842489",
   "metadata": {},
   "source": [
    "## Community detection and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187b912-8762-4271-aeec-c2519c5470c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "import community.community_louvain as community_louvain\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def analyze_network_communities(G, subject_main_dir, subject_type, min_clique_size=3, top_communities=5, top_cliques=5):\n",
    "    \"\"\"\n",
    "    Analyze and visualize communities and cliques in any network graph\n",
    "    \n",
    "    Parameters:\n",
    "    G (networkx.Graph): The network graph to analyze\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    subject_type (str): Type of subject (e.g., \"domains\", \"concepts\", \"fields\")\n",
    "    min_clique_size (int): Minimum size for significant cliques\n",
    "    top_communities (int): Number of top communities to report\n",
    "    top_cliques (int): Number of top cliques to report\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (communities dict, significant cliques list, community stats dict)\n",
    "    \"\"\"\n",
    "    # Detect communities using Louvain method\n",
    "    communities = community_louvain.best_partition(G)\n",
    "    \n",
    "    # Find all maximal cliques of specified minimum size or larger\n",
    "    cliques = list(nx.find_cliques(G))\n",
    "    significant_cliques = [c for c in cliques if len(c) >= min_clique_size]\n",
    "    \n",
    "    # Print community and clique statistics\n",
    "    print(f\"\\n{subject_type.title()} Community Statistics:\")\n",
    "    print(f\"Number of communities: {len(set(communities.values()))}\")\n",
    "    \n",
    "    print(f\"\\nLargest {subject_type} communities:\")\n",
    "    community_counts = Counter(communities.values())\n",
    "    for comm_id, count in sorted(community_counts.items(), key=lambda x: x[1], reverse=True)[:top_communities]:\n",
    "        members = [node for node, c_id in communities.items() if c_id == comm_id]\n",
    "        print(f\"\\nCommunity {comm_id} (Size: {count}):\")\n",
    "        print(f\"Members: {', '.join(members)}\")\n",
    "    \n",
    "    print(f\"\\n{subject_type.title()} Clique Analysis:\")\n",
    "    print(f\"Number of maximal cliques (size ≥ {min_clique_size}): {len(significant_cliques)}\")\n",
    "    print(\"\\nLargest cliques:\")\n",
    "    for clique in sorted(significant_cliques, key=len, reverse=True)[:top_cliques]:\n",
    "        print(f\"Size {len(clique)}: {', '.join(clique)}\")\n",
    "    \n",
    "    # Create visualization with communities\n",
    "    # Set up the figure with GridSpec\n",
    "    fig = plt.figure(figsize=(24, 22), facecolor='white')\n",
    "    gs = gridspec.GridSpec(1, 20)\n",
    "    ax_main = fig.add_subplot(gs[0, :19])\n",
    "    ax_cbar = fig.add_subplot(gs[0, 19])\n",
    "    \n",
    "    # Generate colors for communities\n",
    "    n_communities = len(set(communities.values()))\n",
    "    community_colors = plt.cm.tab20(np.linspace(0, 1, n_communities))\n",
    "    \n",
    "    # Create layout with more spread\n",
    "    pos = nx.spring_layout(G, k=1.5, iterations=100, seed=42)\n",
    "    \n",
    "    # Draw edges with style based on weight\n",
    "    if G.number_of_edges() > 0:\n",
    "        edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "        max_weight = max(edge_weights) if edge_weights else 1\n",
    "        min_weight = min(edge_weights) if edge_weights else 1\n",
    "        \n",
    "        edge_colors = []\n",
    "        edge_widths = []\n",
    "        for weight in edge_weights:\n",
    "            if max_weight > min_weight:\n",
    "                color_val = 0.4 + 0.6 * (np.log1p(weight - min_weight) / np.log1p(max_weight - min_weight))\n",
    "                width = 1 + 4 * (weight - min_weight) / (max_weight - min_weight)\n",
    "            else:\n",
    "                color_val = 0.5\n",
    "                width = 3\n",
    "            edge_colors.append(color_val)\n",
    "            edge_widths.append(width)\n",
    "        \n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "                              edge_color=edge_colors,\n",
    "                              width=edge_widths,\n",
    "                              edge_cmap=plt.cm.Blues,\n",
    "                              alpha=0.5,\n",
    "                              ax=ax_main)\n",
    "    \n",
    "    # Draw nodes colored by community\n",
    "    node_colors = [community_colors[communities[node]] for node in G.nodes()]\n",
    "    \n",
    "    # Check if 'size' attribute exists for all nodes, otherwise use default\n",
    "    if all('size' in G.nodes[node] for node in G.nodes()):\n",
    "        node_sizes = [G.nodes[node]['size'] for node in G.nodes()]\n",
    "    else:\n",
    "        # Use default size based on degree if size attribute not available\n",
    "        node_sizes = [300 + 100 * G.degree(node) for node in G.nodes()]\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos,\n",
    "                          node_color=node_colors,\n",
    "                          node_size=node_sizes,\n",
    "                          alpha=0.7,\n",
    "                          linewidths=2,\n",
    "                          edgecolors='white',\n",
    "                          ax=ax_main)\n",
    "    \n",
    "    # Add labels - scale by node size if available\n",
    "    if all('size' in G.nodes[node] for node in G.nodes()):\n",
    "        label_sizes = {node: np.sqrt(size/1000) * 9 for node, size in zip(G.nodes(), node_sizes)}\n",
    "    else:\n",
    "        # Default font size based on degree if size attribute not available\n",
    "        label_sizes = {node: 9 + 2 * np.sqrt(G.degree(node)) for node in G.nodes()}\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, font_size=label_sizes, font_weight='bold', ax=ax_main)\n",
    "    \n",
    "    # Add title with community information\n",
    "    title = f\"{subject_type.title()} Co-occurrence Network\\nColored by Communities\"\n",
    "    ax_main.set_title(title, fontsize=24, pad=20, fontweight='bold')\n",
    "    ax_main.axis('off')\n",
    "    \n",
    "    # Add colorbar for edge weights if edges exist\n",
    "    if G.number_of_edges() > 0:\n",
    "        norm = plt.Normalize(0, 1)\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=norm)\n",
    "        sm.set_array([])\n",
    "        \n",
    "        tick_positions = np.linspace(0, 1, 5)\n",
    "        if max_weight > min_weight:\n",
    "            tick_values = [min_weight + (np.exp(p * np.log1p(max_weight - min_weight)) - 1) \n",
    "                          for p in tick_positions]\n",
    "        else:\n",
    "            tick_values = [min_weight] * 5\n",
    "        tick_labels = [f\"{int(v)}\" for v in tick_values]\n",
    "        \n",
    "        cbar = plt.colorbar(sm, cax=ax_cbar, ticks=tick_positions)\n",
    "        cbar.set_ticklabels(tick_labels)\n",
    "        cbar.set_label('Number of Co-occurrences', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create filename based on subject type\n",
    "    filename = f\"{subject_type.lower()}_communities_network.png\"\n",
    "    \n",
    "    # Save the community visualization\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    # Calculate detailed community statistics\n",
    "    community_stats = analyze_community_characteristics(G, communities)\n",
    "    \n",
    "    return communities, significant_cliques, community_stats\n",
    "\n",
    "def analyze_community_characteristics(G, communities):\n",
    "    \"\"\"\n",
    "    Analyze characteristics of each community\n",
    "    \n",
    "    Parameters:\n",
    "    G (networkx.Graph): The network graph\n",
    "    communities (dict): Community assignment dictionary\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary of community statistics\n",
    "    \"\"\"\n",
    "    community_stats = {}\n",
    "    for node, comm_id in communities.items():\n",
    "        if comm_id not in community_stats:\n",
    "            community_stats[comm_id] = {\n",
    "                'nodes': [],\n",
    "                'internal_edges': 0,\n",
    "                'external_edges': 0,\n",
    "                'total_weight': 0,\n",
    "                'cohesion': 0,  # Will calculate below\n",
    "                'size': 0,       # Will update below\n",
    "                'central_nodes': []  # Will populate below\n",
    "            }\n",
    "        community_stats[comm_id]['nodes'].append(node)\n",
    "    \n",
    "    # Update size for each community\n",
    "    for comm_id in community_stats:\n",
    "        community_stats[comm_id]['size'] = len(community_stats[comm_id]['nodes'])\n",
    "    \n",
    "    # Calculate edge statistics\n",
    "    for (u, v, data) in G.edges(data=True):\n",
    "        weight = data.get('weight', 1)  # Default to 1 if weight not specified\n",
    "        comm_u = communities[u]\n",
    "        comm_v = communities[v]\n",
    "        if comm_u == comm_v:\n",
    "            community_stats[comm_u]['internal_edges'] += 1\n",
    "            community_stats[comm_u]['total_weight'] += weight\n",
    "        else:\n",
    "            community_stats[comm_u]['external_edges'] += 1\n",
    "            community_stats[comm_v]['external_edges'] += 1\n",
    "    \n",
    "    # Calculate cohesion (ratio of internal to total possible edges)\n",
    "    for comm_id, stats in community_stats.items():\n",
    "        n = stats['size']\n",
    "        possible_edges = n * (n - 1) / 2  # Maximum possible edges in the community\n",
    "        if possible_edges > 0:\n",
    "            stats['cohesion'] = stats['internal_edges'] / possible_edges\n",
    "        else:\n",
    "            stats['cohesion'] = 0\n",
    "        \n",
    "        # Find central nodes (top 3 by degree, or fewer if community is small)\n",
    "        nodes_in_comm = stats['nodes']\n",
    "        subgraph = G.subgraph(nodes_in_comm)\n",
    "        degrees = dict(subgraph.degree())\n",
    "        top_n = min(3, len(nodes_in_comm))\n",
    "        stats['central_nodes'] = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    \n",
    "    return community_stats\n",
    "\n",
    "def visualize_community_stats(community_stats, subject_main_dir, subject_type):\n",
    "    \"\"\"\n",
    "    Create visualizations of community statistics\n",
    "    \n",
    "    Parameters:\n",
    "    community_stats (dict): Dictionary of community statistics\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    subject_type (str): Type of subject (e.g., \"domains\", \"concepts\")\n",
    "    \"\"\"\n",
    "    # Extract data for plotting\n",
    "    comm_ids = list(community_stats.keys())\n",
    "    sizes = [stats['size'] for stats in community_stats.values()]\n",
    "    cohesion = [stats['cohesion'] for stats in community_stats.values()]\n",
    "    internal_edges = [stats['internal_edges'] for stats in community_stats.values()]\n",
    "    external_edges = [stats['external_edges'] for stats in community_stats.values()]\n",
    "    \n",
    "    # Sort communities by size for better visualization\n",
    "    sorted_indices = np.argsort(sizes)[::-1]  # Descending order\n",
    "    top_15_indices = sorted_indices[:15]  # Only show top 15 communities\n",
    "    \n",
    "    top_ids = [comm_ids[i] for i in top_15_indices]\n",
    "    top_sizes = [sizes[i] for i in top_15_indices]\n",
    "    top_cohesion = [cohesion[i] for i in top_15_indices]\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "    # Plot community sizes\n",
    "    ax1.bar(range(len(top_ids)), top_sizes, color='skyblue')\n",
    "    ax1.set_xticks(range(len(top_ids)))\n",
    "    ax1.set_xticklabels([f'Comm {i}' for i in top_ids], rotation=45)\n",
    "    ax1.set_ylabel('Number of Nodes')\n",
    "    ax1.set_title(f'Top 15 {subject_type.title()} Communities by Size')\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(top_sizes):\n",
    "        ax1.text(i, v + 0.5, str(v), ha='center')\n",
    "    \n",
    "    # Plot community cohesion\n",
    "    ax2.bar(range(len(top_ids)), top_cohesion, color='lightgreen')\n",
    "    ax2.set_xticks(range(len(top_ids)))\n",
    "    ax2.set_xticklabels([f'Comm {i}' for i in top_ids], rotation=45)\n",
    "    ax2.set_ylabel('Cohesion (0-1)')\n",
    "    ax2.set_title(f'Cohesion of Top 15 {subject_type.title()} Communities')\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(top_cohesion):\n",
    "        ax2.text(i, v + 0.02, f'{v:.2f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"{subject_type.lower()}_community_stats.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    # Create an additional visualization of internal vs. external connections\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Top 10 communities\n",
    "    top_10_indices = sorted_indices[:10]\n",
    "    top_10_ids = [comm_ids[i] for i in top_10_indices]\n",
    "    top_10_internal = [internal_edges[i] for i in top_10_indices]\n",
    "    top_10_external = [external_edges[i] for i in top_10_indices]\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    x = np.arange(len(top_10_ids))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, top_10_internal, width, label='Internal Connections', color='#5DA5DA')\n",
    "    plt.bar(x + width/2, top_10_external, width, label='External Connections', color='#FAA43A')\n",
    "    \n",
    "    plt.xlabel('Community ID')\n",
    "    plt.ylabel('Number of Connections')\n",
    "    plt.title(f'Internal vs. External Connections for Top 10 {subject_type.title()} Communities')\n",
    "    plt.xticks(x, [f'Comm {i}' for i in top_10_ids])\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"{subject_type.lower()}_community_connections.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "\n",
    "def find_community_bridges(G, communities, subject_main_dir, subject_type, top_n=10):\n",
    "    \"\"\"\n",
    "    Identify and visualize bridge nodes between communities\n",
    "    \n",
    "    Parameters:\n",
    "    G (networkx.Graph): The network graph\n",
    "    communities (dict): Community assignment dictionary\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    subject_type (str): Type of subject (e.g., \"domains\", \"concepts\")\n",
    "    top_n (int): Number of top bridge nodes to report\n",
    "    \n",
    "    Returns:\n",
    "    list: List of bridge nodes with their scores\n",
    "    \"\"\"\n",
    "    # Calculate betweenness centrality\n",
    "    betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "    \n",
    "    # Calculate cross-community connections for each node\n",
    "    bridge_scores = {}\n",
    "    for node in G.nodes():\n",
    "        node_comm = communities[node]\n",
    "        cross_comm_connections = 0\n",
    "        for neighbor in G.neighbors(node):\n",
    "            if communities[neighbor] != node_comm:\n",
    "                cross_comm_connections += 1\n",
    "        \n",
    "        # Bridge score combines betweenness and cross-community connections\n",
    "        bridge_scores[node] = {\n",
    "            'betweenness': betweenness[node],\n",
    "            'cross_comm_connections': cross_comm_connections,\n",
    "            'bridge_score': betweenness[node] * (1 + cross_comm_connections)\n",
    "        }\n",
    "    \n",
    "    # Sort nodes by bridge score\n",
    "    sorted_bridges = sorted(bridge_scores.items(), key=lambda x: x[1]['bridge_score'], reverse=True)\n",
    "    top_bridges = sorted_bridges[:top_n]\n",
    "    \n",
    "    # Print top bridge nodes\n",
    "    print(f\"\\nTop {top_n} {subject_type.title()} Bridge Nodes Between Communities:\")\n",
    "    for node, scores in top_bridges:\n",
    "        print(f\"{node}: Bridge Score={scores['bridge_score']:.4f}, \"\n",
    "              f\"Betweenness={scores['betweenness']:.4f}, \"\n",
    "              f\"Cross-Comm. Connections={scores['cross_comm_connections']}\")\n",
    "    \n",
    "    # Visualize top bridge nodes\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    bridge_names = [node for node, _ in top_bridges]\n",
    "    bridge_scores = [scores['bridge_score'] for _, scores in top_bridges]\n",
    "    \n",
    "    # Sort by score for better visualization\n",
    "    sorted_indices = np.argsort(bridge_scores)\n",
    "    bridge_names = [bridge_names[i] for i in sorted_indices]\n",
    "    bridge_scores = [bridge_scores[i] for i in sorted_indices]\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    plt.barh(bridge_names, bridge_scores, color='#8CD17D')\n",
    "    plt.xlabel('Bridge Score')\n",
    "    plt.title(f'Top {subject_type.title()} Bridge Nodes Between Communities')\n",
    "    \n",
    "    # Add scores next to bars\n",
    "    for i, score in enumerate(bridge_scores):\n",
    "        plt.text(score + 0.01, i, f'{score:.3f}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"{subject_type.lower()}_bridge_nodes.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    return top_bridges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb57887-f208-4c54-8532-dcc1aa3072be",
   "metadata": {},
   "source": [
    "## Temporal community detection and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e782e-0098-4c82-9cef-cbdc6f75333d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "import community.community_louvain as community_louvain\n",
    "import matplotlib.colors as mcolors\n",
    "import ast\n",
    "\n",
    "def analyze_temporal_networks(df, column_name, subject_main_dir, decade_col='decade', \n",
    "                             min_occurrence=2, top_n=None, min_clique_size=3):\n",
    "    \"\"\"\n",
    "    Analyze and visualize network relationships across different decades for any column type\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame\n",
    "    column_name (str): Name of the column to analyze\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    decade_col (str): Name of the column containing decade information\n",
    "    min_occurrence (int): Minimum occurrence for an item to be included\n",
    "    top_n (int, optional): Limit to top N most frequent items per decade\n",
    "    min_clique_size (int): Minimum size for significant cliques\n",
    "    \n",
    "    Returns:\n",
    "    dict: Temporal metrics and analysis results\n",
    "    \"\"\"\n",
    "    # Get subject type from column name\n",
    "    subject_type = column_name.lower().split('_')[0]\n",
    "    if subject_type == 'concept':\n",
    "        subject_type = 'concepts'\n",
    "    elif not subject_type.endswith('s'):\n",
    "        subject_type = f\"{subject_type}s\"\n",
    "    \n",
    "    # Preprocess the column to get lists\n",
    "    items_series = preprocess_column(df, column_name)\n",
    "    \n",
    "    # Ensure decade column exists\n",
    "    if decade_col not in df.columns:\n",
    "        raise ValueError(f\"Decade column '{decade_col}' not found in DataFrame\")\n",
    "    \n",
    "    # Get sorted decades\n",
    "    decades = sorted(df[decade_col].unique())\n",
    "    \n",
    "    # Store network metrics for each decade\n",
    "    temporal_metrics = {\n",
    "        'decades': decades,\n",
    "        'n_nodes': [],\n",
    "        'n_edges': [],\n",
    "        'density': [],\n",
    "        'avg_clustering': [],\n",
    "        'communities': [],\n",
    "        'cliques': [],\n",
    "        'top_items': [],\n",
    "        'bridge_nodes': [],\n",
    "        'graph_objects': [],\n",
    "        'modularity': [],\n",
    "        'avg_path_length': []\n",
    "    }\n",
    "    \n",
    "    # Used to track item persistence across decades\n",
    "    item_decade_presence = defaultdict(set)\n",
    "    item_decade_centrality = defaultdict(dict)\n",
    "    persistent_connections = defaultdict(int)\n",
    "    \n",
    "    # Create subplots for network visualization\n",
    "    n_decades = len(decades)\n",
    "    n_cols = min(3, n_decades)\n",
    "    n_rows = (n_decades + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig = plt.figure(figsize=(8*n_cols, 8*n_rows))\n",
    "    \n",
    "    # Process each decade\n",
    "    for idx, decade in enumerate(decades):\n",
    "        decade_data = df[df[decade_col] == decade]\n",
    "        decade_items = items_series[decade_data.index]\n",
    "        \n",
    "        # Create co-occurrence dictionary for this decade\n",
    "        cooccurrence_dict = {}\n",
    "        item_counts = Counter()\n",
    "        \n",
    "        # Process each record's items in this decade\n",
    "        for items in decade_items:\n",
    "            if not items:  # Skip empty lists\n",
    "                continue\n",
    "                \n",
    "            # Count individual items\n",
    "            item_counts.update(items)\n",
    "            \n",
    "            # Count co-occurrences\n",
    "            for i1, i2 in combinations(sorted(set(items)), 2):\n",
    "                pair = tuple(sorted([i1, i2]))\n",
    "                cooccurrence_dict[pair] = cooccurrence_dict.get(pair, 0) + 1\n",
    "        \n",
    "        # Filter items by minimum occurrences\n",
    "        item_counts = {item: count for item, count in item_counts.items() \n",
    "                      if count >= min_occurrence}\n",
    "        \n",
    "        # Optionally filter to top N items\n",
    "        if top_n is not None and len(item_counts) > top_n:\n",
    "            top_items = set(dict(Counter(item_counts).most_common(top_n)).keys())\n",
    "            item_counts = {item: count for item, count in item_counts.items() \n",
    "                          if item in top_items}\n",
    "            \n",
    "            # Filter co-occurrences to only include top items\n",
    "            cooccurrence_dict = {(i1, i2): count for (i1, i2), count in cooccurrence_dict.items() \n",
    "                                if i1 in top_items and i2 in top_items}\n",
    "        \n",
    "        # Track item presence across decades\n",
    "        for item in item_counts:\n",
    "            item_decade_presence[item].add(decade)\n",
    "        \n",
    "        # Create network\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes with size based on frequency\n",
    "        max_count = max(item_counts.values()) if item_counts else 1\n",
    "        for item, count in item_counts.items():\n",
    "            # Normalize node size between 1000 and 5000\n",
    "            node_size = 1000 + (count / max_count) * 4000\n",
    "            G.add_node(item, size=node_size, count=count)\n",
    "        \n",
    "        # Add edges with weights\n",
    "        for (i1, i2), weight in cooccurrence_dict.items():\n",
    "            # Only add edges between nodes that exist in our filtered graph\n",
    "            if i1 in G.nodes and i2 in G.nodes:\n",
    "                G.add_edge(i1, i2, weight=weight)\n",
    "                \n",
    "                # Track persistent connections across decades\n",
    "                persistent_connections[(i1, i2)] += 1\n",
    "        \n",
    "        # Skip further analysis if graph is empty\n",
    "        if G.number_of_nodes() == 0:\n",
    "            temporal_metrics['n_nodes'].append(0)\n",
    "            temporal_metrics['n_edges'].append(0)\n",
    "            temporal_metrics['density'].append(0)\n",
    "            temporal_metrics['avg_clustering'].append(0)\n",
    "            temporal_metrics['communities'].append({})\n",
    "            temporal_metrics['cliques'].append([])\n",
    "            temporal_metrics['top_items'].append([])\n",
    "            temporal_metrics['bridge_nodes'].append([])\n",
    "            temporal_metrics['graph_objects'].append(G)\n",
    "            temporal_metrics['modularity'].append(0)\n",
    "            temporal_metrics['avg_path_length'].append(0)\n",
    "            continue\n",
    "        \n",
    "        # Calculate metrics\n",
    "        temporal_metrics['n_nodes'].append(G.number_of_nodes())\n",
    "        temporal_metrics['n_edges'].append(G.number_of_edges())\n",
    "        temporal_metrics['density'].append(nx.density(G))\n",
    "        \n",
    "        # Try to calculate clustering coefficient\n",
    "        try:\n",
    "            temporal_metrics['avg_clustering'].append(nx.average_clustering(G))\n",
    "        except:\n",
    "            temporal_metrics['avg_clustering'].append(0)\n",
    "            \n",
    "        # Try to calculate average path length\n",
    "        try:\n",
    "            # Only calculate for the largest connected component\n",
    "            largest_cc = max(nx.connected_components(G), key=len)\n",
    "            largest_cc_graph = G.subgraph(largest_cc)\n",
    "            temporal_metrics['avg_path_length'].append(nx.average_shortest_path_length(largest_cc_graph))\n",
    "        except:\n",
    "            temporal_metrics['avg_path_length'].append(0)\n",
    "        \n",
    "        # Get top items by degree centrality\n",
    "        deg_cent = nx.degree_centrality(G)\n",
    "        top_items = sorted(deg_cent.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        temporal_metrics['top_items'].append(top_items)\n",
    "        \n",
    "        # Store centrality values for tracking over time\n",
    "        for item, centrality in deg_cent.items():\n",
    "            item_decade_centrality[item][decade] = centrality\n",
    "        \n",
    "        # Detect communities\n",
    "        communities = community_louvain.best_partition(G)\n",
    "        temporal_metrics['communities'].append(communities)\n",
    "        \n",
    "        # Calculate modularity\n",
    "        try:\n",
    "            modularity = community_louvain.modularity(communities, G)\n",
    "            temporal_metrics['modularity'].append(modularity)\n",
    "        except:\n",
    "            temporal_metrics['modularity'].append(0)\n",
    "        \n",
    "        # Find cliques\n",
    "        cliques = list(nx.find_cliques(G))\n",
    "        significant_cliques = [c for c in cliques if len(c) >= min_clique_size]\n",
    "        temporal_metrics['cliques'].append(significant_cliques)\n",
    "        \n",
    "        # Find bridge nodes\n",
    "        betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "        bridge_scores = {}\n",
    "        \n",
    "        for node in G.nodes():\n",
    "            if communities:  # Only calculate if communities exist\n",
    "                node_comm = communities[node]\n",
    "                cross_comm_connections = sum(1 for neighbor in G.neighbors(node) \n",
    "                                          if communities.get(neighbor, -1) != node_comm)\n",
    "                \n",
    "                # Bridge score combines betweenness and cross-community connections\n",
    "                bridge_scores[node] = {\n",
    "                    'betweenness': betweenness[node],\n",
    "                    'cross_comm_connections': cross_comm_connections,\n",
    "                    'bridge_score': betweenness[node] * (1 + cross_comm_connections)\n",
    "                }\n",
    "            else:\n",
    "                bridge_scores[node] = {\n",
    "                    'betweenness': betweenness[node],\n",
    "                    'cross_comm_connections': 0,\n",
    "                    'bridge_score': betweenness[node]\n",
    "                }\n",
    "        \n",
    "        # Sort nodes by bridge score\n",
    "        sorted_bridges = sorted(bridge_scores.items(), key=lambda x: x[1]['bridge_score'], reverse=True)\n",
    "        top_bridges = sorted_bridges[:5]  # Top 5 bridge nodes\n",
    "        temporal_metrics['bridge_nodes'].append(top_bridges)\n",
    "        \n",
    "        # Store graph object for later analysis\n",
    "        temporal_metrics['graph_objects'].append(G)\n",
    "        # Create subplot for this decade\n",
    "        ax = plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        \n",
    "        # Draw network\n",
    "        pos = nx.spring_layout(G, k=1.5, iterations=50, seed=42)\n",
    "        \n",
    "        # Draw edges with weight-based colors\n",
    "        edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "        if edge_weights:\n",
    "            max_weight = max(edge_weights)\n",
    "            min_weight = min(edge_weights)\n",
    "            edge_colors = [0.4 + 0.6 * (np.log1p(w - min_weight) / np.log1p(max_weight - min_weight))\n",
    "                         for w in edge_weights]\n",
    "            edge_widths = [1 + 3 * (w - min_weight) / (max_weight - min_weight) \n",
    "                          if max_weight > min_weight else 1 for w in edge_weights]\n",
    "        else:\n",
    "            edge_colors = []\n",
    "            edge_widths = []\n",
    "        \n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "                             edge_color=edge_colors,\n",
    "                             width=edge_widths if edge_widths else 1,\n",
    "                             edge_cmap=plt.cm.Blues,\n",
    "                             alpha=0.5)\n",
    "        \n",
    "        # Draw nodes with community colors if communities exist\n",
    "        node_sizes = [G.nodes[node]['size'] / 4 for node in G.nodes()]  # Smaller for subplot\n",
    "        \n",
    "        if communities:\n",
    "            # Generate colors for communities\n",
    "            n_communities = len(set(communities.values()))\n",
    "            community_colors = plt.cm.tab20(np.linspace(0, 1, n_communities))\n",
    "            node_colors = [community_colors[communities[node]] for node in G.nodes()]\n",
    "            \n",
    "            nx.draw_networkx_nodes(G, pos,\n",
    "                                 node_color=node_colors,\n",
    "                                 node_size=node_sizes,\n",
    "                                 alpha=0.7,\n",
    "                                 linewidths=1,\n",
    "                                 edgecolors='white')\n",
    "        else:\n",
    "            nx.draw_networkx_nodes(G, pos,\n",
    "                                 node_color='lightblue',\n",
    "                                 node_size=node_sizes,\n",
    "                                 alpha=0.7,\n",
    "                                 linewidths=1,\n",
    "                                 edgecolors='white')\n",
    "        \n",
    "        # Add minimal labels (only for top items and bridge nodes)\n",
    "        important_nodes = set()\n",
    "        if top_items:\n",
    "            important_nodes.update([item[0] for item in top_items[:3]])\n",
    "        if top_bridges:\n",
    "            important_nodes.update([node[0] for node in top_bridges[:2]])\n",
    "        \n",
    "        labels = {node: node if node in important_nodes else '' for node in G.nodes()}\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold')\n",
    "        \n",
    "        # Add title with decade and key metrics\n",
    "        n_communities = len(set(communities.values())) if communities else 0\n",
    "        title = f'{decade}s\\nNodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}'\n",
    "        if n_communities > 0:\n",
    "            title += f'\\nCommunities: {n_communities}'\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create filename\n",
    "    filename = f\"temporal_{subject_type}_networks.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    # Create trend visualizations\n",
    "    visualize_network_metric_trends(temporal_metrics, subject_main_dir, subject_type)\n",
    "    \n",
    "    # Analyze item persistence and centrality trends\n",
    "    item_persistence = {item: len(decades_present) for item, decades_present in item_decade_presence.items()}\n",
    "    persistent_items = {item: decades for item, decades in item_decade_presence.items()\n",
    "                       if len(decades) >= max(2, len(decades) // 3)}  # Present in at least 1/3 of decades\n",
    "    \n",
    "    # Visualize persistent items' centrality trends\n",
    "    visualize_centrality_trends(item_decade_centrality, decades, persistent_items, \n",
    "                               subject_main_dir, subject_type)\n",
    "    \n",
    "    # Analyze persistent connections\n",
    "    persistent_connections = {pair: count for pair, count in persistent_connections.items() \n",
    "                             if count >= max(2, len(decades) // 3)}  # Present in at least 1/3 of decades\n",
    "    \n",
    "    # Visualize persistent connections\n",
    "    visualize_persistent_connections(persistent_connections, item_decade_presence, \n",
    "                                    decades, subject_main_dir, subject_type)\n",
    "    \n",
    "    # Visualize community evolution\n",
    "    visualize_community_evolution(temporal_metrics, subject_main_dir, subject_type)\n",
    "    \n",
    "    # Return all metrics for further analysis\n",
    "    return {\n",
    "        'metrics': temporal_metrics,\n",
    "        'persistent_items': persistent_items,\n",
    "        'persistent_connections': persistent_connections,\n",
    "        'item_centrality': item_decade_centrality\n",
    "    }\n",
    "\n",
    "def visualize_network_metric_trends(temporal_metrics, subject_main_dir, subject_type):\n",
    "    \"\"\"\n",
    "    Visualize trends in network metrics over time\n",
    "    \n",
    "    Parameters:\n",
    "    temporal_metrics (dict): The temporal metrics dictionary\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    subject_type (str): Type of subject (e.g., \"domains\", \"concepts\")\n",
    "    \"\"\"\n",
    "    decades = temporal_metrics['decades']\n",
    "    \n",
    "    # Create figure with 3 subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 18))\n",
    "    \n",
    "    # Plot network size metrics trends\n",
    "    ax1.plot(decades, temporal_metrics['n_nodes'], 'o-', color='#4878D0', linewidth=2, \n",
    "             label=f'Number of {subject_type.title()}')\n",
    "    ax1.plot(decades, temporal_metrics['n_edges'], 's-', color='#EE854A', linewidth=2, \n",
    "             label='Number of Co-occurrences')\n",
    "    ax1.set_xlabel('Decade', fontsize=12)\n",
    "    ax1.set_ylabel('Count', fontsize=12)\n",
    "    ax1.set_title(f'Network Size Evolution for {subject_type.title()}', fontsize=16)\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add values on the points\n",
    "    for i, decade in enumerate(decades):\n",
    "        if i < len(temporal_metrics['n_nodes']):\n",
    "            ax1.annotate(f\"{temporal_metrics['n_nodes'][i]}\", \n",
    "                       (decade, temporal_metrics['n_nodes'][i]),\n",
    "                       textcoords=\"offset points\", \n",
    "                       xytext=(0,10), \n",
    "                       ha='center')\n",
    "            ax1.annotate(f\"{temporal_metrics['n_edges'][i]}\", \n",
    "                       (decade, temporal_metrics['n_edges'][i]),\n",
    "                       textcoords=\"offset points\", \n",
    "                       xytext=(0,10), \n",
    "                       ha='center')\n",
    "    \n",
    "    # Plot network characteristics trends\n",
    "    ax2.plot(decades, temporal_metrics['density'], 'o-', color='#55A868', linewidth=2,\n",
    "             label='Network Density')\n",
    "    ax2.plot(decades, temporal_metrics['avg_clustering'], 's-', color='#C44E52', linewidth=2,\n",
    "             label='Average Clustering')\n",
    "    \n",
    "    # Add modularity if available\n",
    "    if 'modularity' in temporal_metrics and any(temporal_metrics['modularity']):\n",
    "        ax2.plot(decades, temporal_metrics['modularity'], '^-', color='#8172B3', linewidth=2,\n",
    "                label='Modularity')\n",
    "    \n",
    "    ax2.set_xlabel('Decade', fontsize=12)\n",
    "    ax2.set_ylabel('Value', fontsize=12)\n",
    "    ax2.set_title(f'Network Characteristics Evolution for {subject_type.title()}', fontsize=16)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    # Plot community and clique counts\n",
    "    comm_counts = [len(set(comm.values())) if comm else 0 for comm in temporal_metrics['communities']]\n",
    "    clique_counts = [len(cliques) for cliques in temporal_metrics['cliques']]\n",
    "    \n",
    "    ax3.plot(decades, comm_counts, 'o-', color='#8172B3', linewidth=2,\n",
    "             label='Number of Communities')\n",
    "    ax3.plot(decades, clique_counts, 's-', color='#937860', linewidth=2,\n",
    "             label='Number of Significant Cliques')\n",
    "    \n",
    "    # Add avg path length if available\n",
    "    if 'avg_path_length' in temporal_metrics and any(temporal_metrics['avg_path_length']):\n",
    "        # Plot on secondary y-axis\n",
    "        ax3_twin = ax3.twinx()\n",
    "        ax3_twin.plot(decades, temporal_metrics['avg_path_length'], '^-', color='#DA8BC3', linewidth=2,\n",
    "                     label='Avg. Path Length')\n",
    "        ax3_twin.set_ylabel('Average Path Length', fontsize=12)\n",
    "        ax3_twin.legend(fontsize=10, loc='upper right')\n",
    "    \n",
    "    ax3.set_xlabel('Decade', fontsize=12)\n",
    "    ax3.set_ylabel('Count', fontsize=12)\n",
    "    ax3.set_title(f'Community Structure Evolution for {subject_type.title()}', fontsize=16)\n",
    "    ax3.legend(fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f\"temporal_{subject_type}_metrics.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "\n",
    "def visualize_centrality_trends(item_centrality, decades, persistent_items, subject_main_dir, subject_type, top_n=10):\n",
    "    \"\"\"\n",
    "    Visualize trends in item centrality over time for persistent items\n",
    "    \n",
    "    Parameters:\n",
    "    item_centrality (dict): Dictionary of item centrality values by decade\n",
    "    decades (list): List of decades\n",
    "    persistent_items (dict): Dictionary of persistent items and their decades\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    subject_type (str): Type of subject (e.g., \"domains\", \"concepts\")\n",
    "    top_n (int): Number of top persistent items to visualize\n",
    "    \"\"\"\n",
    "    # Calculate persistence score (sum of centrality across decades)\n",
    "    persistence_scores = {}\n",
    "    \n",
    "    for item, item_decades in persistent_items.items():\n",
    "        if item in item_centrality:\n",
    "            # Sum centrality across all decades the item appears in\n",
    "            persistence_scores[item] = sum(item_centrality[item].get(decade, 0) for decade in decades)\n",
    "    \n",
    "    # Get top N items by persistence score\n",
    "    top_persistent_items = sorted(persistence_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_item_names = [item[0] for item in top_persistent_items]\n",
    "    \n",
    "    # Create trend visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot centrality evolution for each top item\n",
    "    for item in top_item_names:\n",
    "        # Get centrality for each decade, using 0 if item not present\n",
    "        centrality_trend = [item_centrality[item].get(decade, 0) for decade in decades]\n",
    "        plt.plot(decades, centrality_trend, 'o-', linewidth=2, label=item)\n",
    "    \n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel('Degree Centrality', fontsize=12)\n",
    "    plt.title(f'Centrality Evolution of Top {top_n} Persistent {subject_type.title()}', fontsize=16)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filename = f\"temporal_{subject_type}_centrality_trends.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    # Create heatmap visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Prepare heatmap data\n",
    "    heatmap_data = []\n",
    "    for item in top_item_names:\n",
    "        row = [item_centrality[item].get(decade, 0) for decade in decades]\n",
    "        heatmap_data.append(row)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt=\".3f\", \n",
    "               xticklabels=[f\"{d}s\" for d in decades],\n",
    "               yticklabels=top_item_names,\n",
    "               cmap=\"YlOrRd\")\n",
    "    \n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel(f'{subject_type.title()}', fontsize=12)\n",
    "    plt.title(f'Centrality Heatmap of Top {top_n} Persistent {subject_type.title()}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filename = f\"temporal_{subject_type}_centrality_heatmap.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77efc16a-7510-4677-a528-149211e7fc1b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_persistent_connections(persistent_connections, item_decade_presence, \n",
    "                                    decades, subject_main_dir, subject_type, top_n=15):\n",
    "    \"\"\"\n",
    "    Visualize persistent connections between items over time\n",
    "    \n",
    "    Parameters:\n",
    "    persistent_connections (dict): Dictionary of persistent connections\n",
    "    item_decade_presence (dict): Dictionary tracking item presence across decades\n",
    "    decades (list): List of decades\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    subject_type (str): Type of subject (e.g., \"domains\", \"concepts\")\n",
    "    top_n (int): Number of top persistent connections to visualize\n",
    "    \"\"\"\n",
    "    if not persistent_connections:\n",
    "        return\n",
    "    \n",
    "    # Sort connections by persistence count\n",
    "    sorted_connections = sorted(persistent_connections.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_connections = sorted_connections[:top_n]\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # For each persistent connection, create a timeline visualization\n",
    "    connection_labels = []\n",
    "    connection_decades = []\n",
    "    \n",
    "    for i, ((item1, item2), count) in enumerate(top_connections):\n",
    "        # Get decades where both items appear\n",
    "        decades_together = set()\n",
    "        for decade in decades:\n",
    "            if decade in item_decade_presence.get(item1, set()) and decade in item_decade_presence.get(item2, set()):\n",
    "                decades_together.add(decade)\n",
    "        \n",
    "        # Add to visualization data\n",
    "        connection_labels.append(f\"{item1} — {item2}\")\n",
    "        connection_decades.append(sorted(decades_together))\n",
    "    \n",
    "    # Create the plot\n",
    "    for i, (label, connection_decade_list) in enumerate(zip(connection_labels, connection_decades)):\n",
    "        plt.plot(connection_decade_list, [i] * len(connection_decade_list), 'o-', linewidth=2, \n",
    "                label=label if i < 10 else \"_nolegend_\")  # Only show first 10 in legend\n",
    "    \n",
    "    plt.yticks(range(len(connection_labels)), connection_labels)\n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.title(f'Temporal Evolution of Top {top_n} Persistent {subject_type.title()} Connections', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Only show legend for first 10 items to avoid overcrowding\n",
    "    if len(connection_labels) > 10:\n",
    "        plt.legend(fontsize=10, loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
    "    else:\n",
    "        plt.legend(fontsize=10, loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filename = f\"temporal_{subject_type}_persistent_connections.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    # Create a persistence graph (showing only persistent connections)\n",
    "    G_persistent = nx.Graph()\n",
    "    \n",
    "    # Add all items involved in persistent connections\n",
    "    all_persistent_items = set()\n",
    "    for (item1, item2), _ in top_connections:\n",
    "        all_persistent_items.add(item1)\n",
    "        all_persistent_items.add(item2)\n",
    "    \n",
    "    for item in all_persistent_items:\n",
    "        G_persistent.add_node(item)\n",
    "    \n",
    "    # Add edges with weight equal to persistence count\n",
    "    for (item1, item2), count in top_connections:\n",
    "        G_persistent.add_edge(item1, item2, weight=count)\n",
    "    \n",
    "    # Visualize the persistent connections graph\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    # Create layout\n",
    "    pos = nx.spring_layout(G_persistent, k=0.5, seed=42)\n",
    "    \n",
    "    # Draw edges with weight-based width\n",
    "    edge_weights = [G_persistent[u][v]['weight'] for u, v in G_persistent.edges()]\n",
    "    max_weight = max(edge_weights) if edge_weights else 1\n",
    "    min_weight = min(edge_weights) if edge_weights else 1\n",
    "    \n",
    "    # Normalize edge widths between 1 and 8\n",
    "    edge_widths = [1 + 7 * (w - min_weight) / (max_weight - min_weight) \n",
    "                  if max_weight > min_weight else 3 for w in edge_weights]\n",
    "    \n",
    "    nx.draw_networkx_edges(G_persistent, pos,\n",
    "                         width=edge_widths,\n",
    "                         alpha=0.7,\n",
    "                         edge_color='#5DA5DA')\n",
    "    \n",
    "    # Draw nodes\n",
    "    node_sizes = [300 + 200 * G_persistent.degree(node) for node in G_persistent.nodes()]\n",
    "    \n",
    "    nx.draw_networkx_nodes(G_persistent, pos,\n",
    "                         node_size=node_sizes,\n",
    "                         node_color='#FAA43A',\n",
    "                         alpha=0.8,\n",
    "                         linewidths=2,\n",
    "                         edgecolors='white')\n",
    "    \n",
    "    # Add labels\n",
    "    font_sizes = {node: min(12 + G_persistent.degree(node), 18) for node in G_persistent.nodes()}\n",
    "    nx.draw_networkx_labels(G_persistent, pos, font_size=font_sizes, font_weight='bold')\n",
    "    \n",
    "    plt.title(f'Persistent {subject_type.title()} Connections Network', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filename = f\"temporal_{subject_type}_persistent_network.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f7cc7-04e0-4eb2-8bbc-ff3bb7f93015",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_community_evolution(temporal_metrics, subject_main_dir, subject_type, top_n=5):\n",
    "    \"\"\"\n",
    "    Visualize the evolution of communities over time\n",
    "    \n",
    "    Parameters:\n",
    "    temporal_metrics (dict): The temporal metrics dictionary\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    subject_type (str): Type of subject (e.g., \"domains\", \"concepts\")\n",
    "    top_n (int): Number of top communities to track per decade\n",
    "    \"\"\"\n",
    "    decades = temporal_metrics['decades']\n",
    "    communities = temporal_metrics['communities']\n",
    "    \n",
    "    # Create a dictionary to track top community members across decades\n",
    "    community_evolution = {}\n",
    "    \n",
    "    for i, decade in enumerate(decades):\n",
    "        if i >= len(communities) or not communities[i]:\n",
    "            continue\n",
    "            \n",
    "        decade_communities = communities[i]\n",
    "        \n",
    "        # Count items per community\n",
    "        community_counts = Counter(decade_communities.values())\n",
    "        \n",
    "        # Get top N communities\n",
    "        top_communities = community_counts.most_common(top_n)\n",
    "        \n",
    "        for comm_id, size in top_communities:\n",
    "            # Get members of this community\n",
    "            members = [node for node, c_id in decade_communities.items() if c_id == comm_id]\n",
    "            \n",
    "            # Create a key for this decade and community\n",
    "            key = f\"{decade}_comm{comm_id}\"\n",
    "            community_evolution[key] = {\n",
    "                'decade': decade,\n",
    "                'comm_id': comm_id,\n",
    "                'size': size,\n",
    "                'members': members\n",
    "            }\n",
    "    \n",
    "    # Create a similarity matrix between communities across decades\n",
    "    similarity_matrix = {}\n",
    "    community_keys = list(community_evolution.keys())\n",
    "    \n",
    "    for i, key1 in enumerate(community_keys):\n",
    "        similarity_matrix[key1] = {}\n",
    "        \n",
    "        for key2 in community_keys:\n",
    "            if key1 == key2:\n",
    "                similarity_matrix[key1][key2] = 1.0\n",
    "                continue\n",
    "                \n",
    "            # Skip if same decade (different communities)\n",
    "            if community_evolution[key1]['decade'] == community_evolution[key2]['decade']:\n",
    "                similarity_matrix[key1][key2] = 0.0\n",
    "                continue\n",
    "            \n",
    "            # Calculate Jaccard similarity\n",
    "            set1 = set(community_evolution[key1]['members'])\n",
    "            set2 = set(community_evolution[key2]['members'])\n",
    "            \n",
    "            intersection = len(set1.intersection(set2))\n",
    "            union = len(set1.union(set2))\n",
    "            \n",
    "            similarity = intersection / union if union > 0 else 0\n",
    "            similarity_matrix[key1][key2] = similarity\n",
    "    \n",
    "    # Create a visualization of community similarities across decades\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    heatmap_labels = [f\"{key.split('_')[0]}s - {key.split('_')[1]}\" for key in community_keys]\n",
    "    heatmap_data = [[similarity_matrix[key1][key2] for key2 in community_keys] for key1 in community_keys]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", \n",
    "               xticklabels=heatmap_labels,\n",
    "               yticklabels=heatmap_labels,\n",
    "               cmap=\"YlGnBu\")\n",
    "    \n",
    "    plt.xlabel('Community', fontsize=12)\n",
    "    plt.ylabel('Community', fontsize=12)\n",
    "    plt.title(f'Similarity Between {subject_type.title()} Communities Across Decades', fontsize=16)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filename = f\"temporal_{subject_type}_community_similarity.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    # Create a visualization of top items in each community\n",
    "    # Group communities by decade\n",
    "    communities_by_decade = {}\n",
    "    for key, data in community_evolution.items():\n",
    "        decade = data['decade']\n",
    "        if decade not in communities_by_decade:\n",
    "            communities_by_decade[decade] = []\n",
    "        communities_by_decade[decade].append((key, data))\n",
    "    \n",
    "    # Create a decade-by-decade community content visualization\n",
    "    for decade, comms in communities_by_decade.items():\n",
    "        if not comms:\n",
    "            continue\n",
    "            \n",
    "        # Sort communities by size\n",
    "        comms = sorted(comms, key=lambda x: x[1]['size'], reverse=True)\n",
    "        \n",
    "        # Create visualization for this decade\n",
    "        plt.figure(figsize=(15, len(comms) * 2.5))\n",
    "        \n",
    "        for i, (key, data) in enumerate(comms):\n",
    "            # Get top items by centrality from the original graph\n",
    "            decade_idx = list(decades).index(decade)\n",
    "            if decade_idx < len(temporal_metrics['graph_objects']):\n",
    "                G = temporal_metrics['graph_objects'][decade_idx]\n",
    "                \n",
    "                # Get centrality measure for this graph\n",
    "                if G.number_of_nodes() > 0:\n",
    "                    centrality = nx.degree_centrality(G)\n",
    "                    \n",
    "                    # Filter for just community members\n",
    "                    comm_centrality = {node: centrality[node] for node in data['members'] if node in centrality}\n",
    "                    \n",
    "                    # Sort by centrality\n",
    "                    top_items = sorted(comm_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "                    \n",
    "                    # Create a horizontal bar chart\n",
    "                    ax = plt.subplot(len(comms), 1, i+1)\n",
    "                    \n",
    "                    if top_items:\n",
    "                        items = [item[0] for item in top_items]\n",
    "                        values = [item[1] for item in top_items]\n",
    "                        \n",
    "                        # Horizontal bar chart\n",
    "                        ax.barh(range(len(items)), values, color=plt.cm.Set3(i % 12))\n",
    "                        ax.set_yticks(range(len(items)))\n",
    "                        ax.set_yticklabels(items)\n",
    "                        ax.set_title(f\"Community {data['comm_id']} - Size: {data['size']}\")\n",
    "                        ax.set_xlabel(\"Centrality\")\n",
    "                        \n",
    "                        # Add values\n",
    "                        for j, value in enumerate(values):\n",
    "                            ax.text(value, j, f\"{value:.3f}\", va='center')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename = f\"temporal_{subject_type}_communities_{decade}s.png\"\n",
    "        save_plot(plt, filename, subject_main_dir, subject_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65741d48-d2f8-4df4-988d-c5a26669b616",
   "metadata": {},
   "source": [
    "# Multi-Column Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f3e51-640d-425f-8e11-19780cf216c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "import community.community_louvain as community_louvain\n",
    "import matplotlib.colors as mcolors\n",
    "import ast\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def create_multi_column_network(df, list_columns, dict_column=None, subject_main_dir=None, \n",
    "                               min_occurrences=2, top_n_per_column=None, overall_top_n=None):\n",
    "    \"\"\"\n",
    "    Create a network visualization combining values from multiple list columns and optionally a dict column.\n",
    "    Values from different columns appear as different node shapes.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame\n",
    "    list_columns (list): List of column names containing list values\n",
    "    dict_column (str, optional): Name of a column containing dictionaries (e.g. concept_dict)\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    min_occurrences (int): Minimum occurrence for a value to be included\n",
    "    top_n_per_column (dict, optional): Dict of {column_name: n} to limit top N values per column\n",
    "    overall_top_n (int, optional): Overall limit on number of nodes (applied after column filters)\n",
    "    \n",
    "    Returns:\n",
    "    networkx.Graph: The network graph object\n",
    "    dict: Column mapping for node attributes\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    all_columns = list_columns.copy()\n",
    "    if dict_column:\n",
    "        all_columns.append(dict_column)\n",
    "    \n",
    "    for col in all_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n",
    "    \n",
    "    # Create subject type from combined column names\n",
    "    subject_type = \"_\".join([col.lower().split('_')[0] for col in all_columns])\n",
    "    \n",
    "    # Process each column to get lists of values\n",
    "    column_values = {}\n",
    "    for col in list_columns:\n",
    "        column_values[col] = preprocess_column(df, col)\n",
    "    \n",
    "    # Process dict column if provided\n",
    "    if dict_column:\n",
    "        dict_values = preprocess_column(df, dict_column)\n",
    "        column_values[dict_column] = dict_values\n",
    "    \n",
    "    # Count occurrences of all values by column\n",
    "    column_counts = {}\n",
    "    for col, values_series in column_values.items():\n",
    "        counts = Counter()\n",
    "        for value_list in values_series:\n",
    "            counts.update(value_list)\n",
    "        \n",
    "        # Filter by minimum occurrences\n",
    "        filtered_counts = {value: count for value, count in counts.items() \n",
    "                          if count >= min_occurrences}\n",
    "        \n",
    "        # Apply top N filter per column if specified\n",
    "        if top_n_per_column and col in top_n_per_column:\n",
    "            n = top_n_per_column[col]\n",
    "            filtered_counts = dict(Counter(filtered_counts).most_common(n))\n",
    "        \n",
    "        column_counts[col] = filtered_counts\n",
    "    \n",
    "    # Create sets of values to include from each column\n",
    "    column_value_sets = {col: set(counts.keys()) for col, counts in column_counts.items()}\n",
    "    \n",
    "    # Track co-occurrences between values (within and across columns)\n",
    "    cooccurrence_dict = {}\n",
    "    \n",
    "    # Process each row in the DataFrame\n",
    "    for idx in df.index:\n",
    "        # Collect all values from this row across specified columns\n",
    "        row_values = []\n",
    "        value_column_map = {}  # Maps value to its column\n",
    "        \n",
    "        for col in all_columns:\n",
    "            values = column_values[col][idx]\n",
    "            \n",
    "            # Only include values that passed the filters\n",
    "            filtered_values = [v for v in values if v in column_value_sets[col]]\n",
    "            \n",
    "            row_values.extend(filtered_values)\n",
    "            \n",
    "            # Track which column each value came from\n",
    "            for v in filtered_values:\n",
    "                value_column_map[v] = col\n",
    "        \n",
    "        # Count co-occurrences\n",
    "        for i1, i2 in combinations(sorted(set(row_values)), 2):\n",
    "            pair = tuple(sorted([i1, i2]))\n",
    "            cooccurrence_dict[pair] = cooccurrence_dict.get(pair, 0) + 1\n",
    "    \n",
    "    # Create combined value counts for overall filtering\n",
    "    all_value_counts = {}\n",
    "    for col, counts in column_counts.items():\n",
    "        all_value_counts.update(counts)\n",
    "    \n",
    "    # Apply overall top N filter if specified\n",
    "    if overall_top_n is not None and overall_top_n < len(all_value_counts):\n",
    "        top_values = set(dict(Counter(all_value_counts).most_common(overall_top_n)).keys())\n",
    "        \n",
    "        # Update column value sets\n",
    "        for col in all_columns:\n",
    "            column_value_sets[col] = {v for v in column_value_sets[col] if v in top_values}\n",
    "        \n",
    "        # Filter cooccurrence dict\n",
    "        cooccurrence_dict = {(i1, i2): count for (i1, i2), count in cooccurrence_dict.items() \n",
    "                            if i1 in top_values and i2 in top_values}\n",
    "    \n",
    "    # Create network graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes with attributes from their columns\n",
    "    node_column_map = {}  # To return for later use\n",
    "    \n",
    "    for col in all_columns:\n",
    "        for value in column_value_sets[col]:\n",
    "            count = column_counts[col][value]\n",
    "            # Size based on frequency, normalized per column\n",
    "            max_count = max(column_counts[col].values()) if column_counts[col] else 1\n",
    "            node_size = 1000 + (count / max_count) * 4000\n",
    "            \n",
    "            # Add node with column information and count\n",
    "            G.add_node(value, column=col, size=node_size, count=count)\n",
    "            node_column_map[value] = col\n",
    "    \n",
    "    # Add edges with weights\n",
    "    for (i1, i2), weight in cooccurrence_dict.items():\n",
    "        # Only add edges between nodes that exist in our filtered graph\n",
    "        if i1 in G.nodes and i2 in G.nodes:\n",
    "            G.add_edge(i1, i2, weight=weight)\n",
    "    \n",
    "    print(f\"Created multi-column network with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "    print(f\"Node distribution by column:\")\n",
    "    for col in all_columns:\n",
    "        col_count = sum(1 for _, attr in G.nodes(data=True) if attr.get('column') == col)\n",
    "        print(f\"  {col}: {col_count} nodes\")\n",
    "    \n",
    "    # Return the graph and the column mapping\n",
    "    return G, all_columns, subject_type, node_column_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de4218-5db3-43b3-9342-c3ccb9fe7101",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_multi_column_graph(G, columns, subject_main_dir, subject_type):\n",
    "    \"\"\"\n",
    "    Visualize a multi-column graph with different shapes for different columns\n",
    "    \n",
    "    Parameters:\n",
    "    G (networkx.Graph): The network graph\n",
    "    columns (list): List of column names\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    subject_type (str): Combined subject type\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (figure, axis) matplotlib objects\n",
    "    \"\"\"\n",
    "    # Set up the figure with GridSpec\n",
    "    fig = plt.figure(figsize=(20, 16), facecolor='white')\n",
    "    gs = gridspec.GridSpec(1, 20)\n",
    "    ax_main = fig.add_subplot(gs[0, :19])\n",
    "    ax_legend = fig.add_subplot(gs[0, 19])\n",
    "    \n",
    "    # Define node shapes and colors for each column\n",
    "    # Using 'o' for circular nodes, '^' for triangular nodes, 's' for square nodes,\n",
    "    # 'd' for diamond nodes, 'p' for pentagonal nodes, etc.\n",
    "    shapes = ['o', '^', 's', 'd', 'p', 'h', '8']\n",
    "    column_shapes = {col: shapes[i % len(shapes)] for i, col in enumerate(columns)}\n",
    "    \n",
    "    # Define distinct colors for each column\n",
    "    color_list = list(mcolors.TABLEAU_COLORS.values())\n",
    "    column_colors = {col: color_list[i % len(color_list)] for i, col in enumerate(columns)}\n",
    "    \n",
    "    # Create layout with more spread\n",
    "    pos = nx.spring_layout(G, k=0.3, iterations=100, seed=42)\n",
    "    \n",
    "    # Draw edges with weight-based colors\n",
    "    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    if edge_weights:\n",
    "        max_weight = max(edge_weights)\n",
    "        min_weight = min(edge_weights)\n",
    "        \n",
    "        # Calculate normalized edge colors and widths\n",
    "        edge_colors = []\n",
    "        edge_widths = []\n",
    "        \n",
    "        for weight in edge_weights:\n",
    "            # Color scale from light to dark blue based on weight\n",
    "            if max_weight > min_weight:\n",
    "                color_val = 0.4 + 0.6 * (np.log1p(weight - min_weight) / np.log1p(max_weight - min_weight))\n",
    "            else:\n",
    "                color_val = 0.5\n",
    "            edge_colors.append(color_val)\n",
    "            \n",
    "            # Width scale from 1 to 5 based on weight\n",
    "            if max_weight > min_weight:\n",
    "                width = 1 + 4 * (weight - min_weight) / (max_weight - min_weight)\n",
    "            else:\n",
    "                width = 3\n",
    "            edge_widths.append(width)\n",
    "        \n",
    "        # Draw all edges\n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "                             edge_color=edge_colors,\n",
    "                             width=edge_widths,\n",
    "                             edge_cmap=plt.cm.Blues,\n",
    "                             alpha=0.6,\n",
    "                             ax=ax_main)\n",
    "    \n",
    "    # Draw nodes by column types\n",
    "    for col in columns:\n",
    "        # Get nodes from this column\n",
    "        col_nodes = [node for node, attr in G.nodes(data=True) if attr.get('column') == col]\n",
    "        \n",
    "        if not col_nodes:\n",
    "            continue\n",
    "        \n",
    "        # Get sizes\n",
    "        node_sizes = [G.nodes[node]['size'] for node in col_nodes]\n",
    "        \n",
    "        # Draw nodes with specific shape and color\n",
    "        shape = column_shapes[col]\n",
    "        color = column_colors[col]\n",
    "        \n",
    "        # For non-circular shapes, adjust size for visual consistency\n",
    "        size_multiplier = 0.5 if shape != 'o' else 1\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos,\n",
    "                             nodelist=col_nodes,\n",
    "                             node_size=[size * size_multiplier for size in node_sizes],\n",
    "                             node_color=color,\n",
    "                             node_shape=shape,\n",
    "                             alpha=0.8,\n",
    "                             linewidths=1,\n",
    "                             edgecolors='white',\n",
    "                             ax=ax_main)\n",
    "    \n",
    "    # Get top nodes by degree and betweenness for labeling\n",
    "    # Calculate degree centrality\n",
    "    degree_cent = nx.degree_centrality(G)\n",
    "    sorted_degree = sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_degree_nodes = [node for node, _ in sorted_degree[:min(30, len(G.nodes()))]]\n",
    "    \n",
    "    # Calculate betweenness centrality for larger graphs\n",
    "    if len(G.nodes()) > 10:\n",
    "        betweenness_cent = nx.betweenness_centrality(G, k=min(50, len(G.nodes())), normalized=True)\n",
    "        sorted_betweenness = sorted(betweenness_cent.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_between_nodes = [node for node, _ in sorted_betweenness[:min(15, len(G.nodes()))]]\n",
    "    else:\n",
    "        top_between_nodes = []\n",
    "    \n",
    "    # Combine important nodes\n",
    "    important_nodes = set(top_degree_nodes) | set(top_between_nodes)\n",
    "    \n",
    "    # Add labels for important nodes only\n",
    "    labels = {node: node if node in important_nodes else '' for node in G.nodes()}\n",
    "    \n",
    "    # Adjust font size based on graph size\n",
    "    if len(G.nodes()) > 100:\n",
    "        font_size = 8\n",
    "    elif len(G.nodes()) > 50:\n",
    "        font_size = 10\n",
    "    else:\n",
    "        font_size = 12\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=font_size, \n",
    "                          font_weight='bold', font_color='black', ax=ax_main)\n",
    "    \n",
    "    # Create legend for column types\n",
    "    ax_legend.axis('off')\n",
    "    legend_elements = []\n",
    "    y_positions = np.linspace(0.9, 0.7, len(columns))\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        shape = column_shapes[col]\n",
    "        color = column_colors[col]\n",
    "        \n",
    "        # Create patch for legend\n",
    "        if shape == 'o':\n",
    "            patch = mpatches.Circle((0.2, 0.9 - (i * 0.1)), radius=0.05, \n",
    "                                   facecolor=color, edgecolor='white', alpha=0.8,\n",
    "                                   transform=ax_legend.transData)\n",
    "        elif shape == '^':\n",
    "            patch = mpatches.RegularPolygon((0.2, 0.9 - (i * 0.1)), 3, radius=0.06, \n",
    "                                           facecolor=color, edgecolor='white', alpha=0.8,\n",
    "                                           transform=ax_legend.transData)\n",
    "        elif shape == 's':\n",
    "            patch = mpatches.Rectangle((0.15, 0.85 - (i * 0.1)), 0.1, 0.1, \n",
    "                                      facecolor=color, edgecolor='white', alpha=0.8,\n",
    "                                      transform=ax_legend.transData)\n",
    "        elif shape == 'd':\n",
    "            patch = mpatches.RegularPolygon((0.2, 0.9 - (i * 0.1)), 4, radius=0.06, \n",
    "                                           orientation=np.pi/4, facecolor=color, \n",
    "                                           edgecolor='white', alpha=0.8,\n",
    "                                           transform=ax_legend.transData)\n",
    "        elif shape == 'p':\n",
    "            patch = mpatches.RegularPolygon((0.2, 0.9 - (i * 0.1)), 5, radius=0.06, \n",
    "                                           facecolor=color, edgecolor='white', alpha=0.8,\n",
    "                                           transform=ax_legend.transData)\n",
    "        elif shape == 'h':\n",
    "            patch = mpatches.RegularPolygon((0.2, 0.9 - (i * 0.1)), 6, radius=0.06, \n",
    "                                           facecolor=color, edgecolor='white', alpha=0.8,\n",
    "                                           transform=ax_legend.transData)\n",
    "        else:\n",
    "            patch = mpatches.Circle((0.2, 0.9 - (i * 0.1)), radius=0.05, \n",
    "                                   facecolor=color, edgecolor='white', alpha=0.8,\n",
    "                                   transform=ax_legend.transData)\n",
    "        \n",
    "        # Format column name for legend\n",
    "        col_name = col.replace('_', ' ').title()\n",
    "        \n",
    "        # Add patch directly with the position already set\n",
    "        ax_legend.add_patch(patch)\n",
    "        \n",
    "        # Add text label\n",
    "        y_pos = 0.9 - (i * 0.1)\n",
    "        ax_legend.text(0.4, y_pos, col_name, va='center', fontsize=12)\n",
    "\n",
    "    # Add title with node and edge counts\n",
    "    num_columns = len(columns)\n",
    "    title = f\"Multi-Column Network: {num_columns} Entity Types\\n\"\n",
    "    title += f\"Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\"\n",
    "    \n",
    "    ax_main.set_title(title, fontsize=16, pad=20)\n",
    "    ax_main.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create filename\n",
    "    columns_string = \"_\".join([col.lower().split('_')[0] for col in columns])\n",
    "    filename = f\"multi_column_network_{columns_string}.png\"\n",
    "    \n",
    "    # Save the visualization\n",
    "    if subject_main_dir:\n",
    "        save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    return fig, ax_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2538f-62d5-4bf5-9d83-83321c4cf858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_multi_column_network(G, columns, subject_main_dir, subject_type, min_clique_size=3):\n",
    "    \"\"\"\n",
    "    Analyze a multi-column network using community detection, \n",
    "    clique analysis, and bridge node identification\n",
    "    \n",
    "    Parameters:\n",
    "    G (networkx.Graph): The multi-column network\n",
    "    columns (list): List of column names\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    subject_type (str): Combined subject type\n",
    "    min_clique_size (int): Minimum size for significant cliques\n",
    "    \n",
    "    Returns:\n",
    "    dict: Analysis results including communities, cliques, and bridge nodes\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        'communities': None,\n",
    "        'cliques': None,\n",
    "        'bridge_nodes': None,\n",
    "        'column_interactions': None\n",
    "    }\n",
    "    \n",
    "    # Skip analysis if graph is too small\n",
    "    if G.number_of_nodes() < 3 or G.number_of_edges() < 2:\n",
    "        print(\"Graph is too small for meaningful analysis\")\n",
    "        return results\n",
    "    \n",
    "    # Detect communities\n",
    "    communities = community_louvain.best_partition(G)\n",
    "    results['communities'] = communities\n",
    "    \n",
    "    # Count community distribution\n",
    "    community_counts = Counter(communities.values())\n",
    "    \n",
    "    # Run community analysis\n",
    "    print(f\"\\n{subject_type.title()} Community Analysis:\")\n",
    "    print(f\"Number of communities: {len(set(communities.values()))}\")\n",
    "    \n",
    "    print(\"\\nLargest communities:\")\n",
    "    for comm_id, count in sorted(community_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        members = [node for node, c_id in communities.items() if c_id == comm_id]\n",
    "        \n",
    "        # Count members by column type\n",
    "        column_counts = {}\n",
    "        for member in members:\n",
    "            col = G.nodes[member]['column']\n",
    "            column_counts[col] = column_counts.get(col, 0) + 1\n",
    "        \n",
    "        # Format column distribution\n",
    "        col_distribution = \", \".join([f\"{col}: {count}\" for col, count in column_counts.items()])\n",
    "        \n",
    "        print(f\"\\nCommunity {comm_id} (Size: {count}):\")\n",
    "        print(f\"Distribution: {col_distribution}\")\n",
    "        print(f\"Sample members: {', '.join(members[:5])}...\")\n",
    "    \n",
    "    # Find cliques\n",
    "    cliques = list(nx.find_cliques(G))\n",
    "    significant_cliques = [c for c in cliques if len(c) >= min_clique_size]\n",
    "    results['cliques'] = significant_cliques\n",
    "    \n",
    "    print(f\"\\nClique Analysis:\")\n",
    "    print(f\"Number of maximal cliques (size ≥ {min_clique_size}): {len(significant_cliques)}\")\n",
    "    \n",
    "    # Print largest cliques\n",
    "    print(\"\\nLargest cliques:\")\n",
    "    for clique in sorted(significant_cliques, key=len, reverse=True)[:5]:\n",
    "        # Count clique members by column type\n",
    "        column_counts = {}\n",
    "        for member in clique:\n",
    "            col = G.nodes[member]['column']\n",
    "            column_counts[col] = column_counts.get(col, 0) + 1\n",
    "        \n",
    "        # Format column distribution\n",
    "        col_distribution = \", \".join([f\"{col}: {count}\" for col, count in column_counts.items()])\n",
    "        \n",
    "        print(f\"\\nSize {len(clique)}:\")\n",
    "        print(f\"Distribution: {col_distribution}\")\n",
    "        print(f\"Members: {', '.join(clique)}\")\n",
    "    \n",
    "    # Find bridge nodes\n",
    "    betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "    bridge_scores = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        node_comm = communities[node]\n",
    "        cross_comm_connections = sum(1 for neighbor in G.neighbors(node) \n",
    "                                  if communities.get(neighbor, -1) != node_comm)\n",
    "        \n",
    "        # Bridge score combines betweenness and cross-community connections\n",
    "        bridge_scores[node] = {\n",
    "            'betweenness': betweenness[node],\n",
    "            'cross_comm_connections': cross_comm_connections,\n",
    "            'bridge_score': betweenness[node] * (1 + cross_comm_connections),\n",
    "            'column': G.nodes[node]['column']\n",
    "        }\n",
    "    \n",
    "    # Sort nodes by bridge score\n",
    "    sorted_bridges = sorted(bridge_scores.items(), key=lambda x: x[1]['bridge_score'], reverse=True)\n",
    "    top_bridges = sorted_bridges[:10]\n",
    "    results['bridge_nodes'] = top_bridges\n",
    "    \n",
    "    print(f\"\\nTop Bridge Nodes Between Communities:\")\n",
    "    for node, scores in top_bridges:\n",
    "        print(f\"{node} ({G.nodes[node]['column']}): Bridge Score={scores['bridge_score']:.4f}, \"\n",
    "              f\"Betweenness={scores['betweenness']:.4f}, \"\n",
    "              f\"Cross-Comm. Connections={scores['cross_comm_connections']}\")\n",
    "    \n",
    "    # Analyze column interactions\n",
    "    column_interactions = analyze_column_interactions(G, columns)\n",
    "    results['column_interactions'] = column_interactions\n",
    "    \n",
    "    # Visualize community structure\n",
    "    visualize_multi_column_communities(G, communities, columns, subject_main_dir, subject_type)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_column_interactions(G, columns):\n",
    "    \"\"\"\n",
    "    Analyze how different column types interact in the network\n",
    "    \n",
    "    Parameters:\n",
    "    G (networkx.Graph): The multi-column network\n",
    "    columns (list): List of column names\n",
    "    \n",
    "    Returns:\n",
    "    dict: Analysis of interactions between column types\n",
    "    \"\"\"\n",
    "    # Initialize interaction counters\n",
    "    interactions = {\n",
    "        'within': {col: 0 for col in columns},\n",
    "        'between': {(col1, col2): 0 for col1 in columns for col2 in columns if col1 < col2}\n",
    "    }\n",
    "    \n",
    "    # Count interactions by edge type\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        col_u = G.nodes[u]['column']\n",
    "        col_v = G.nodes[v]['column']\n",
    "        weight = data.get('weight', 1)\n",
    "        \n",
    "        if col_u == col_v:\n",
    "            # Within-column interaction\n",
    "            interactions['within'][col_u] += weight\n",
    "        else:\n",
    "            # Between-column interaction\n",
    "            pair = tuple(sorted([col_u, col_v]))\n",
    "            interactions['between'][pair] += weight\n",
    "    \n",
    "    # Count node distribution by column\n",
    "    node_counts = {}\n",
    "    for col in columns:\n",
    "        node_counts[col] = sum(1 for _, attr in G.nodes(data=True) if attr.get('column') == col)\n",
    "    \n",
    "    # Calculate interaction density relative to potential interactions\n",
    "    densities = {\n",
    "        'within': {},\n",
    "        'between': {}\n",
    "    }\n",
    "    \n",
    "    for col in columns:\n",
    "        n = node_counts[col]\n",
    "        potential = n * (n - 1) / 2\n",
    "        if potential > 0:\n",
    "            densities['within'][col] = interactions['within'][col] / potential\n",
    "        else:\n",
    "            densities['within'][col] = 0\n",
    "    \n",
    "    for col1, col2 in interactions['between'].keys():\n",
    "        n1 = node_counts[col1]\n",
    "        n2 = node_counts[col2]\n",
    "        potential = n1 * n2\n",
    "        if potential > 0:\n",
    "            densities['between'][(col1, col2)] = interactions['between'][(col1, col2)] / potential\n",
    "        else:\n",
    "            densities['between'][(col1, col2)] = 0\n",
    "    \n",
    "    # Print interaction summary\n",
    "    print(\"\\nColumn Interaction Analysis:\")\n",
    "    \n",
    "    print(\"\\nNode counts by column:\")\n",
    "    for col, count in node_counts.items():\n",
    "        print(f\"  {col}: {count} nodes\")\n",
    "    \n",
    "    print(\"\\nWithin-column interactions:\")\n",
    "    for col, count in interactions['within'].items():\n",
    "        if node_counts[col] > 1:\n",
    "            density = densities['within'][col]\n",
    "            print(f\"  {col}: {count} interactions (density: {density:.4f})\")\n",
    "    \n",
    "    print(\"\\nBetween-column interactions:\")\n",
    "    for (col1, col2), count in interactions['between'].items():\n",
    "        if node_counts[col1] > 0 and node_counts[col2] > 0:\n",
    "            density = densities['between'][(col1, col2)]\n",
    "            print(f\"  {col1} <-> {col2}: {count} interactions (density: {density:.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'interactions': interactions,\n",
    "        'densities': densities,\n",
    "        'node_counts': node_counts\n",
    "    }\n",
    "\n",
    "def visualize_multi_column_communities(G, communities, columns, subject_main_dir, subject_type):\n",
    "    \"\"\"\n",
    "    Visualize communities in a multi-column network with improved legend\n",
    "    \n",
    "    Parameters:\n",
    "    G (networkx.Graph): The multi-column network\n",
    "    communities (dict): Community assignments\n",
    "    columns (list): List of column names\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    subject_type (str): Combined subject type\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as mpatches\n",
    "    import matplotlib.colors as mcolors\n",
    "    import matplotlib.gridspec as gridspec\n",
    "    import networkx as nx\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Set up the figure with GridSpec - allocate more space for the legend\n",
    "    fig = plt.figure(figsize=(22, 16), facecolor='white')\n",
    "    gs = gridspec.GridSpec(1, 24)\n",
    "    ax_main = fig.add_subplot(gs[0, :19])  # Main graph takes 19/24 of width\n",
    "    ax_legend = fig.add_subplot(gs[0, 19:])  # Legend takes 5/24 of width\n",
    "    \n",
    "    # Define node shapes and colors for each column\n",
    "    shapes = ['o', '^', 's', 'd', 'p', 'h', '8']\n",
    "    column_shapes = {col: shapes[i % len(shapes)] for i, col in enumerate(columns)}\n",
    "    \n",
    "    # Define distinct colors for each column (for legend)\n",
    "    type_colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "    column_colors = {col: type_colors[i % len(type_colors)] for i, col in enumerate(columns)}\n",
    "    \n",
    "    # Generate colors for communities\n",
    "    n_communities = len(set(communities.values()))\n",
    "    community_colors = plt.cm.tab20(np.linspace(0, 1, n_communities))\n",
    "    \n",
    "    # Create layout\n",
    "    pos = nx.spring_layout(G, k=0.3, iterations=100, seed=42)\n",
    "    \n",
    "    # Draw edges with reduced opacity\n",
    "    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    if edge_weights:\n",
    "        max_weight = max(edge_weights)\n",
    "        min_weight = min(edge_weights)\n",
    "        \n",
    "        edge_colors = []\n",
    "        edge_widths = []\n",
    "        \n",
    "        for weight in edge_weights:\n",
    "            if max_weight > min_weight:\n",
    "                # Use weight for width but fixed color (more transparent)\n",
    "                width = 1 + 4 * (weight - min_weight) / (max_weight - min_weight)\n",
    "            else:\n",
    "                width = 1.5\n",
    "            edge_widths.append(width)\n",
    "        \n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "                             width=edge_widths,\n",
    "                             edge_color='gray',\n",
    "                             alpha=0.4,\n",
    "                             ax=ax_main)\n",
    "    \n",
    "    # Draw nodes by column types with community colors\n",
    "    for col in columns:\n",
    "        for comm_id in set(communities.values()):\n",
    "            # Get nodes from this column and community\n",
    "            nodes = [node for node, attr in G.nodes(data=True) \n",
    "                   if attr.get('column') == col and communities[node] == comm_id]\n",
    "            \n",
    "            if not nodes:\n",
    "                continue\n",
    "            \n",
    "            # Get node sizes\n",
    "            node_sizes = [G.nodes[node]['size'] for node in nodes]\n",
    "            \n",
    "            # Draw nodes with specific shape and community color\n",
    "            shape = column_shapes[col]\n",
    "            color = community_colors[comm_id]\n",
    "            \n",
    "            # For non-circular shapes, adjust size for visual consistency\n",
    "            size_multiplier = 0.5 if shape != 'o' else 1\n",
    "            \n",
    "            nx.draw_networkx_nodes(G, pos,\n",
    "                                 nodelist=nodes,\n",
    "                                 node_size=[size * size_multiplier for size in node_sizes],\n",
    "                                 node_color=[color] * len(nodes),\n",
    "                                 node_shape=shape,\n",
    "                                 alpha=0.8,\n",
    "                                 linewidths=1,\n",
    "                                 edgecolors='white',\n",
    "                                 ax=ax_main)\n",
    "    \n",
    "    # Add labels for important nodes\n",
    "    # Calculate degree centrality\n",
    "    degree_cent = nx.degree_centrality(G)\n",
    "    sorted_degree = sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_degree_nodes = [node for node, _ in sorted_degree[:min(30, len(G.nodes()))]]\n",
    "    \n",
    "    # Add labels for important nodes only\n",
    "    labels = {node: node if node in top_degree_nodes else '' for node in G.nodes()}\n",
    "    \n",
    "    # Adjust font size based on graph size\n",
    "    if len(G.nodes()) > 100:\n",
    "        font_size = 8\n",
    "    elif len(G.nodes()) > 50:\n",
    "        font_size = 10\n",
    "    else:\n",
    "        font_size = 12\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=font_size, \n",
    "                          font_weight='bold', font_color='black', ax=ax_main)\n",
    "    \n",
    "    # ----------- IMPROVED LEGEND SECTION -----------\n",
    "    \n",
    "    # Clean up legend axis\n",
    "    ax_legend.axis('off')\n",
    "    ax_legend.set_xlim(0, 1)\n",
    "    ax_legend.set_ylim(0, 1)\n",
    "    \n",
    "    # Add title for column section\n",
    "    ax_legend.text(0.5, 0.98, \"Column Types\", ha='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add column type legend with better spacing\n",
    "    spacing = min(0.08, 0.7 / len(columns))  # Adaptive spacing based on number of columns\n",
    "    \n",
    "    for i, col in enumerate(columns):\n",
    "        shape = column_shapes[col]\n",
    "        color = column_colors[col]\n",
    "        \n",
    "        # Calculate y position with adaptive spacing\n",
    "        y_pos = 0.93 - (i * spacing * 1.5)\n",
    "        \n",
    "        # Create patch for legend based on shape\n",
    "        if shape == 'o':\n",
    "            patch = mpatches.Circle((0.15, y_pos), radius=0.03, \n",
    "                                  facecolor=color, edgecolor='black', alpha=0.8)\n",
    "        elif shape == '^':\n",
    "            patch = mpatches.RegularPolygon((0.15, y_pos), 3, radius=0.035, \n",
    "                                          facecolor=color, edgecolor='black', alpha=0.8)\n",
    "        elif shape == 's':\n",
    "            patch = mpatches.Rectangle((0.12, y_pos-0.03), 0.06, 0.06, \n",
    "                                     facecolor=color, edgecolor='black', alpha=0.8)\n",
    "        elif shape == 'd':\n",
    "            patch = mpatches.RegularPolygon((0.15, y_pos), 4, radius=0.035, \n",
    "                                          orientation=np.pi/4, facecolor=color, \n",
    "                                          edgecolor='black', alpha=0.8)\n",
    "        elif shape == 'p':\n",
    "            patch = mpatches.RegularPolygon((0.15, y_pos), 5, radius=0.035, \n",
    "                                          facecolor=color, edgecolor='black', alpha=0.8)\n",
    "        elif shape == 'h':\n",
    "            patch = mpatches.RegularPolygon((0.15, y_pos), 6, radius=0.035, \n",
    "                                          facecolor=color, edgecolor='black', alpha=0.8)\n",
    "        else:\n",
    "            patch = mpatches.Circle((0.15, y_pos), radius=0.03, \n",
    "                                  facecolor=color, edgecolor='black', alpha=0.8)\n",
    "        \n",
    "        # Add patch to legend\n",
    "        ax_legend.add_patch(patch)\n",
    "        \n",
    "        # Format column name for legend (clean up and ensure reasonable length)\n",
    "        col_name = col.replace('_', ' ').title()\n",
    "        if len(col_name) > 20:  # Truncate long column names\n",
    "            col_name = col_name[:18] + '...'\n",
    "            \n",
    "        # Add text label with more horizontal spacing\n",
    "        ax_legend.text(0.3, y_pos, col_name, va='center', fontsize=12)\n",
    "    \n",
    "    # Add divider line between column types and communities\n",
    "    community_section_start = 0.93 - (len(columns) * spacing * 1.5) - 0.1\n",
    "    ax_legend.axhline(y=community_section_start + 0.05, xmin=0.05, xmax=0.95, color='gray', linestyle='-', linewidth=1)\n",
    "    \n",
    "    # Add title for community section\n",
    "    ax_legend.text(0.5, community_section_start, \"Communities\", ha='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Count communities\n",
    "    community_counts = Counter(communities.values())\n",
    "    top_communities = sorted(community_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Determine how many communities to show based on available space and total communities\n",
    "    max_communities_to_show = min(10, len(top_communities), int((community_section_start - 0.05) / (spacing * 1.5)))\n",
    "    top_communities = top_communities[:max_communities_to_show]\n",
    "    \n",
    "    # Add community entries with better spacing\n",
    "    for i, (comm_id, count) in enumerate(top_communities):\n",
    "        y_pos = community_section_start - 0.08 - (i * spacing * 1.5)\n",
    "        color = community_colors[comm_id]\n",
    "        \n",
    "        # Add color patch\n",
    "        patch = mpatches.Circle((0.15, y_pos), radius=0.03, \n",
    "                              facecolor=color, edgecolor='black', alpha=0.8)\n",
    "        ax_legend.add_patch(patch)\n",
    "        \n",
    "        # Add text label with count\n",
    "        ax_legend.text(0.3, y_pos, f\"Community {comm_id} ({count})\", va='center', fontsize=11)\n",
    "    \n",
    "    # If there are more communities than shown, add an indication\n",
    "    if len(community_counts) > max_communities_to_show:\n",
    "        remaining = len(community_counts) - max_communities_to_show\n",
    "        y_pos = community_section_start - 0.08 - (max_communities_to_show * spacing * 1.5) - (spacing * 0.5)\n",
    "        ax_legend.text(0.5, y_pos, f\"+ {remaining} more communities\", \n",
    "                     ha='center', va='center', fontsize=10, fontstyle='italic', color='gray')\n",
    "    \n",
    "    # Add title with community information\n",
    "    title = f\"{subject_type.title()} Network\\nColored by Communities ({n_communities} communities)\"\n",
    "    ax_main.set_title(title, fontsize=16)\n",
    "    ax_main.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create filename\n",
    "    filename = f\"multi_column_communities_{subject_type}.png\"\n",
    "    \n",
    "    # Save the plot\n",
    "    if subject_main_dir:\n",
    "        save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    return fig, ax_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92635e3a-c4be-462f-a744-54d58412d92f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_column_interactions(column_interactions, columns, subject_main_dir, subject_type):\n",
    "    \"\"\"\n",
    "    Visualize interactions between different column types\n",
    "    \n",
    "    Parameters:\n",
    "    column_interactions (dict): The column interaction analysis results\n",
    "    columns (list): List of column names\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    subject_type (str): Combined subject type\n",
    "    \"\"\"\n",
    "    interactions = column_interactions['interactions']\n",
    "    densities = column_interactions['densities']\n",
    "    node_counts = column_interactions['node_counts']\n",
    "    \n",
    "    # 1. Create a heatmap of interaction densities\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    # Create a matrix with all column pairs\n",
    "    all_columns = columns.copy()\n",
    "    n_cols = len(all_columns)\n",
    "    density_matrix = np.zeros((n_cols, n_cols))\n",
    "    \n",
    "    # Fill diagonal with within-column densities\n",
    "    for i, col in enumerate(all_columns):\n",
    "        density_matrix[i, i] = densities['within'].get(col, 0)\n",
    "    \n",
    "    # Fill off-diagonal with between-column densities\n",
    "    for i, col1 in enumerate(all_columns):\n",
    "        for j, col2 in enumerate(all_columns):\n",
    "            if i < j:  # Upper triangle\n",
    "                pair = (col1, col2)\n",
    "                density_matrix[i, j] = densities['between'].get(pair, 0)\n",
    "                density_matrix[j, i] = density_matrix[i, j]  # Mirror\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(density_matrix, annot=True, fmt='.4f',\n",
    "               xticklabels=[col.replace('_', ' ').title() for col in all_columns],\n",
    "               yticklabels=[col.replace('_', ' ').title() for col in all_columns],\n",
    "               cmap='YlGnBu')\n",
    "    \n",
    "    plt.title(f'Interaction Density Between Entity Types', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"column_interaction_density_{subject_type}.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    # 2. Create a network visualization of column interactions\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create a graph of column interactions\n",
    "    G_cols = nx.Graph()\n",
    "    \n",
    "    # Add nodes (columns)\n",
    "    for col in all_columns:\n",
    "        # Size proportional to number of entities\n",
    "        size = node_counts.get(col, 0)\n",
    "        G_cols.add_node(col, size=size)\n",
    "    \n",
    "    # Add edges (interactions between columns)\n",
    "    for (col1, col2), weight in interactions['between'].items():\n",
    "        if weight > 0:\n",
    "            G_cols.add_edge(col1, col2, weight=weight)\n",
    "    \n",
    "    # Add self-loops (interactions within columns)\n",
    "    for col, weight in interactions['within'].items():\n",
    "        if weight > 0:\n",
    "            G_cols.add_edge(col, col, weight=weight)\n",
    "    \n",
    "    # Calculate node positions\n",
    "    if len(all_columns) <= 3:\n",
    "        # For few columns, use circular layout\n",
    "        pos = nx.circular_layout(G_cols)\n",
    "    else:\n",
    "        # For more columns, use spring layout\n",
    "        pos = nx.spring_layout(G_cols, k=0.9, iterations=50, seed=42)\n",
    "    \n",
    "    # Get node sizes based on entity count\n",
    "    node_sizes = [1000 + (node_counts.get(col, 0) * 20) for col in G_cols.nodes()]\n",
    "    \n",
    "    # Get edge widths based on interaction weight\n",
    "    edge_weights = [G_cols[u][v]['weight'] for u, v in G_cols.edges()]\n",
    "    max_weight = max(edge_weights) if edge_weights else 1\n",
    "    \n",
    "    # Scale edge widths between 1 and 10\n",
    "    edge_widths = [1 + 9 * (weight / max_weight) for weight in edge_weights]\n",
    "    \n",
    "    # Draw the graph\n",
    "    nx.draw_networkx_nodes(G_cols, pos,\n",
    "                         node_size=node_sizes,\n",
    "                         node_color='lightblue',\n",
    "                         alpha=0.8,\n",
    "                         edgecolors='white',\n",
    "                         linewidths=2)\n",
    "    \n",
    "    nx.draw_networkx_edges(G_cols, pos,\n",
    "                         width=edge_widths,\n",
    "                         alpha=0.7,\n",
    "                         edge_color='gray',\n",
    "                         style='solid')\n",
    "    \n",
    "    # Add edge labels (interaction counts)\n",
    "    edge_labels = {}\n",
    "    for (u, v), width in zip(G_cols.edges(), edge_weights):\n",
    "        # Format large numbers with k suffix\n",
    "        weight = G_cols[u][v]['weight']\n",
    "        if weight >= 1000:\n",
    "            label = f\"{weight/1000:.1f}k\"\n",
    "        else:\n",
    "            label = f\"{weight}\"\n",
    "        edge_labels[(u, v)] = label\n",
    "    \n",
    "    nx.draw_networkx_edge_labels(G_cols, pos,\n",
    "                               edge_labels=edge_labels,\n",
    "                               font_size=10)\n",
    "    \n",
    "    # Add node labels (column names)\n",
    "    labels = {col: col.replace('_', ' ').title() for col in G_cols.nodes()}\n",
    "    nx.draw_networkx_labels(G_cols, pos,\n",
    "                          labels=labels,\n",
    "                          font_size=12,\n",
    "                          font_weight='bold')\n",
    "    \n",
    "    plt.title(f'Entity Type Interaction Network', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"column_interaction_network_{subject_type}.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    # 3. Create a stacked bar chart of community composition\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Get community distribution by column\n",
    "    community_composition = {}\n",
    "    \n",
    "    # This requires community data, which we don't have in this function\n",
    "    # Let's create a placeholder - the actual function would need to be called\n",
    "    # with the community data from analyze_multi_column_network\n",
    "    \n",
    "    return True\n",
    "\n",
    "def analyze_cross_column_communities(G, communities, columns):\n",
    "    \"\"\"\n",
    "    Analyze how communities span across different column types\n",
    "    \n",
    "    Parameters:\n",
    "    G (networkx.Graph): The multi-column network\n",
    "    communities (dict): Community assignments\n",
    "    columns (list): List of column names\n",
    "    \n",
    "    Returns:\n",
    "    dict: Analysis of cross-column community structure\n",
    "    \"\"\"\n",
    "    # Count community distribution by column\n",
    "    community_composition = {}\n",
    "    \n",
    "    # Get unique community IDs\n",
    "    community_ids = sorted(set(communities.values()))\n",
    "    \n",
    "    for comm_id in community_ids:\n",
    "        # Get members of this community\n",
    "        members = [node for node, c_id in communities.items() if c_id == comm_id]\n",
    "        \n",
    "        # Count by column\n",
    "        col_counts = {}\n",
    "        for member in members:\n",
    "            col = G.nodes[member]['column']\n",
    "            col_counts[col] = col_counts.get(col, 0) + 1\n",
    "        \n",
    "        # Store counts\n",
    "        community_composition[comm_id] = col_counts\n",
    "    \n",
    "    # Count total nodes by column\n",
    "    total_by_column = {}\n",
    "    for col in columns:\n",
    "        total_by_column[col] = sum(1 for _, attr in G.nodes(data=True) if attr.get('column') == col)\n",
    "    \n",
    "    # Calculate what percentage of each column's nodes are in each community\n",
    "    percentage_by_column = {}\n",
    "    \n",
    "    for comm_id, col_counts in community_composition.items():\n",
    "        percentage_by_column[comm_id] = {}\n",
    "        \n",
    "        for col in columns:\n",
    "            # Calculate percentage (avoid division by zero)\n",
    "            if total_by_column.get(col, 0) > 0:\n",
    "                percentage = (col_counts.get(col, 0) / total_by_column[col]) * 100\n",
    "            else:\n",
    "                percentage = 0\n",
    "            \n",
    "            percentage_by_column[comm_id][col] = percentage\n",
    "    \n",
    "    # Find communities with good cross-column representation\n",
    "    cross_column_communities = []\n",
    "    \n",
    "    for comm_id, col_percentages in percentage_by_column.items():\n",
    "        # Count columns with significant representation (>10%)\n",
    "        significant_columns = sum(1 for col, pct in col_percentages.items() if pct >= 10)\n",
    "        \n",
    "        if significant_columns >= 2:  # At least two columns have >10% of their nodes in this community\n",
    "            cross_column_communities.append({\n",
    "                'community_id': comm_id,\n",
    "                'significant_columns': significant_columns,\n",
    "                'column_percentages': col_percentages\n",
    "            })\n",
    "    \n",
    "    # Sort by number of significant columns\n",
    "    cross_column_communities.sort(key=lambda x: x['significant_columns'], reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'community_composition': community_composition,\n",
    "        'percentage_by_column': percentage_by_column,\n",
    "        'cross_column_communities': cross_column_communities,\n",
    "        'total_by_column': total_by_column\n",
    "    }\n",
    "\n",
    "def visualize_community_composition(community_analysis, columns, subject_main_dir, subject_type):\n",
    "    \"\"\"\n",
    "    Visualize how communities are composed of different column types\n",
    "    \n",
    "    Parameters:\n",
    "    community_analysis (dict): The community composition analysis results\n",
    "    columns (list): List of column names\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    subject_type (str): Combined subject type\n",
    "    \"\"\"\n",
    "    community_composition = community_analysis['community_composition']\n",
    "    percentage_by_column = community_analysis['percentage_by_column']\n",
    "    cross_column_communities = community_analysis['cross_column_communities']\n",
    "    \n",
    "    # Get top communities by size\n",
    "    community_sizes = {comm_id: sum(col_counts.values()) \n",
    "                      for comm_id, col_counts in community_composition.items()}\n",
    "    \n",
    "    top_communities = sorted(community_sizes.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    top_comm_ids = [comm_id for comm_id, _ in top_communities]\n",
    "    \n",
    "    # 1. Create a stacked bar chart of community composition\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Prepare data for stacked bar chart\n",
    "    comm_labels = [f\"Comm {comm_id}\" for comm_id in top_comm_ids]\n",
    "    \n",
    "    # Create bottom value for stacked bars\n",
    "    bottoms = np.zeros(len(top_comm_ids))\n",
    "    \n",
    "    # Define colors for columns\n",
    "    color_list = list(mcolors.TABLEAU_COLORS.values())\n",
    "    column_colors = {col: color_list[i % len(color_list)] for i, col in enumerate(columns)}\n",
    "    \n",
    "    # Plot each column's contribution as a section of the stacked bar\n",
    "    for col in columns:\n",
    "        # Get values for this column across top communities\n",
    "        values = [community_composition[comm_id].get(col, 0) for comm_id in top_comm_ids]\n",
    "        \n",
    "        # Skip if no values\n",
    "        if sum(values) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Plot this column's contribution\n",
    "        plt.bar(comm_labels, values, bottom=bottoms, label=col.replace('_', ' ').title(),\n",
    "               color=column_colors[col])\n",
    "        \n",
    "        # Update bottoms for next layer\n",
    "        bottoms += values\n",
    "    \n",
    "    plt.title('Entity Type Composition of Top Communities', fontsize=14)\n",
    "    plt.xlabel('Community')\n",
    "    plt.ylabel('Number of Entities')\n",
    "    plt.legend(title='Entity Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"community_composition_{subject_type}.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    # 2. Create a heatmap of column percentages in communities\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    heatmap_data = []\n",
    "    for comm_id in top_comm_ids:\n",
    "        row = [percentage_by_column[comm_id].get(col, 0) for col in columns]\n",
    "        heatmap_data.append(row)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.1f',\n",
    "               xticklabels=[col.replace('_', ' ').title() for col in columns],\n",
    "               yticklabels=comm_labels,\n",
    "               cmap='YlGnBu')\n",
    "    \n",
    "    plt.title('Percentage of Each Entity Type in Communities', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = f\"community_percentages_{subject_type}.png\"\n",
    "    save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    # 3. Highlight cross-column communities\n",
    "    if cross_column_communities:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Limit to top 8 cross-column communities\n",
    "        top_cross = cross_column_communities[:min(8, len(cross_column_communities))]\n",
    "        \n",
    "        # Prepare data\n",
    "        cross_labels = [f\"Comm {c['community_id']}\" for c in top_cross]\n",
    "        \n",
    "        # Plot heatmap of percentages\n",
    "        cross_data = []\n",
    "        for c in top_cross:\n",
    "            row = [c['column_percentages'].get(col, 0) for col in columns]\n",
    "            cross_data.append(row)\n",
    "        \n",
    "        sns.heatmap(cross_data, annot=True, fmt='.1f',\n",
    "                   xticklabels=[col.replace('_', ' ').title() for col in columns],\n",
    "                   yticklabels=cross_labels,\n",
    "                   cmap='YlGnBu')\n",
    "        \n",
    "        plt.title('Communities with Strong Cross-Entity Type Representation', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        filename = f\"cross_column_communities_{subject_type}.png\"\n",
    "        save_plot(plt, filename, subject_main_dir, subject_type)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d31c35-466e-4682-b463-f328c1ae47fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_countries_column(df):\n",
    "    \"\"\"\n",
    "    Convert the nested list structure in the 'Countries' column to a flat list.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the 'Countries' column\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with flattened 'Countries' column\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Get the Countries column as processed lists\n",
    "    countries_lists = preprocess_column(df_copy, 'Countries')\n",
    "    \n",
    "    # Flatten any nested lists\n",
    "    flattened_countries = []\n",
    "    \n",
    "    for countries in countries_lists:\n",
    "        # If already a flat list, keep as is\n",
    "        if all(isinstance(item, str) for item in countries):\n",
    "            flattened_countries.append(countries)\n",
    "        else:\n",
    "            # Handle nested lists by flattening\n",
    "            flat_list = []\n",
    "            for item in countries:\n",
    "                if isinstance(item, list):\n",
    "                    flat_list.extend(item)\n",
    "                else:\n",
    "                    flat_list.append(item)\n",
    "            flattened_countries.append(flat_list)\n",
    "    \n",
    "    # Update the DataFrame with the flattened lists\n",
    "    df_copy['Countries_flat'] = flattened_countries\n",
    "    \n",
    "    # Print some statistics\n",
    "    total_countries = sum(len(countries) for countries in flattened_countries)\n",
    "    unique_countries = len(set(country for countries in flattened_countries for country in countries))\n",
    "    \n",
    "    print(f\"Flattened 'Countries' column: {total_countries} total country mentions\")\n",
    "    print(f\"Found {unique_countries} unique countries\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Example usage\n",
    "df_flattened = flatten_countries_column(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1666c-2d9a-42fa-ba2b-fa5547bdada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_at, columns_at, subject_type_at, node_column_map_at = create_multi_column_network(\n",
    "    df_flattened,\n",
    "    list_columns=['Institutions', 'Countries_flat'],  # List columns\n",
    "    dict_column=None,                    # No dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=3,                   # Only include items with at least 3 occurrences\n",
    "    top_n_per_column={                   # Limit items per column\n",
    "        'Institutions': 10, \n",
    "        'Countries_flat': 10\n",
    "    },\n",
    "    overall_top_n=20\n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "fig, ax_main=visualize_multi_column_graph(G_at, columns_at, subject_main_dir, subject_type_at)\n",
    "# legend_ax = fix_legend_display(fig, ax_main)\n",
    "# Analyze the multi-column network\n",
    "analysis_at = analyze_multi_column_network(\n",
    "    G_at, \n",
    "    columns_at, \n",
    "    subject_main_dir, \n",
    "    subject_type_at,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Analyze cross-column communities\n",
    "community_analysis_at = analyze_cross_column_communities(\n",
    "    G_at,\n",
    "    analysis_at['communities'],\n",
    "    columns_at\n",
    ")\n",
    "\n",
    "# Visualize community composition\n",
    "visualize_community_composition(\n",
    "    community_analysis_at,\n",
    "    columns_at,\n",
    "    subject_main_dir,\n",
    "    subject_type_at\n",
    ")\n",
    "\n",
    "G_at, columns_at, subject_type_at, node_column_map_at = create_multi_column_network(\n",
    "    df_flattened,\n",
    "    list_columns=['Authors', 'Institutions', 'Countries_flat'],  # List columns\n",
    "    dict_column=None,                    # No dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=3,                   # Only include items with at least 3 occurrences\n",
    "    top_n_per_column={                   # Limit items per column\n",
    "        'Authors': 10,\n",
    "        'Institutions': 10, \n",
    "        'Countries_flat': 5\n",
    "    },\n",
    "    overall_top_n=25\n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "fig, ax_main=visualize_multi_column_graph(G_at, columns_at, subject_main_dir, subject_type_at)\n",
    "# legend_ax = fix_legend_display(fig, ax_main)\n",
    "# Analyze the multi-column network\n",
    "analysis_at = analyze_multi_column_network(\n",
    "    G_at, \n",
    "    columns_at, \n",
    "    subject_main_dir, \n",
    "    subject_type_at,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Analyze cross-column communities\n",
    "community_analysis_at = analyze_cross_column_communities(\n",
    "    G_at,\n",
    "    analysis_at['communities'],\n",
    "    columns_at\n",
    ")\n",
    "\n",
    "# Visualize community composition\n",
    "visualize_community_composition(\n",
    "    community_analysis_at,\n",
    "    columns_at,\n",
    "    subject_main_dir,\n",
    "    subject_type_at\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f65833-e873-4176-a54a-8c933bb1c91f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G_at, columns_at, subject_type_at, node_column_map_at = create_multi_column_network(\n",
    "    df,\n",
    "    list_columns=['Authors', 'Topics'],  # List columns\n",
    "    dict_column=None,                    # No dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=3,                   # Only include items with at least 3 occurrences\n",
    "    top_n_per_column={                   # Limit items per column\n",
    "        'Authors': 10,\n",
    "        'Topics': 10\n",
    "    },\n",
    "    overall_top_n=20 \n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "fig, ax_main=visualize_multi_column_graph(G_at, columns_at, subject_main_dir, subject_type_at)\n",
    "# legend_ax = fix_legend_display(fig, ax_main)\n",
    "# Analyze the multi-column network\n",
    "analysis_at = analyze_multi_column_network(\n",
    "    G_at, \n",
    "    columns_at, \n",
    "    subject_main_dir, \n",
    "    subject_type_at,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Analyze cross-column communities\n",
    "community_analysis_at = analyze_cross_column_communities(\n",
    "    G_at,\n",
    "    analysis_at['communities'],\n",
    "    columns_at\n",
    ")\n",
    "\n",
    "# Visualize community composition\n",
    "visualize_community_composition(\n",
    "    community_analysis_at,\n",
    "    columns_at,\n",
    "    subject_main_dir,\n",
    "    subject_type_at\n",
    ")\n",
    "\n",
    "G_at, columns_at, subject_type_at, node_column_map_at = create_multi_column_network(\n",
    "    df,\n",
    "    list_columns=['Authors', 'Fields'],  # List columns\n",
    "    dict_column=None,                    # No dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=3,                   # Only include items with at least 3 occurrences\n",
    "    top_n_per_column={                   # Limit items per column\n",
    "        'Authors': 10, \n",
    "        'Fields': 10\n",
    "    },\n",
    "    overall_top_n=20 \n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "fig, ax_main=visualize_multi_column_graph(G_at, columns_at, subject_main_dir, subject_type_at)\n",
    "# legend_ax = fix_legend_display(fig, ax_main)\n",
    "# Analyze the multi-column network\n",
    "analysis_at = analyze_multi_column_network(\n",
    "    G_at, \n",
    "    columns_at, \n",
    "    subject_main_dir, \n",
    "    subject_type_at,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Analyze cross-column communities\n",
    "community_analysis_at = analyze_cross_column_communities(\n",
    "    G_at,\n",
    "    analysis_at['communities'],\n",
    "    columns_at\n",
    ")\n",
    "\n",
    "# Visualize community composition\n",
    "visualize_community_composition(\n",
    "    community_analysis_at,\n",
    "    columns_at,\n",
    "    subject_main_dir,\n",
    "    subject_type_at\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8dc57-7e14-4f44-a8cc-f61369072d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. EXAMPLE: Analyzing Institutions with Fields and Sub-fields\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-COLUMN NETWORK ANALYSIS: INSTITUTIONS + FIELDS + SUB-FIELDS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create multi-column network\n",
    "G_ifs, columns_ifs, subject_type_ifs, node_column_map_ifs = create_multi_column_network(\n",
    "    df,\n",
    "    list_columns=['Institutions', 'Fields', 'Sub-fields'],  # List columns\n",
    "    dict_column=None,                                      # No dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=3,                                     # Only include items with at least 3 occurrences\n",
    "    top_n_per_column={                                     # Limit items per column\n",
    "        'Institutions': 10,\n",
    "        'Fields': 5,\n",
    "        'Sub-fields': 5\n",
    "    }\n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "visualize_multi_column_graph(G_ifs, columns_ifs, subject_main_dir, subject_type_ifs)\n",
    "\n",
    "# Analyze the multi-column network\n",
    "analysis_ifs = analyze_multi_column_network(\n",
    "    G_ifs, \n",
    "    columns_ifs, \n",
    "    subject_main_dir, \n",
    "    subject_type_ifs,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Analyze cross-column communities\n",
    "community_analysis_ifs = analyze_cross_column_communities(\n",
    "    G_ifs,\n",
    "    analysis_ifs['communities'],\n",
    "    columns_ifs\n",
    ")\n",
    "\n",
    "# Visualize community composition\n",
    "visualize_community_composition(\n",
    "    community_analysis_ifs,\n",
    "    columns_ifs,\n",
    "    subject_main_dir,\n",
    "    subject_type_ifs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763a241-743d-42d4-8c94-34041af63165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the multi-column network analysis functions\n",
    "\n",
    "# 1. EXAMPLE: Analyzing Concepts, Fields, and Domains together\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-COLUMN NETWORK ANALYSIS: CONCEPTS + FIELDS + DOMAINS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create multi-column network\n",
    "G_cfd, columns_cfd, subject_type_cfd, node_column_map_cfd = create_multi_column_network(\n",
    "    df,\n",
    "    list_columns=['Fields', 'Domains', 'Topics'],  # List columns\n",
    "    dict_column=None,          # Dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=5,                   # Only include items with at least 5 occurrences\n",
    "    top_n_per_column={                   # Limit items per column\n",
    "        'Fields': 5,\n",
    "        'Domains': 5,\n",
    "        'Topics': 5\n",
    "    },\n",
    "    overall_top_n=20                  # Overall limit on total nodes\n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "fig, ax_main=visualize_multi_column_graph(G_cfd, columns_cfd, subject_main_dir, subject_type_cfd)\n",
    "# legend_ax = fix_legend_display(fig, ax_main)\n",
    "\n",
    "# Analyze the multi-column network\n",
    "analysis_cfd = analyze_multi_column_network(\n",
    "    G_cfd, \n",
    "    columns_cfd, \n",
    "    subject_main_dir, \n",
    "    subject_type_cfd,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Analyze cross-column communities\n",
    "community_analysis_cfd = analyze_cross_column_communities(\n",
    "    G_cfd,\n",
    "    analysis_cfd['communities'],\n",
    "    columns_cfd\n",
    ")\n",
    "\n",
    "# Visualize community composition\n",
    "visualize_community_composition(\n",
    "    community_analysis_cfd,\n",
    "    columns_cfd,\n",
    "    subject_main_dir,\n",
    "    subject_type_cfd\n",
    ")\n",
    "\n",
    "# 2. EXAMPLE: Analyzing Authors and Topics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-COLUMN NETWORK ANALYSIS: AUTHORS + TOPICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create multi-column network\n",
    "G_at, columns_at, subject_type_at, node_column_map_at = create_multi_column_network(\n",
    "    df,\n",
    "    list_columns=['Authors'],  # List columns\n",
    "    dict_column='concept_dict',                    # No dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=3,                   # Only include items with at least 3 occurrences\n",
    "    top_n_per_column={                   # Limit items per column\n",
    "        'Authors': 10\n",
    "    },\n",
    "    overall_top_n=20 \n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "fig, ax_main=visualize_multi_column_graph(G_at, columns_at, subject_main_dir, subject_type_at)\n",
    "# legend_ax = fix_legend_display(fig, ax_main)\n",
    "# Analyze the multi-column network\n",
    "analysis_at = analyze_multi_column_network(\n",
    "    G_at, \n",
    "    columns_at, \n",
    "    subject_main_dir, \n",
    "    subject_type_at,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Analyze cross-column communities\n",
    "community_analysis_at = analyze_cross_column_communities(\n",
    "    G_at,\n",
    "    analysis_at['communities'],\n",
    "    columns_at\n",
    ")\n",
    "\n",
    "# Visualize community composition\n",
    "visualize_community_composition(\n",
    "    community_analysis_at,\n",
    "    columns_at,\n",
    "    subject_main_dir,\n",
    "    subject_type_at\n",
    ")\n",
    "\n",
    "# 3. EXAMPLE: Analyzing Institutions with Fields and Sub-fields\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-COLUMN NETWORK ANALYSIS: INSTITUTIONS + FIELDS + SUB-FIELDS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create multi-column network\n",
    "G_ifs, columns_ifs, subject_type_ifs, node_column_map_ifs = create_multi_column_network(\n",
    "    df,\n",
    "    list_columns=['Institutions', 'Topics'],  # List columns\n",
    "    dict_column=None,                                      # No dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=3,                                     # Only include items with at least 3 occurrences\n",
    "    top_n_per_column={                                     # Limit items per column\n",
    "        'Institutions': 10,\n",
    "        'Topics': 10\n",
    "    }\n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "visualize_multi_column_graph(G_ifs, columns_ifs, subject_main_dir, subject_type_ifs)\n",
    "\n",
    "# Analyze the multi-column network\n",
    "analysis_ifs = analyze_multi_column_network(\n",
    "    G_ifs, \n",
    "    columns_ifs, \n",
    "    subject_main_dir, \n",
    "    subject_type_ifs,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Analyze cross-column communities\n",
    "community_analysis_ifs = analyze_cross_column_communities(\n",
    "    G_ifs,\n",
    "    analysis_ifs['communities'],\n",
    "    columns_ifs\n",
    ")\n",
    "\n",
    "# Visualize community composition\n",
    "visualize_community_composition(\n",
    "    community_analysis_ifs,\n",
    "    columns_ifs,\n",
    "    subject_main_dir,\n",
    "    subject_type_ifs\n",
    ")\n",
    "\n",
    "# 4. EXAMPLE: Linking Authors, Institutions, and Concepts\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-COLUMN NETWORK ANALYSIS: AUTHORS + INSTITUTIONS + CONCEPTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create multi-column network\n",
    "G_aic, columns_aic, subject_type_aic, node_column_map_aic = create_multi_column_network(\n",
    "    df_flattened,\n",
    "    list_columns=['Institutions', 'Countries_flat'],  # List columns\n",
    "    dict_column=None,               # Dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=3,                        # Only include items with at least 3 occurrences\n",
    "    top_n_per_column={                        # Limit items per column\n",
    "        'Institutions': 10,\n",
    "        'Countries_flat': 10\n",
    "    },\n",
    "    overall_top_n=100                         # Overall limit on total nodes\n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "visualize_multi_column_graph(G_aic, columns_aic, subject_main_dir, subject_type_aic)\n",
    "\n",
    "# Analyze the multi-column network\n",
    "analysis_aic = analyze_multi_column_network(\n",
    "    G_aic, \n",
    "    columns_aic, \n",
    "    subject_main_dir, \n",
    "    subject_type_aic,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# 5. CUSTOM ANALYSIS: Finding bridging concepts between fields\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CUSTOM ANALYSIS: BRIDGING CONCEPTS BETWEEN FIELDS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 6. EXAMPLE: Analyzing Concepts, Fields, and Domains together\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-COLUMN NETWORK ANALYSIS: FIELDS + SUB-FIELDS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create multi-column network\n",
    "G_cfd, columns_cfd, subject_type_cfd, node_column_map_cfd = create_multi_column_network(\n",
    "    df,\n",
    "    list_columns=['Fields', 'Sub-fields'],  # List columns\n",
    "    dict_column=None,          # Dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=5,                   # Only include items with at least 5 occurrences\n",
    "    top_n_per_column={                   # Limit items per column\n",
    "        'Fields': 10,\n",
    "        'Sub-fields': 10\n",
    "    },\n",
    "    overall_top_n=20                  # Overall limit on total nodes\n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "fig, ax_main=visualize_multi_column_graph(G_cfd, columns_cfd, subject_main_dir, subject_type_cfd)\n",
    "# legend_ax = fix_legend_display(fig, ax_main)\n",
    "\n",
    "# Analyze the multi-column network\n",
    "analysis_cfd = analyze_multi_column_network(\n",
    "    G_cfd, \n",
    "    columns_cfd, \n",
    "    subject_main_dir, \n",
    "    subject_type_cfd,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Analyze cross-column communities\n",
    "community_analysis_cfd = analyze_cross_column_communities(\n",
    "    G_cfd,\n",
    "    analysis_cfd['communities'],\n",
    "    columns_cfd\n",
    ")\n",
    "\n",
    "# Visualize community composition\n",
    "visualize_community_composition(\n",
    "    community_analysis_cfd,\n",
    "    columns_cfd,\n",
    "    subject_main_dir,\n",
    "    subject_type_cfd\n",
    ")\n",
    "# Assuming we've created a Fields +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c441e-b191-48a6-a655-c564b26f2edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the multi-column network analysis functions\n",
    "\n",
    "# 1. EXAMPLE: Analyzing Concepts, Fields, and Domains together\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-COLUMN NETWORK ANALYSIS: CONCEPTS + FIELDS + DOMAINS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create multi-column network\n",
    "G_cfd, columns_cfd, subject_type_cfd, node_column_map_cfd = create_multi_column_network(\n",
    "    df,\n",
    "    list_columns=['Fields', 'Domains', 'Topics'],  # List columns\n",
    "    dict_column=None,          # Dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=5,                   # Only include items with at least 5 occurrences\n",
    "    top_n_per_column={                   # Limit items per column\n",
    "        'Fields': 5,\n",
    "        'Domains': 5,\n",
    "        'Topics': 5\n",
    "    },\n",
    "    overall_top_n=20                  # Overall limit on total nodes\n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "fig, ax_main=visualize_multi_column_graph(G_cfd, columns_cfd, subject_main_dir, subject_type_cfd)\n",
    "# legend_ax = fix_legend_display(fig, ax_main)\n",
    "\n",
    "# Analyze the multi-column network\n",
    "analysis_cfd = analyze_multi_column_network(\n",
    "    G_cfd, \n",
    "    columns_cfd, \n",
    "    subject_main_dir, \n",
    "    subject_type_cfd,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Analyze cross-column communities\n",
    "community_analysis_cfd = analyze_cross_column_communities(\n",
    "    G_cfd,\n",
    "    analysis_cfd['communities'],\n",
    "    columns_cfd\n",
    ")\n",
    "\n",
    "# Visualize community composition\n",
    "visualize_community_composition(\n",
    "    community_analysis_cfd,\n",
    "    columns_cfd,\n",
    "    subject_main_dir,\n",
    "    subject_type_cfd\n",
    ")\n",
    "\n",
    "# 2. EXAMPLE: Analyzing Authors and Topics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-COLUMN NETWORK ANALYSIS: AUTHORS + TOPICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create multi-column network\n",
    "G_at, columns_at, subject_type_at, node_column_map_at = create_multi_column_network(\n",
    "    df,\n",
    "    list_columns=['Authors', 'Topics'],  # List columns\n",
    "    dict_column=None,                    # No dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=3,                   # Only include items with at least 3 occurrences\n",
    "    top_n_per_column={                   # Limit items per column\n",
    "        'Authors': 10,\n",
    "        'Topics': 10\n",
    "    }\n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "fig, ax_main=visualize_multi_column_graph(G_at, columns_at, subject_main_dir, subject_type_at)\n",
    "# legend_ax = fix_legend_display(fig, ax_main)\n",
    "# Analyze the multi-column network\n",
    "analysis_at = analyze_multi_column_network(\n",
    "    G_at, \n",
    "    columns_at, \n",
    "    subject_main_dir, \n",
    "    subject_type_at,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Analyze cross-column communities\n",
    "community_analysis_at = analyze_cross_column_communities(\n",
    "    G_at,\n",
    "    analysis_at['communities'],\n",
    "    columns_at\n",
    ")\n",
    "\n",
    "# Visualize community composition\n",
    "visualize_community_composition(\n",
    "    community_analysis_at,\n",
    "    columns_at,\n",
    "    subject_main_dir,\n",
    "    subject_type_at\n",
    ")\n",
    "\n",
    "# 3. EXAMPLE: Analyzing Institutions with Fields and Sub-fields\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-COLUMN NETWORK ANALYSIS: INSTITUTIONS + FIELDS + SUB-FIELDS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create multi-column network\n",
    "G_ifs, columns_ifs, subject_type_ifs, node_column_map_ifs = create_multi_column_network(\n",
    "    df,\n",
    "    list_columns=['Institutions', 'Topics'],  # List columns\n",
    "    dict_column=None,                                      # No dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=3,                                     # Only include items with at least 3 occurrences\n",
    "    top_n_per_column={                                     # Limit items per column\n",
    "        'Institutions': 10,\n",
    "        'Topics': 10\n",
    "    }\n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "visualize_multi_column_graph(G_ifs, columns_ifs, subject_main_dir, subject_type_ifs)\n",
    "\n",
    "# Analyze the multi-column network\n",
    "analysis_ifs = analyze_multi_column_network(\n",
    "    G_ifs, \n",
    "    columns_ifs, \n",
    "    subject_main_dir, \n",
    "    subject_type_ifs,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Analyze cross-column communities\n",
    "community_analysis_ifs = analyze_cross_column_communities(\n",
    "    G_ifs,\n",
    "    analysis_ifs['communities'],\n",
    "    columns_ifs\n",
    ")\n",
    "\n",
    "# Visualize community composition\n",
    "visualize_community_composition(\n",
    "    community_analysis_ifs,\n",
    "    columns_ifs,\n",
    "    subject_main_dir,\n",
    "    subject_type_ifs\n",
    ")\n",
    "\n",
    "# 4. EXAMPLE: Linking Authors, Institutions, and Concepts\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-COLUMN NETWORK ANALYSIS: AUTHORS + INSTITUTIONS + CONCEPTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create multi-column network\n",
    "G_aic, columns_aic, subject_type_aic, node_column_map_aic = create_multi_column_network(\n",
    "    df_flattened,\n",
    "    list_columns=['Institutions', 'Countries_flat'],  # List columns\n",
    "    dict_column=None,               # Dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=3,                        # Only include items with at least 3 occurrences\n",
    "    top_n_per_column={                        # Limit items per column\n",
    "        'Institutions': 10,\n",
    "        'Countries_flat': 10\n",
    "    },\n",
    "    overall_top_n=100                         # Overall limit on total nodes\n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "visualize_multi_column_graph(G_aic, columns_aic, subject_main_dir, subject_type_aic)\n",
    "\n",
    "# Analyze the multi-column network\n",
    "analysis_aic = analyze_multi_column_network(\n",
    "    G_aic, \n",
    "    columns_aic, \n",
    "    subject_main_dir, \n",
    "    subject_type_aic,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# 5. CUSTOM ANALYSIS: Finding bridging concepts between fields\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CUSTOM ANALYSIS: BRIDGING CONCEPTS BETWEEN FIELDS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 6. EXAMPLE: Analyzing Concepts, Fields, and Domains together\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-COLUMN NETWORK ANALYSIS: FIELDS + SUB-FIELDS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create multi-column network\n",
    "G_cfd, columns_cfd, subject_type_cfd, node_column_map_cfd = create_multi_column_network(\n",
    "    df,\n",
    "    list_columns=['Fields', 'Sub-fields'],  # List columns\n",
    "    dict_column=None,          # Dictionary column\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=5,                   # Only include items with at least 5 occurrences\n",
    "    top_n_per_column={                   # Limit items per column\n",
    "        'Fields': 10,\n",
    "        'Sub-fields': 10\n",
    "    },\n",
    "    overall_top_n=20                  # Overall limit on total nodes\n",
    ")\n",
    "\n",
    "# Visualize the multi-column network\n",
    "fig, ax_main=visualize_multi_column_graph(G_cfd, columns_cfd, subject_main_dir, subject_type_cfd)\n",
    "# legend_ax = fix_legend_display(fig, ax_main)\n",
    "\n",
    "# Analyze the multi-column network\n",
    "analysis_cfd = analyze_multi_column_network(\n",
    "    G_cfd, \n",
    "    columns_cfd, \n",
    "    subject_main_dir, \n",
    "    subject_type_cfd,\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Analyze cross-column communities\n",
    "community_analysis_cfd = analyze_cross_column_communities(\n",
    "    G_cfd,\n",
    "    analysis_cfd['communities'],\n",
    "    columns_cfd\n",
    ")\n",
    "\n",
    "# Visualize community composition\n",
    "visualize_community_composition(\n",
    "    community_analysis_cfd,\n",
    "    columns_cfd,\n",
    "    subject_main_dir,\n",
    "    subject_type_cfd\n",
    ")\n",
    "# Assuming we've created a Fields +"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705cbe35-4c0c-4122-ab3b-b1fefb5958bd",
   "metadata": {},
   "source": [
    "# Concepts analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64033b5f-457c-473b-8d34-5204518275f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_top_concepts(df, column='concept_dict', n=20, subject_main_dir=subject_main_dir):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the top n common concepts from the concept dictionary column.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe containing the data\n",
    "    column (str): The name of the column containing concept dictionaries\n",
    "    n (int): Number of top concepts to display\n",
    "    subject_main_dir (str/Path): Main directory for all subject analyses\n",
    "    \n",
    "    Returns:\n",
    "    pandas.Series: Series containing the counts of the top n concepts\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Extract subject type from column name\n",
    "    subject_type = column.lower().split('_')[0] + 's'  # Assuming concept_dict -> concepts\n",
    "    \n",
    "    # Collect all concepts\n",
    "    all_concepts = []\n",
    "    \n",
    "    # Parse each concept dictionary and extract the concepts (keys)\n",
    "    for concept_str in df[column]:\n",
    "        try:\n",
    "            # Convert string representation of dict to actual dict\n",
    "            concept_dict = ast.literal_eval(concept_str) if isinstance(concept_str, str) else concept_str\n",
    "            \n",
    "            # Add all concept keys to our list\n",
    "            all_concepts.extend(list(concept_dict.keys()))\n",
    "        except (ValueError, SyntaxError, TypeError):\n",
    "            # Skip invalid entries\n",
    "            continue\n",
    "    \n",
    "    # Count the occurrences of each concept\n",
    "    concept_counts = pd.Series(all_concepts).value_counts()\n",
    "    \n",
    "    # Get the top n concepts\n",
    "    top_concepts = concept_counts.head(n)\n",
    "    \n",
    "    # Create the visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create a horizontal bar chart\n",
    "    ax = sns.barplot(x=top_concepts.values, y=top_concepts.index, palette='viridis')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title(f'Top {n} Common Concepts', fontsize=16)\n",
    "    plt.xlabel('Count', fontsize=12)\n",
    "    plt.ylabel('Concept', fontsize=12)\n",
    "    \n",
    "    # Add count values to the end of each bar\n",
    "    for i, v in enumerate(top_concepts.values):\n",
    "        ax.text(v + 0.5, i, str(v), va='center')\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, f\"top_{n}_common_concepts.png\", subject_main_dir, subject_type)\n",
    "    \n",
    "    # Return the data for further analysis if needed\n",
    "    return top_concepts\n",
    "\n",
    "# Example usage:\n",
    "top_concepts = analyze_top_concepts(df, column='concept_dict', n=20, subject_main_dir=subject_main_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62371be4-980b-4d83-bcdb-46dec21bfdcb",
   "metadata": {},
   "source": [
    "## General graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79bab9-0059-4bb2-b883-e666547ec0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_graph = create_network_visualization(\n",
    "    df, \n",
    "    column_name='concept_dict', \n",
    "    subject_main_dir=subject_main_dir,\n",
    "    top_n=50,  # Limit to top 50 most frequent concepts\n",
    "    min_occurrences=5  # Only include concepts that appear at least 5 times\n",
    ")\n",
    "print_network_stats(concepts_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411de84-fb4e-4a9a-b5ff-9602a4a916ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONCEPTS COMMUNITY ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "concept_communities, concept_cliques, concept_comm_stats = analyze_network_communities(\n",
    "    concepts_graph, \n",
    "    subject_main_dir, \n",
    "    \"concepts\", \n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Visualize concept community statistics\n",
    "visualize_community_stats(concept_comm_stats, subject_main_dir, \"concepts\")\n",
    "\n",
    "# Find concept bridge nodes\n",
    "concept_bridges = find_community_bridges(concepts_graph, concept_communities, subject_main_dir, \"concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fff425-991c-41c6-983a-5d5334ceb515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal analysis of Concepts\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEMPORAL ANALYSIS OF CONCEPTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "concept_temporal_results = analyze_temporal_networks(\n",
    "    df,\n",
    "    column_name='concept_dict',\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    decade_col='decade',  # Using the existing decade column\n",
    "    min_occurrence=3,     # Only include concepts that appear at least 3 times in a decade\n",
    "    top_n=50,             # Limit to top 50 most frequent concepts per decade\n",
    "    min_clique_size=3     # Consider cliques of size 3 or larger as significant\n",
    ")\n",
    "\n",
    "# Print some insights from the temporal analysis\n",
    "print(f\"\\nFound {len(concept_temporal_results['persistent_items'])} concepts that persisted across multiple decades\")\n",
    "print(f\"Found {len(concept_temporal_results['persistent_connections'])} persistent concept connections\")\n",
    "\n",
    "# Get the most persistent concepts (present in most decades)\n",
    "most_persistent_concepts = sorted(\n",
    "    [(item, len(decades)) for item, decades in concept_temporal_results['persistent_items'].items()],\n",
    "    key=lambda x: x[1], \n",
    "    reverse=True\n",
    ")[:10]\n",
    "\n",
    "print(\"\\nMost persistent concepts across decades:\")\n",
    "for concept, num_decades in most_persistent_concepts:\n",
    "    print(f\"  {concept}: present in {num_decades} decades\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3776253-06a3-4cdd-ab0f-1c447767eb82",
   "metadata": {},
   "source": [
    "## Score-based graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7208d23c-a14c-424c-8be0-877b71a872c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def visualize_concept_network(df, concept_dict_column, \n",
    "                             top_n=20, subject_main_dir=\"output\", \n",
    "                             filename_prefix=\"concept_network\"):\n",
    "    \"\"\"\n",
    "    Visualize a network of top concepts based on their scores and co-appearances\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the concept dictionaries\n",
    "    concept_dict_column (str): Column in df containing dictionaries of concepts and their scores\n",
    "    top_n (int): Number of top concepts to include in the visualization\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    filename_prefix (str): Prefix for saved files\n",
    "    \n",
    "    Returns:\n",
    "    nx.Graph: The created network graph\n",
    "    \"\"\"\n",
    "    # Step 1: Extract and aggregate all concepts from the dataframe\n",
    "    aggregated_concepts = defaultdict(float)\n",
    "    \n",
    "    # First, extract all concepts and sum their scores across all rows\n",
    "    for _, row in df.iterrows():\n",
    "        # Get the concept dictionary from this row\n",
    "        concept_dict = row[concept_dict_column]\n",
    "        \n",
    "        # Handle string representation of dictionaries if needed\n",
    "        if isinstance(concept_dict, str):\n",
    "            try:\n",
    "                concept_dict = eval(concept_dict)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Skip if not a dictionary\n",
    "        if not isinstance(concept_dict, dict):\n",
    "            continue\n",
    "            \n",
    "        # Add the scores to our aggregated dictionary\n",
    "        for concept, score in concept_dict.items():\n",
    "            aggregated_concepts[concept] += score\n",
    "    \n",
    "    # Get top N concepts by aggregated score\n",
    "    top_concepts = sorted(aggregated_concepts.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    \n",
    "    # Create graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes with sizes based on scores\n",
    "    max_score = max([score for _, score in top_concepts])\n",
    "    min_score = min([score for _, score in top_concepts])\n",
    "    score_range = max_score - min_score if max_score != min_score else 1\n",
    "    \n",
    "    for concept, score in top_concepts:\n",
    "        # Normalize scores to node sizes (between 500 and 3000)\n",
    "        normalized_score = 500 + ((score - min_score) / score_range) * 2500\n",
    "        G.add_node(concept, score=score, size=normalized_score)\n",
    "    \n",
    "    # Calculate co-appearances from the concept dictionaries\n",
    "    # Create set of top concept names for efficient lookup\n",
    "    top_concept_names = {concept for concept, _ in top_concepts}\n",
    "    \n",
    "    # Count co-appearances\n",
    "    co_appearances = defaultdict(int)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Get the concept dictionary from this row\n",
    "        concept_dict = row[concept_dict_column]\n",
    "        \n",
    "        # Handle string representation of dictionaries if needed\n",
    "        if isinstance(concept_dict, str):\n",
    "            try:\n",
    "                concept_dict = eval(concept_dict)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Skip if not a dictionary\n",
    "        if not isinstance(concept_dict, dict):\n",
    "            continue\n",
    "        \n",
    "        # Get concepts that are in our top N list\n",
    "        row_concepts = [c for c in concept_dict.keys() if c in top_concept_names]\n",
    "        \n",
    "        # Add co-appearances for each pair\n",
    "        for i, concept1 in enumerate(row_concepts):\n",
    "            for concept2 in row_concepts[i+1:]:\n",
    "                # Create a sorted tuple to ensure consistent key ordering\n",
    "                pair = tuple(sorted([concept1, concept2]))\n",
    "                co_appearances[pair] += 1\n",
    "        \n",
    "        # Add edges to graph\n",
    "    if co_appearances:\n",
    "        max_weight = max(co_appearances.values())\n",
    "        min_weight = min(co_appearances.values()) if co_appearances else 1\n",
    "        weight_range = max_weight - min_weight if max_weight != min_weight else 1\n",
    "        \n",
    "        for (concept1, concept2), weight in co_appearances.items():\n",
    "            # Only add edges if both concepts are in our top N\n",
    "            G.add_edge(concept1, concept2, weight=weight)\n",
    "    \n",
    "    # Draw and save the network visualization\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    \n",
    "    # Set positions using spring layout\n",
    "    pos = nx.spring_layout(G, k=0.3, seed=42)\n",
    "    \n",
    "    # Draw nodes with sizes based on scores\n",
    "    node_sizes = [G.nodes[node]['size'] for node in G.nodes()]\n",
    "    \n",
    "    # Use a color gradient for nodes based on score\n",
    "    node_colors = []\n",
    "    for node in G.nodes():\n",
    "        score = G.nodes[node]['score']\n",
    "        # Normalize score between 0 and 1\n",
    "        norm_score = (score - min_score) / score_range if score_range > 0 else 0.5\n",
    "        # Use blue gradient (lighter blue for lower scores, darker for higher)\n",
    "        node_colors.append(plt.cm.Blues(0.5 + norm_score/2))  # Start from middle of colormap\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, \n",
    "                          node_size=node_sizes, \n",
    "                          node_color=node_colors, \n",
    "                          alpha=0.8,\n",
    "                          edgecolors='white',\n",
    "                          linewidths=1.5)\n",
    "    \n",
    "    # Draw edges with weights determining thickness and color intensity\n",
    "    edges = G.edges()\n",
    "    weights = [G[u][v]['weight'] for u, v in edges]\n",
    "    \n",
    "    if weights:\n",
    "        max_weight = max(weights)\n",
    "        min_weight = min(weights)\n",
    "        weight_range = max_weight - min_weight if max_weight != min_weight else 1\n",
    "            \n",
    "        for (u, v, data) in G.edges(data=True):\n",
    "            weight = data['weight']\n",
    "            # Normalize weight for visual properties\n",
    "            norm_weight = (weight - min_weight) / weight_range if weight_range > 0 else 0.5\n",
    "            # Width between 1 and 5\n",
    "            width = 1 + norm_weight * 4\n",
    "            # Alpha between 0.3 and 0.9\n",
    "            alpha = 0.3 + norm_weight * 0.6\n",
    "            \n",
    "            nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], \n",
    "                                  width=width, \n",
    "                                  alpha=alpha,\n",
    "                                  edge_color='gray')\n",
    "    \n",
    "    # Calculate node labels with appropriate font sizes\n",
    "    labels = {}\n",
    "    for node in G.nodes():\n",
    "        # Scale font size based on node size (from 9 to 14)\n",
    "        node_size = G.nodes[node]['size']\n",
    "        size_ratio = (node_size - 500) / 2500  # Normalized between 0 and 1\n",
    "        font_size = 9 + size_ratio * 5  # Scale between 9 and 14\n",
    "        \n",
    "        labels[node] = {'label': node, 'fontsize': font_size}\n",
    "    \n",
    "    # Draw labels with varying font sizes\n",
    "    for node, label_info in labels.items():\n",
    "        plt.text(pos[node][0], pos[node][1], label_info['label'],\n",
    "                 fontsize=label_info['fontsize'],\n",
    "                 ha='center', va='center',\n",
    "                 bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "    \n",
    "    # Add legend for node sizes\n",
    "    sizes = [500, 1500, 3000]  # Small, medium, large\n",
    "    size_labels = [\n",
    "        f\"Low Score ({min_score:.2f})\",\n",
    "        f\"Medium Score\",\n",
    "        f\"High Score ({max_score:.2f})\"\n",
    "    ]\n",
    "    \n",
    "    # Create legend handles\n",
    "    legend_handles = []\n",
    "    for size, label in zip(sizes, size_labels):\n",
    "        handle = plt.Line2D([0], [0], marker='o', color='w', \n",
    "                          label=label, \n",
    "                          markerfacecolor=plt.cm.Blues(0.7),\n",
    "                          markersize=np.sqrt(size)/50)  # Scale down for legend\n",
    "        legend_handles.append(handle)\n",
    "    \n",
    "    # Add legend for edge weights if we have co-appearance data\n",
    "    if weights:\n",
    "        # Add edge weight legend\n",
    "        edge_weights = [min_weight, (min_weight + max_weight)/2, max_weight]\n",
    "        edge_labels = [\n",
    "            f\"Few Co-appearances ({min_weight})\",\n",
    "            f\"Medium Co-appearances\",\n",
    "            f\"Many Co-appearances ({max_weight})\"\n",
    "        ]\n",
    "        \n",
    "        # Add line handles to legend\n",
    "        for weight, label in zip(edge_weights, edge_labels):\n",
    "            norm_weight = (weight - min_weight) / weight_range if weight_range > 0 else 0.5\n",
    "            width = 1 + norm_weight * 4\n",
    "            handle = plt.Line2D([0], [0], color='gray', \n",
    "                              linewidth=width, \n",
    "                              alpha=0.3 + norm_weight * 0.6,\n",
    "                              label=label)\n",
    "            legend_handles.append(handle)\n",
    "    \n",
    "    plt.legend(handles=legend_handles, loc='upper right', fontsize=10)\n",
    "    \n",
    "    plt.title(f\"Top {top_n} Concepts Network by Score\", fontsize=16)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Save the visualization\n",
    "    save_plot(plt, f\"{filename_prefix}.png\", subject_main_dir, \"concepts\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "visualize_concept_network(\n",
    "    df=df,\n",
    "    concept_dict_column=\"concept_dict\",\n",
    "    top_n=20,\n",
    "    subject_main_dir=subject_main_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97406ba1-dd03-4d22-b654-26d62066504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "\n",
    "def analyze_concept_trends_by_decade(df, concept_dict_column, decade_column, \n",
    "                                    top_n=20, subject_main_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    Analyze temporal trends of concept scores and co-appearances across decades\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing concept dictionaries and decade information\n",
    "    concept_dict_column (str): Column containing dictionaries of concepts and scores\n",
    "    decade_column (str): Column containing decade information (e.g., \"1980s\", \"1990s\")\n",
    "    top_n (int): Number of top concepts to analyze\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (concept_trends_df, co_appearance_trends_df)\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Step 1: Identify top concepts across the entire dataset\n",
    "    # Extract and aggregate all concepts from the dataframe\n",
    "    aggregated_concepts = defaultdict(float)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Get the concept dictionary from this row\n",
    "        concept_dict = row[concept_dict_column]\n",
    "        \n",
    "        # Handle string representation of dictionaries if needed\n",
    "        if isinstance(concept_dict, str):\n",
    "            try:\n",
    "                concept_dict = eval(concept_dict)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Skip if not a dictionary\n",
    "        if not isinstance(concept_dict, dict):\n",
    "            continue\n",
    "            \n",
    "        # Add the scores to our aggregated dictionary\n",
    "        for concept, score in concept_dict.items():\n",
    "            aggregated_concepts[concept] += score\n",
    "    \n",
    "    # Get top N concepts by aggregated score\n",
    "    top_concepts = [concept for concept, _ in \n",
    "                   sorted(aggregated_concepts.items(), \n",
    "                         key=lambda x: x[1], reverse=True)[:top_n]]\n",
    "    \n",
    "    # Step 2: Ensure decade column is properly formatted\n",
    "    if df[decade_column].dtype != 'object':\n",
    "        df[decade_column] = df[decade_column].astype(str)\n",
    "    \n",
    "    # Get sorted unique decades\n",
    "    all_decades = sorted(df[decade_column].unique())\n",
    "    \n",
    "    # Step 3: Analyze concept scores by decade\n",
    "    concept_trends = {concept: [] for concept in top_concepts}\n",
    "    \n",
    "    # Process each decade\n",
    "    for decade in all_decades:\n",
    "        decade_df = df[df[decade_column] == decade]\n",
    "        \n",
    "        # Initialize scores for this decade\n",
    "        decade_scores = defaultdict(float)\n",
    "        \n",
    "        # Process each row in this decade\n",
    "        for _, row in decade_df.iterrows():\n",
    "            concept_dict = row[concept_dict_column]\n",
    "            \n",
    "            # Handle string representation if needed\n",
    "            if isinstance(concept_dict, str):\n",
    "                try:\n",
    "                    concept_dict = eval(concept_dict)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Skip if not a dictionary\n",
    "            if not isinstance(concept_dict, dict):\n",
    "                continue\n",
    "                \n",
    "            # Sum scores for top concepts in this decade\n",
    "            for concept, score in concept_dict.items():\n",
    "                if concept in top_concepts:\n",
    "                    decade_scores[concept] += score\n",
    "        \n",
    "        # Store scores for each concept in this decade\n",
    "        for concept in top_concepts:\n",
    "            concept_trends[concept].append(decade_scores[concept])\n",
    "    \n",
    "    # Create dataframe from trend data\n",
    "    trend_df = pd.DataFrame(concept_trends, index=all_decades)\n",
    "    \n",
    "    # Step 4: Analyze concept co-appearances by decade\n",
    "    co_appearance_trends = {}\n",
    "    \n",
    "    # For each decade, count co-appearances between top concepts\n",
    "    for decade in all_decades:\n",
    "        decade_df = df[df[decade_column] == decade]\n",
    "        \n",
    "        # Initialize co-appearance counter for this decade\n",
    "        decade_co_appearances = defaultdict(int)\n",
    "        \n",
    "        # Process each row in this decade\n",
    "        for _, row in decade_df.iterrows():\n",
    "            concept_dict = row[concept_dict_column]\n",
    "            \n",
    "            # Handle string representation if needed\n",
    "            if isinstance(concept_dict, str):\n",
    "                try:\n",
    "                    concept_dict = eval(concept_dict)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Skip if not a dictionary\n",
    "            if not isinstance(concept_dict, dict):\n",
    "                continue\n",
    "                \n",
    "            # Get top concepts that appear in this row\n",
    "            row_concepts = [c for c in concept_dict.keys() if c in top_concepts]\n",
    "            \n",
    "            # Count co-appearances for each pair\n",
    "            for i, concept1 in enumerate(row_concepts):\n",
    "                for concept2 in row_concepts[i+1:]:\n",
    "                    # Create a sorted tuple to ensure consistent key ordering\n",
    "                    pair = tuple(sorted([concept1, concept2]))\n",
    "                    decade_co_appearances[pair] += 1\n",
    "        \n",
    "        # Store co-appearance counts for this decade\n",
    "        for pair, count in decade_co_appearances.items():\n",
    "            # Create key for the pair if it doesn't exist\n",
    "            if pair not in co_appearance_trends:\n",
    "                co_appearance_trends[pair] = []\n",
    "            \n",
    "            # Ensure we have data for all preceding decades\n",
    "            while len(co_appearance_trends[pair]) < len(all_decades) - 1:\n",
    "                co_appearance_trends[pair].append(0)\n",
    "            \n",
    "            # Add this decade's count\n",
    "            co_appearance_trends[pair].append(count)\n",
    "    \n",
    "    # Ensure all pairs have values for all decades\n",
    "    for pair in co_appearance_trends:\n",
    "        while len(co_appearance_trends[pair]) < len(all_decades):\n",
    "            co_appearance_trends[pair].append(0)\n",
    "    \n",
    "    # Create dataframe from co-appearance data\n",
    "    # Use concept pairs as column names\n",
    "    pair_columns = {pair: f\"{pair[0]} & {pair[1]}\" for pair in co_appearance_trends.keys()}\n",
    "    \n",
    "    # Verify all arrays have the same length as all_decades before creating DataFrame\n",
    "    for pair, counts in co_appearance_trends.items():\n",
    "        if len(counts) != len(all_decades):\n",
    "            # Fix the length by either truncating or padding with zeros\n",
    "            if len(counts) > len(all_decades):\n",
    "                co_appearance_trends[pair] = counts[:len(all_decades)]\n",
    "            else:\n",
    "                co_appearance_trends[pair] = counts + [0] * (len(all_decades) - len(counts))\n",
    "    \n",
    "    co_appearance_df = pd.DataFrame(\n",
    "        {pair_columns[pair]: counts for pair, counts in co_appearance_trends.items()},\n",
    "        index=all_decades\n",
    "    )\n",
    "    \n",
    "    # Step 5: Visualize the results\n",
    "    \n",
    "    # 5.1: Visualize concept score trends\n",
    "    visualize_concept_trends(trend_df, subject_main_dir, top_n)\n",
    "    \n",
    "    # 5.2: Visualize co-appearance trends for top pairs\n",
    "    visualize_co_appearance_trends(co_appearance_df, subject_main_dir, top_n)\n",
    "    \n",
    "    # 5.3: Create network evolution visualization\n",
    "    if len(all_decades) > 1:\n",
    "        visualize_network_evolution(df, concept_dict_column, decade_column, \n",
    "                                  all_decades, top_concepts, subject_main_dir)\n",
    "    \n",
    "    return trend_df, co_appearance_df\n",
    "\n",
    "def visualize_concept_trends(trend_df, subject_main_dir, top_n=20):\n",
    "    \"\"\"Visualize trends in concept scores across decades\"\"\"\n",
    "    # Create concepts subfolder\n",
    "    concept_dir = Path(subject_main_dir) / \"concepts\"\n",
    "    concept_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Limit to displaying a reasonable number of lines for readability\n",
    "    display_limit = min(10, len(trend_df.columns))\n",
    "    \n",
    "    # Sort concepts by their total score (sum across all decades)\n",
    "    concept_totals = trend_df.sum().sort_values(ascending=False)\n",
    "    top_concepts = concept_totals.index[:display_limit]\n",
    "    \n",
    "    # Prepare multiple visualizations\n",
    "    \n",
    "    # 1. Line plot of concept scores over decades for top concepts\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot each concept as a line\n",
    "    for concept in top_concepts:\n",
    "        plt.plot(trend_df.index, trend_df[concept], marker='o', linewidth=2, label=concept)\n",
    "    \n",
    "    plt.title(f'Top {display_limit} Concept Scores Across Decades', fontsize=16)\n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel('Aggregated Score', fontsize=12)\n",
    "    plt.legend(title='Concepts', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(concept_dir / f'concept_trends_line_top{display_limit}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Heatmap of all top concepts\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Normalize data for better visualization\n",
    "    # Use only the top N concepts sorted by total score\n",
    "    heatmap_data = trend_df[concept_totals.index[:top_n]]\n",
    "    \n",
    "    # Create the heatmap\n",
    "    sns.heatmap(heatmap_data.T, cmap='YlOrRd', annot=False, \n",
    "               linewidths=0.5, cbar_kws={'label': 'Score'})\n",
    "    \n",
    "    plt.title(f'Heatmap of Top {top_n} Concept Scores Across Decades', fontsize=16)\n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel('Concept', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(concept_dir / f'concept_trends_heatmap_top{top_n}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Stacked area chart to show relative importance\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Use only the top concepts for clarity\n",
    "    area_data = trend_df[top_concepts]\n",
    "    \n",
    "    # Create stacked area chart\n",
    "    plt.stackplot(area_data.index, [area_data[concept] for concept in top_concepts], \n",
    "                labels=top_concepts, alpha=0.8)\n",
    "    \n",
    "    plt.title(f'Relative Importance of Top {display_limit} Concepts Across Decades', fontsize=16)\n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel('Aggregated Score', fontsize=12)\n",
    "    plt.legend(title='Concepts', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(concept_dir / f'concept_trends_area_top{display_limit}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Normalized line plot to show relative trends regardless of volume\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Normalize each concept's scores to percentages of its maximum\n",
    "    norm_data = trend_df[top_concepts].copy()\n",
    "    for concept in top_concepts:\n",
    "        max_val = norm_data[concept].max()\n",
    "        if max_val > 0:  # Avoid division by zero\n",
    "            norm_data[concept] = (norm_data[concept] / max_val) * 100\n",
    "    \n",
    "    # Plot normalized trends\n",
    "    for concept in top_concepts:\n",
    "        plt.plot(norm_data.index, norm_data[concept], marker='o', linewidth=2, label=concept)\n",
    "    \n",
    "    plt.title(f'Normalized Trends of Top {display_limit} Concepts (% of Peak Value)', fontsize=16)\n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel('Percentage of Peak Value', fontsize=12)\n",
    "    plt.legend(title='Concepts', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(concept_dir / f'concept_trends_normalized_top{display_limit}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def visualize_co_appearance_trends(co_appearance_df, subject_main_dir, top_n=20):\n",
    "    \"\"\"Visualize trends in concept co-appearances across decades\"\"\"\n",
    "    # Create concepts subfolder\n",
    "    concept_dir = Path(subject_main_dir) / \"concepts\"\n",
    "    concept_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # If no co-appearances, exit\n",
    "    if co_appearance_df.empty:\n",
    "        print(\"No co-appearance data to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Limit to displaying a reasonable number of co-occurrence pairs\n",
    "    display_limit = min(8, len(co_appearance_df.columns))\n",
    "    \n",
    "    # Find top co-appearance pairs\n",
    "    pair_totals = co_appearance_df.sum().sort_values(ascending=False)\n",
    "    top_pairs = pair_totals.index[:display_limit]\n",
    "    \n",
    "    # 1. Line plot of co-appearance trends for top pairs\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot each pair as a line\n",
    "    for pair in top_pairs:\n",
    "        plt.plot(co_appearance_df.index, co_appearance_df[pair], marker='o', \n",
    "                linewidth=2, label=pair)\n",
    "    \n",
    "    plt.title(f'Top {display_limit} Concept Co-appearance Trends Across Decades', fontsize=16)\n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel('Co-appearance Count', fontsize=12)\n",
    "    plt.legend(title='Concept Pairs', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(concept_dir / f'coappearance_trends_line_top{display_limit}.png', \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Heatmap of all top co-appearances\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Limit to top N pairs for readability\n",
    "    heatmap_pairs = min(top_n, len(pair_totals))\n",
    "    heatmap_data = co_appearance_df[pair_totals.index[:heatmap_pairs]]\n",
    "    \n",
    "    # Create the heatmap\n",
    "    sns.heatmap(heatmap_data.T, cmap='YlGnBu', annot=False, \n",
    "               linewidths=0.5, cbar_kws={'label': 'Co-appearance Count'})\n",
    "    \n",
    "    plt.title(f'Heatmap of Top {heatmap_pairs} Concept Co-appearances Across Decades', fontsize=16)\n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel('Concept Pair', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(concept_dir / f'coappearance_trends_heatmap_top{heatmap_pairs}.png', \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Stacked bar chart for comparing co-appearance patterns by decade\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Prepare data\n",
    "    bar_data = heatmap_data.copy()\n",
    "    decades = bar_data.index\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    bottom = np.zeros(len(decades))\n",
    "    for pair in bar_data.columns:\n",
    "        plt.bar(decades, bar_data[pair], bottom=bottom, label=pair)\n",
    "        bottom += bar_data[pair].values\n",
    "    \n",
    "    plt.title(f'Stacked Co-appearance Patterns Across Decades', fontsize=16)\n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel('Co-appearance Count', fontsize=12)\n",
    "    plt.legend(title='Concept Pairs', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(concept_dir / f'coappearance_stacked_bar.png', \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def visualize_network_evolution(df, concept_dict_column, decade_column, \n",
    "                               decades, top_concepts, subject_main_dir):\n",
    "    \"\"\"Create network visualizations showing concept relationships across decades\"\"\"\n",
    "    # Create concepts/networks subfolder\n",
    "    network_dir = Path(subject_main_dir) / \"concepts\" / \"network_evolution\"\n",
    "    network_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # We'll create individual decade networks and a summary visualization\n",
    "    decade_networks = []\n",
    "    \n",
    "    # Process each decade\n",
    "    for decade in decades:\n",
    "        # Filter data for this decade\n",
    "        decade_df = df[df[decade_column] == decade]\n",
    "        \n",
    "        # Skip if no data for this decade\n",
    "        if decade_df.empty:\n",
    "            continue\n",
    "            \n",
    "        # Create network for this decade\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes (top concepts)\n",
    "        for concept in top_concepts:\n",
    "            G.add_node(concept, count=0, score=0)\n",
    "        \n",
    "        # Initialize co-appearance counter\n",
    "        co_appearances = defaultdict(int)\n",
    "        \n",
    "        # Process each row to extract concept relationships\n",
    "        for _, row in decade_df.iterrows():\n",
    "            concept_dict = row[concept_dict_column]\n",
    "            \n",
    "            # Handle string representation if needed\n",
    "            if isinstance(concept_dict, str):\n",
    "                try:\n",
    "                    concept_dict = eval(concept_dict)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Skip if not a dictionary\n",
    "            if not isinstance(concept_dict, dict):\n",
    "                continue\n",
    "                \n",
    "            # Update node data and count co-appearances\n",
    "            row_concepts = []\n",
    "            \n",
    "            for concept, score in concept_dict.items():\n",
    "                if concept in top_concepts:\n",
    "                    # Add to row concepts for co-appearance calculation\n",
    "                    row_concepts.append(concept)\n",
    "                    \n",
    "                    # Update node data\n",
    "                    G.nodes[concept]['count'] = G.nodes[concept].get('count', 0) + 1\n",
    "                    G.nodes[concept]['score'] = G.nodes[concept].get('score', 0) + score\n",
    "            \n",
    "            # Count co-appearances\n",
    "            for i, concept1 in enumerate(row_concepts):\n",
    "                for concept2 in row_concepts[i+1:]:\n",
    "                    pair = tuple(sorted([concept1, concept2]))\n",
    "                    co_appearances[pair] += 1\n",
    "        \n",
    "        # Add edges based on co-appearances\n",
    "        for (concept1, concept2), weight in co_appearances.items():\n",
    "            G.add_edge(concept1, concept2, weight=weight)\n",
    "        \n",
    "        # Store the network\n",
    "        decade_networks.append((decade, G))\n",
    "        \n",
    "        # Create visualization for this decade\n",
    "        visualize_decade_network(G, decade, network_dir)\n",
    "    \n",
    "    # Create summary visualization comparing networks across decades\n",
    "    if len(decade_networks) > 1:\n",
    "        visualize_network_comparison(decade_networks, network_dir)\n",
    "\n",
    "def visualize_decade_network(G, decade, output_dir):\n",
    "    \"\"\"Visualize network for a specific decade\"\"\"\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    # Calculate node sizes based on scores\n",
    "    node_sizes = []\n",
    "    node_colors = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        score = G.nodes[node].get('score', 0)\n",
    "        count = G.nodes[node].get('count', 0)\n",
    "        \n",
    "        # Size based on score, with a minimum size\n",
    "        size = max(300, score * 10)  # Adjusted multiplier for better visibility\n",
    "        node_sizes.append(size)\n",
    "        \n",
    "        # Color intensity based on frequency\n",
    "        # More frequent = darker blue\n",
    "        color_intensity = min(1.0, count / 10)  # Cap at 1.0\n",
    "        node_colors.append((0.1, 0.1, 0.5 + 0.5 * color_intensity))\n",
    "    \n",
    "    # Set positions using spring layout\n",
    "    pos = nx.spring_layout(G, k=0.3, seed=42)\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, \n",
    "                          node_size=node_sizes, \n",
    "                          node_color=node_colors, \n",
    "                          alpha=0.8)\n",
    "    \n",
    "    # Draw edges with weights determining thickness\n",
    "    edges = G.edges()\n",
    "    weights = [G[u][v]['weight'] for u, v in edges]\n",
    "    \n",
    "    if weights:\n",
    "        max_weight = max(weights)\n",
    "        min_weight = min(weights)\n",
    "        weight_range = max_weight - min_weight if max_weight != min_weight else 1\n",
    "        \n",
    "        for (u, v, data) in G.edges(data=True):\n",
    "            weight = data['weight']\n",
    "            # Normalize weight for visual properties\n",
    "            norm_weight = (weight - min_weight) / weight_range if weight_range > 0 else 0.5\n",
    "            # Width between 1 and 5\n",
    "            width = 1 + norm_weight * 4\n",
    "            # Alpha between 0.3 and 0.9\n",
    "            alpha = 0.3 + norm_weight * 0.6\n",
    "            \n",
    "            nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], \n",
    "                                 width=width, \n",
    "                                 alpha=alpha,\n",
    "                                 edge_color='gray')\n",
    "    \n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold',\n",
    "                          bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', \n",
    "                                   boxstyle='round,pad=0.3'))\n",
    "    \n",
    "    plt.title(f'Concept Network for Decade: {decade}', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(output_dir / f'network_{decade}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def visualize_network_comparison(decade_networks, output_dir):\n",
    "    \"\"\"Create a visualization comparing networks across decades\"\"\"\n",
    "    # We'll focus on how specific metrics change over time\n",
    "    \n",
    "    # 1. Track changes in node importance (score) over time\n",
    "    # Extract data\n",
    "    decades = []\n",
    "    concept_scores = defaultdict(list)\n",
    "    concept_counts = defaultdict(list)\n",
    "    \n",
    "    for decade, G in decade_networks:\n",
    "        decades.append(decade)\n",
    "        \n",
    "        for node in G.nodes():\n",
    "            score = G.nodes[node].get('score', 0)\n",
    "            count = G.nodes[node].get('count', 0)\n",
    "            \n",
    "            concept_scores[node].append(score)\n",
    "            concept_counts[node].append(count)\n",
    "    \n",
    "    # Find top concepts based on total score\n",
    "    total_scores = {concept: sum(scores) for concept, scores in concept_scores.items()}\n",
    "    top_concepts = sorted(total_scores.items(), key=lambda x: x[1], reverse=True)[:8]\n",
    "    \n",
    "    # Create line plot showing score changes\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    for concept, _ in top_concepts:\n",
    "        plt.plot(decades, concept_scores[concept], marker='o', linewidth=2, label=concept)\n",
    "    \n",
    "    plt.title('Evolution of Concept Scores Across Decades', fontsize=16)\n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.legend(title='Top Concepts', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(output_dir / 'concept_score_evolution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Track changes in relationship strength over time\n",
    "    # Track existing edges for this decade\n",
    "    decade_edges = set()\n",
    "    edge_weights=defaultdict()\n",
    "    for decade, G in decade_networks:\n",
    "        decade_count = 0  # Keep track of the decade's position\n",
    "        \n",
    "        for u, v, data in G.edges(data=True):\n",
    "            edge = tuple(sorted([u, v]))\n",
    "            weight = data.get('weight', 0)\n",
    "            \n",
    "            # Initialize the edge list if it doesn't exist\n",
    "            if edge not in edge_weights:\n",
    "                edge_weights[edge] = [0] * decade_count  # Fill with zeros for previous decades\n",
    "            \n",
    "            # Make sure we're not adding more values than decades\n",
    "            while len(edge_weights[edge]) < decade_count:\n",
    "                edge_weights[edge].append(0)\n",
    "                \n",
    "            edge_weights[edge].append(weight)\n",
    "            decade_edges.add(edge)\n",
    "        \n",
    "        # Fill in zeros for edges that don't appear in this decade\n",
    "        for edge in edge_weights:\n",
    "            if edge not in decade_edges and len(edge_weights[edge]) <= decade_count:\n",
    "                edge_weights[edge].append(0)\n",
    "        \n",
    "        decade_count += 1  # Move to the next decade position\n",
    "    \n",
    "    # Find top edges based on total weight\n",
    "    total_weights = {edge: sum(weights) for edge, weights in edge_weights.items()}\n",
    "    top_edges = sorted(total_weights.items(), key=lambda x: x[1], reverse=True)[:6]\n",
    "    \n",
    "    # Create line plot showing relationship evolution\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    for (u, v), _ in top_edges:\n",
    "        label = f\"{u} & {v}\"\n",
    "        plt.plot(decades, edge_weights[(u, v)], marker='o', linewidth=2, label=label)\n",
    "    \n",
    "    plt.title('Evolution of Concept Relationships Across Decades', fontsize=16)\n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel('Co-appearance Count', fontsize=12)\n",
    "    plt.legend(title='Top Relationships', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(output_dir / 'relationship_evolution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Create a multi-panel visualization showing network density evolution\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Calculate network metrics over time\n",
    "    densities = []\n",
    "    avg_degrees = []\n",
    "    edge_counts = []\n",
    "    active_concepts = []\n",
    "    \n",
    "    for _, G in decade_networks:\n",
    "        densities.append(nx.density(G))\n",
    "        avg_degrees.append(np.mean([deg for _, deg in G.degree()]))\n",
    "        edge_counts.append(G.number_of_edges())\n",
    "        active_concepts.append(sum(1 for n in G.nodes() if G.nodes[n].get('count', 0) > 0))\n",
    "    \n",
    "    # Plot metrics\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(decades, densities, marker='o', color='blue', linewidth=2)\n",
    "    plt.title('Network Density', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(decades, avg_degrees, marker='o', color='green', linewidth=2)\n",
    "    plt.title('Average Degree', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(decades, edge_counts, marker='o', color='red', linewidth=2)\n",
    "    plt.title('Number of Connections', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(decades, active_concepts, marker='o', color='purple', linewidth=2)\n",
    "    plt.title('Active Concepts', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.suptitle('Evolution of Network Properties Across Decades', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(output_dir / 'network_metrics_evolution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Example usage\n",
    "trend_df, co_appearance_df = analyze_concept_trends_by_decade(\n",
    "    df=df,\n",
    "    concept_dict_column=\"concept_dict\",  # Column with concept dictionaries\n",
    "    decade_column=\"decade\",             # Column with decade information\n",
    "    top_n=20,                           # Analyze top 20 concepts\n",
    "    subject_main_dir=\"results\"          # Output directory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf7111-f024-48e4-9896-ec482112b096",
   "metadata": {},
   "source": [
    "# Domains with general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7b0b8-431b-46b1-b9bd-c5e041a52e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_graph = create_network_visualization(\n",
    "    df, \n",
    "    column_name='Domains', \n",
    "    subject_main_dir=subject_main_dir,\n",
    "    top_n=50,  \n",
    "    min_occurrences=5 \n",
    ")\n",
    "print_network_stats(domains_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f3165-3778-4cc0-9df0-f795b279adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_graph = create_network_visualization(\n",
    "    df, \n",
    "    column_name='Domains', \n",
    "    subject_main_dir=subject_main_dir,\n",
    "    min_occurrences=3  # Only include domains that appear at least 3 times\n",
    ")\n",
    "print_network_stats(domains_graph)\n",
    "\n",
    "# Fields visualization\n",
    "fields_graph = create_network_visualization(\n",
    "    df, \n",
    "    column_name='Fields', \n",
    "    subject_main_dir=subject_main_dir,\n",
    "    top_n=30  # Limit to top 30 most frequent fields\n",
    ")\n",
    "print_network_stats(fields_graph)\n",
    "\n",
    "# Topics visualization\n",
    "topics_graph = create_network_visualization(\n",
    "    df, \n",
    "    column_name='Topics', \n",
    "    subject_main_dir=subject_main_dir,\n",
    "    top_n=40,\n",
    "    min_occurrences=3\n",
    ")\n",
    "print_network_stats(topics_graph)\n",
    "\n",
    "# Sub-fields visualization\n",
    "subfields_graph = create_network_visualization(\n",
    "    df, \n",
    "    column_name='Sub-fields', \n",
    "    subject_main_dir=subject_main_dir,\n",
    "    top_n=50\n",
    ")\n",
    "print_network_stats(subfields_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969ae5c-eda0-4366-8d57-d5fdcc8aea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import networkx as nx\n",
    "\n",
    "def analyze_concept_trends(df, concept_dict_column, date_column, \n",
    "                          time_granularity='M', top_n=20, \n",
    "                          subject_main_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    Analyze temporal trends of concept scores and co-appearances over time\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing concept dictionaries and dates\n",
    "    concept_dict_column (str): Column containing dictionaries of concepts and scores\n",
    "    date_column (str): Column containing date information\n",
    "    time_granularity (str): Pandas time frequency for aggregation ('Y'=year, 'M'=month, 'Q'=quarter)\n",
    "    top_n (int): Number of top concepts to analyze\n",
    "    subject_main_dir (str/Path): Directory to save visualizations\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (concept_trends_df, co_appearance_trends_df)\n",
    "    \"\"\"\n",
    "    # Ensure date column is datetime type\n",
    "    df = df.copy()\n",
    "    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "    \n",
    "    # Drop rows with invalid dates\n",
    "    df = df.dropna(subset=[date_column])\n",
    "    \n",
    "    # Step 1: Identify top concepts across the entire dataset\n",
    "    # This keeps the analysis focused on the most important concepts\n",
    "    \n",
    "    # Extract and aggregate all concepts from the dataframe\n",
    "    aggregated_concepts = defaultdict(float)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Get the concept dictionary from this row\n",
    "        concept_dict = row[concept_dict_column]\n",
    "        \n",
    "        # Handle string representation of dictionaries if needed\n",
    "        if isinstance(concept_dict, str):\n",
    "            try:\n",
    "                concept_dict = eval(concept_dict)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Skip if not a dictionary\n",
    "        if not isinstance(concept_dict, dict):\n",
    "            continue\n",
    "            \n",
    "        # Add the scores to our aggregated dictionary\n",
    "        for concept, score in concept_dict.items():\n",
    "            aggregated_concepts[concept] += score\n",
    "    \n",
    "    # Get top N concepts by aggregated score\n",
    "    top_concepts = [concept for concept, _ in \n",
    "                   sorted(aggregated_concepts.items(), \n",
    "                         key=lambda x: x[1], reverse=True)[:top_n]]\n",
    "    \n",
    "    # Step 2: Create time-based grouping for analysis\n",
    "    # Add a period column based on the specified granularity\n",
    "    if time_granularity == 'Y':\n",
    "        df['period'] = df[date_column].dt.to_period('Y')\n",
    "        period_format = '%Y'\n",
    "        x_label = 'Year'\n",
    "    elif time_granularity == 'Q':\n",
    "        df['period'] = df[date_column].dt.to_period('Q')\n",
    "        period_format = '%Y-Q%q'\n",
    "        x_label = 'Quarter'\n",
    "    else:  # Default to monthly\n",
    "        df['period'] = df[date_column].dt.to_period('M')\n",
    "        period_format = '%Y-%m'\n",
    "        x_label = 'Month'\n",
    "    \n",
    "    # Convert to string for easier handling\n",
    "    df['period_str'] = df['period'].astype(str)\n",
    "    \n",
    "    # Get sorted unique periods\n",
    "    all_periods = sorted(df['period'].unique())\n",
    "    period_strs = [str(period) for period in all_periods]\n",
    "    \n",
    "    # Step 3: Analyze concept scores over time\n",
    "    \n",
    "    # Initialize dictionary for storing trend data\n",
    "    concept_trends = {concept: [] for concept in top_concepts}\n",
    "    period_data = []\n",
    "    \n",
    "    # Process each time period\n",
    "    for period in all_periods:\n",
    "        period_df = df[df['period'] == period]\n",
    "        period_str = str(period)\n",
    "        period_data.append(period_str)\n",
    "        \n",
    "        # Initialize scores for this period\n",
    "        period_scores = defaultdict(float)\n",
    "        \n",
    "        # Process each row in this period\n",
    "        for _, row in period_df.iterrows():\n",
    "            concept_dict = row[concept_dict_column]\n",
    "            \n",
    "            # Handle string representation if needed\n",
    "            if isinstance(concept_dict, str):\n",
    "                try:\n",
    "                    concept_dict = eval(concept_dict)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Skip if not a dictionary\n",
    "            if not isinstance(concept_dict, dict):\n",
    "                continue\n",
    "                \n",
    "            # Sum scores for top concepts in this period\n",
    "            for concept, score in concept_dict.items():\n",
    "                if concept in top_concepts:\n",
    "                    period_scores[concept] += score\n",
    "        \n",
    "        # Store scores for each concept in this period\n",
    "        for concept in top_concepts:\n",
    "            concept_trends[concept].append(period_scores[concept])\n",
    "    \n",
    "    # Create dataframe from trend data\n",
    "    trend_df = pd.DataFrame(concept_trends, index=period_data)\n",
    "    \n",
    "    # Step 4: Analyze concept co-appearances over time\n",
    "    \n",
    "    # Initialize dictionary for storing co-appearance data\n",
    "    co_appearance_trends = {}\n",
    "    \n",
    "    # For each period, count co-appearances between top concepts\n",
    "    for period in all_periods:\n",
    "        period_df = df[df['period'] == period]\n",
    "        period_str = str(period)\n",
    "        \n",
    "        # Initialize co-appearance counter for this period\n",
    "        period_co_appearances = defaultdict(int)\n",
    "        \n",
    "        # Process each row in this period\n",
    "        for _, row in period_df.iterrows():\n",
    "            concept_dict = row[concept_dict_column]\n",
    "            \n",
    "            # Handle string representation if needed\n",
    "            if isinstance(concept_dict, str):\n",
    "                try:\n",
    "                    concept_dict = eval(concept_dict)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Skip if not a dictionary\n",
    "            if not isinstance(concept_dict, dict):\n",
    "                continue\n",
    "                \n",
    "            # Get top concepts that appear in this row\n",
    "            row_concepts = [c for c in concept_dict.keys() if c in top_concepts]\n",
    "            \n",
    "            # Count co-appearances for each pair\n",
    "            for i, concept1 in enumerate(row_concepts):\n",
    "                for concept2 in row_concepts[i+1:]:\n",
    "                    # Create a sorted tuple to ensure consistent key ordering\n",
    "                    pair = tuple(sorted([concept1, concept2]))\n",
    "                    period_co_appearances[pair] += 1\n",
    "        \n",
    "        # Store co-appearance counts for this period\n",
    "        for pair, count in period_co_appearances.items():\n",
    "            # Create key for the pair if it doesn't exist\n",
    "            if pair not in co_appearance_trends:\n",
    "                co_appearance_trends[pair] = []\n",
    "            \n",
    "            # Ensure we have data for all preceding periods\n",
    "            while len(co_appearance_trends[pair]) < len(period_data) - 1:\n",
    "                co_appearance_trends[pair].append(0)\n",
    "            \n",
    "            # Add this period's count\n",
    "            co_appearance_trends[pair].append(count)\n",
    "    \n",
    "    # Ensure all pairs have values for all periods\n",
    "    for pair in co_appearance_trends:\n",
    "        while len(co_appearance_trends[pair]) < len(period_data):\n",
    "            co_appearance_trends[pair].append(0)\n",
    "    \n",
    "    # Create dataframe from co-appearance data\n",
    "    # Use concept pairs as column names\n",
    "    pair_columns = {pair: f\"{pair[0]} & {pair[1]}\" for pair in co_appearance_trends.keys()}\n",
    "    co_appearance_df = pd.DataFrame(\n",
    "        {pair_columns[pair]: counts for pair, counts in co_appearance_trends.items()},\n",
    "        index=period_data\n",
    "    )\n",
    "    \n",
    "    # Step 5: Visualize the results\n",
    "    \n",
    "    # 5.1: Visualize concept score trends\n",
    "    visualize_concept_trends(trend_df, subject_main_dir, top_n)\n",
    "    \n",
    "    # 5.2: Visualize co-appearance trends for top pairs\n",
    "    visualize_co_appearance_trends(co_appearance_df, subject_main_dir, top_n)\n",
    "    \n",
    "    # 5.3: Create animated network evolution (if periods > 1)\n",
    "    if len(period_data) > 1:\n",
    "        visualize_network_evolution(df, concept_dict_column, 'period_str', \n",
    "                                  period_data, top_concepts, subject_main_dir)\n",
    "    \n",
    "    return trend_df, co_appearance_df\n",
    "\n",
    "def visualize_concept_trends(trend_df, subject_main_dir, top_n=20):\n",
    "    \"\"\"Visualize trends in concept scores over time\"\"\"\n",
    "    # Create concepts subfolder\n",
    "    concept_dir = Path(subject_main_dir) / \"concepts\"\n",
    "    concept_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Limit to displaying a reasonable number of lines for readability\n",
    "    display_limit = min(10, len(trend_df.columns))\n",
    "    \n",
    "    # Sort concepts by their total score (sum across all periods)\n",
    "    concept_totals = trend_df.sum().sort_values(ascending=False)\n",
    "    top_concepts = concept_totals.index[:display_limit]\n",
    "    \n",
    "    # Prepare multiple visualizations\n",
    "    \n",
    "    # 1. Line plot of concept scores over time for top concepts\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot each concept as a line\n",
    "    for concept in top_concepts:\n",
    "        plt.plot(trend_df.index, trend_df[concept], marker='o', linewidth=2, label=concept)\n",
    "    \n",
    "    plt.title(f'Top {display_limit} Concept Scores Over Time', fontsize=16)\n",
    "    plt.xlabel('Time Period', fontsize=12)\n",
    "    plt.ylabel('Aggregated Score', fontsize=12)\n",
    "    plt.legend(title='Concepts', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(concept_dir / f'concept_trends_line_top{display_limit}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Heatmap of all top concepts\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Normalize data for better visualization\n",
    "    # Use only the top N concepts sorted by total score\n",
    "    heatmap_data = trend_df[concept_totals.index[:top_n]]\n",
    "    \n",
    "    # Create the heatmap\n",
    "    sns.heatmap(heatmap_data.T, cmap='YlOrRd', annot=False, \n",
    "               linewidths=0.5, cbar_kws={'label': 'Score'})\n",
    "    \n",
    "    plt.title(f'Heatmap of Top {top_n} Concept Scores Over Time', fontsize=16)\n",
    "    plt.xlabel('Time Period', fontsize=12)\n",
    "    plt.ylabel('Concept', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(concept_dir / f'concept_trends_heatmap_top{top_n}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Stacked area chart to show relative importance\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Use only the top concepts for clarity\n",
    "    area_data = trend_df[top_concepts]\n",
    "    \n",
    "    # Create stacked area chart\n",
    "    plt.stackplot(area_data.index, [area_data[concept] for concept in top_concepts], \n",
    "                labels=top_concepts, alpha=0.8)\n",
    "    \n",
    "    plt.title(f'Relative Importance of Top {display_limit} Concepts Over Time', fontsize=16)\n",
    "    plt.xlabel('Time Period', fontsize=12)\n",
    "    plt.ylabel('Aggregated Score', fontsize=12)\n",
    "    plt.legend(title='Concepts', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(concept_dir / f'concept_trends_area_top{display_limit}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def visualize_co_appearance_trends(co_appearance_df, subject_main_dir, top_n=20):\n",
    "    \"\"\"Visualize trends in concept co-appearances over time\"\"\"\n",
    "    # Create concepts subfolder\n",
    "    concept_dir = Path(subject_main_dir) / \"concepts\"\n",
    "    concept_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # If no co-appearances, exit\n",
    "    if co_appearance_df.empty:\n",
    "        print(\"No co-appearance data to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Limit to displaying a reasonable number of co-occurrence pairs\n",
    "    display_limit = min(8, len(co_appearance_df.columns))\n",
    "    \n",
    "    # Find top co-appearance pairs\n",
    "    pair_totals = co_appearance_df.sum().sort_values(ascending=False)\n",
    "    top_pairs = pair_totals.index[:display_limit]\n",
    "    \n",
    "    # 1. Line plot of co-appearance trends for top pairs\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot each pair as a line\n",
    "    for pair in top_pairs:\n",
    "        plt.plot(co_appearance_df.index, co_appearance_df[pair], marker='o', \n",
    "                linewidth=2, label=pair)\n",
    "    \n",
    "    plt.title(f'Top {display_limit} Concept Co-appearance Trends', fontsize=16)\n",
    "    plt.xlabel('Time Period', fontsize=12)\n",
    "    plt.ylabel('Co-appearance Count', fontsize=12)\n",
    "    plt.legend(title='Concept Pairs', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(concept_dir / f'coappearance_trends_line_top{display_limit}.png', \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Heatmap of all top co-appearances\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Limit to top N pairs for readability\n",
    "    heatmap_pairs = min(top_n, len(pair_totals))\n",
    "    heatmap_data = co_appearance_df[pair_totals.index[:heatmap_pairs]]\n",
    "    \n",
    "    # Create the heatmap\n",
    "    sns.heatmap(heatmap_data.T, cmap='YlGnBu', annot=False, \n",
    "               linewidths=0.5, cbar_kws={'label': 'Co-appearance Count'})\n",
    "    \n",
    "    plt.title(f'Heatmap of Top {heatmap_pairs} Concept Co-appearances Over Time', fontsize=16)\n",
    "    plt.xlabel('Time Period', fontsize=12)\n",
    "    plt.ylabel('Concept Pair', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(concept_dir / f'coappearance_trends_heatmap_top{heatmap_pairs}.png', \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def visualize_network_evolution(df, concept_dict_column, period_column, \n",
    "                               period_list, top_concepts, subject_main_dir):\n",
    "    \"\"\"Create network visualizations showing concept relationships over time\"\"\"\n",
    "    # Create concepts/networks subfolder\n",
    "    network_dir = Path(subject_main_dir) / \"concepts\" / \"network_evolution\"\n",
    "    network_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # We'll create individual period networks and a summary visualization\n",
    "    period_networks = []\n",
    "    \n",
    "    # Process each time period\n",
    "    for i, period in enumerate(period_list):\n",
    "        # Filter data for this period\n",
    "        period_df = df[df[period_column] == period]\n",
    "        \n",
    "        # Skip if no data for this period\n",
    "        if period_df.empty:\n",
    "            continue\n",
    "            \n",
    "        # Create network for this period\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes (top concepts)\n",
    "        for concept in top_concepts:\n",
    "            G.add_node(concept, count=0, score=0)\n",
    "        \n",
    "        # Initialize co-appearance counter\n",
    "        co_appearances = defaultdict(int)\n",
    "        \n",
    "        # Process each row to extract concept relationships\n",
    "        for _, row in period_df.iterrows():\n",
    "            concept_dict = row[concept_dict_column]\n",
    "            \n",
    "            # Handle string representation if needed\n",
    "            if isinstance(concept_dict, str):\n",
    "                try:\n",
    "                    concept_dict = eval(concept_dict)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Skip if not a dictionary\n",
    "            if not isinstance(concept_dict, dict):\n",
    "                continue\n",
    "                \n",
    "            # Update node data and count co-appearances\n",
    "            row_concepts = []\n",
    "            \n",
    "            for concept, score in concept_dict.items():\n",
    "                if concept in top_concepts:\n",
    "                    # Add to row concepts for co-appearance calculation\n",
    "                    row_concepts.append(concept)\n",
    "                    \n",
    "                    # Update node data\n",
    "                    G.nodes[concept]['count'] = G.nodes[concept].get('count', 0) + 1\n",
    "                    G.nodes[concept]['score'] = G.nodes[concept].get('score', 0) + score\n",
    "            \n",
    "            # Count co-appearances\n",
    "            for i, concept1 in enumerate(row_concepts):\n",
    "                for concept2 in row_concepts[i+1:]:\n",
    "                    pair = tuple(sorted([concept1, concept2]))\n",
    "                    co_appearances[pair] += 1\n",
    "        \n",
    "        # Add edges based on co-appearances\n",
    "        for (concept1, concept2), weight in co_appearances.items():\n",
    "            G.add_edge(concept1, concept2, weight=weight)\n",
    "        \n",
    "        # Store the network\n",
    "        period_networks.append((period, G))\n",
    "        \n",
    "        # Create visualization for this period\n",
    "        visualize_period_network(G, period, network_dir)\n",
    "    \n",
    "    # Create summary visualization comparing networks across time periods\n",
    "    if len(period_networks) > 1:\n",
    "        visualize_network_comparison(period_networks, network_dir)\n",
    "\n",
    "def visualize_period_network(G, period, output_dir):\n",
    "    \"\"\"Visualize network for a specific time period\"\"\"\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    # Calculate node sizes based on scores\n",
    "    node_sizes = []\n",
    "    node_colors = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        score = G.nodes[node].get('score', 0)\n",
    "        count = G.nodes[node].get('count', 0)\n",
    "        \n",
    "        # Size based on score, with a minimum size\n",
    "        size = max(300, score * 30)\n",
    "        node_sizes.append(size)\n",
    "        \n",
    "        # Color intensity based on frequency\n",
    "        # More frequent = darker blue\n",
    "        color_intensity = min(1.0, count / 10)  # Cap at 1.0\n",
    "        node_colors.append((0.1, 0.1, 0.5 + 0.5 * color_intensity))\n",
    "    \n",
    "    # Set positions using spring layout\n",
    "    pos = nx.spring_layout(G, k=0.3, seed=42)\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, \n",
    "                          node_size=node_sizes, \n",
    "                          node_color=node_colors, \n",
    "                          alpha=0.8)\n",
    "    \n",
    "    # Draw edges with weights determining thickness\n",
    "    edges = G.edges()\n",
    "    weights = [G[u][v]['weight'] for u, v in edges]\n",
    "    \n",
    "    if weights:\n",
    "        max_weight = max(weights)\n",
    "        min_weight = min(weights)\n",
    "        weight_range = max_weight - min_weight if max_weight != min_weight else 1\n",
    "        \n",
    "        for (u, v, data) in G.edges(data=True):\n",
    "            weight = data['weight']\n",
    "            # Normalize weight for visual properties\n",
    "            norm_weight = (weight - min_weight) / weight_range if weight_range > 0 else 0.5\n",
    "            # Width between\n",
    "            width = 1 + norm_weight * 4\n",
    "            # Alpha between 0.3 and 0.9\n",
    "            alpha = 0.3 + norm_weight * 0.6\n",
    "            \n",
    "            nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], \n",
    "                                 width=width, \n",
    "                                 alpha=alpha,\n",
    "                                 edge_color='gray')\n",
    "    \n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold',\n",
    "                          bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', \n",
    "                                   boxstyle='round,pad=0.3'))\n",
    "    \n",
    "    plt.title(f'Concept Network for Period: {period}', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(output_dir / f'network_{period}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def visualize_network_comparison(period_networks, output_dir):\n",
    "    \"\"\"Create a visualization comparing networks across time\"\"\"\n",
    "    # We'll focus on how specific metrics change over time\n",
    "    \n",
    "    # 1. Track changes in node importance (score) over time\n",
    "    # Extract data\n",
    "    periods = []\n",
    "    concept_scores = defaultdict(list)\n",
    "    concept_counts = defaultdict(list)\n",
    "    \n",
    "    for period, G in period_networks:\n",
    "        periods.append(period)\n",
    "        \n",
    "        for node in G.nodes():\n",
    "            score = G.nodes[node].get('score', 0)\n",
    "            count = G.nodes[node].get('count', 0)\n",
    "            \n",
    "            concept_scores[node].append(score)\n",
    "            concept_counts[node].append(count)\n",
    "    \n",
    "    # Find top concepts based on total score\n",
    "    total_scores = {concept: sum(scores) for concept, scores in concept_scores.items()}\n",
    "    top_concepts = sorted(total_scores.items(), key=lambda x: x[1], reverse=True)[:8]\n",
    "    \n",
    "    # Create line plot showing score changes\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    for concept, _ in top_concepts:\n",
    "        plt.plot(periods, concept_scores[concept], marker='o', linewidth=2, label=concept)\n",
    "    \n",
    "    plt.title('Evolution of Concept Scores Over Time', fontsize=16)\n",
    "    plt.xlabel('Time Period', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.legend(title='Top Concepts', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(output_dir / 'concept_score_evolution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Track changes in relationship strength over time\n",
    "    # Extract edge data\n",
    "    edge_weights = defaultdict(list)\n",
    "    \n",
    "    for period, G in period_networks:\n",
    "        # Track existing edges for this period\n",
    "        period_edges = set()\n",
    "        \n",
    "        for u, v, data in G.edges(data=True):\n",
    "            edge = tuple(sorted([u, v]))\n",
    "            weight = data.get('weight', 0)\n",
    "            \n",
    "            edge_weights[edge].append(weight)\n",
    "            period_edges.add(edge)\n",
    "        \n",
    "        # Ensure all edges have a value for this period (0 if not present)\n",
    "        for edge in edge_weights:\n",
    "            if edge not in period_edges:\n",
    "                # This edge wasn't in the current period\n",
    "                while len(edge_weights[edge]) < len(periods):\n",
    "                    edge_weights[edge].append(0)\n",
    "    \n",
    "    # Find top edges based on total weight\n",
    "    total_weights = {edge: sum(weights) for edge, weights in edge_weights.items()}\n",
    "    top_edges = sorted(total_weights.items(), key=lambda x: x[1], reverse=True)[:6]\n",
    "    \n",
    "    # Create line plot showing relationship evolution\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    for (u, v), _ in top_edges:\n",
    "        label = f\"{u} & {v}\"\n",
    "        plt.plot(periods, edge_weights[(u, v)], marker='o', linewidth=2, label=label)\n",
    "    \n",
    "    plt.title('Evolution of Concept Relationships Over Time', fontsize=16)\n",
    "    plt.xlabel('Time Period', fontsize=12)\n",
    "    plt.ylabel('Co-appearance Count', fontsize=12)\n",
    "    plt.legend(title='Top Relationships', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(output_dir / 'relationship_evolution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Create a multi-panel visualization showing network density evolution\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Calculate network metrics over time\n",
    "    densities = []\n",
    "    avg_degrees = []\n",
    "    edge_counts = []\n",
    "    active_concepts = []\n",
    "    \n",
    "    for _, G in period_networks:\n",
    "        densities.append(nx.density(G))\n",
    "        avg_degrees.append(np.mean([deg for _, deg in G.degree()]))\n",
    "        edge_counts.append(G.number_of_edges())\n",
    "        active_concepts.append(sum(1 for n in G.nodes() if G.nodes[n].get('count', 0) > 0))\n",
    "    \n",
    "    # Plot metrics\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(periods, densities, marker='o', color='blue', linewidth=2)\n",
    "    plt.title('Network Density', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(periods, avg_degrees, marker='o', color='green', linewidth=2)\n",
    "    plt.title('Average Degree', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(periods, edge_counts, marker='o', color='red', linewidth=2)\n",
    "    plt.title('Number of Connections', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(periods, active_concepts, marker='o', color='purple', linewidth=2)\n",
    "    plt.title('Active Concepts', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.suptitle('Evolution of Network Properties Over Time', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(output_dir / 'network_metrics_evolution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Example usage\n",
    "'''\n",
    "# Assuming your dataframe has:\n",
    "# - A column with concept dictionaries\n",
    "# - A date column\n",
    "\n",
    "# Analyze temporal trends\n",
    "trend_df, co_appearance_df = analyze_concept_trends(\n",
    "    df=your_dataframe,\n",
    "    concept_dict_column=\"ConceptDict\",  # Column with concept dictionaries\n",
    "    date_column=\"Date\",                 # Column with date information\n",
    "    time_granularity='M',               # Monthly analysis ('Y' for yearly, 'Q' for quarterly)\n",
    "    top_n=20,                           # Analyze top 20 concepts\n",
    "    subject_main_dir=\"results\"          # Output directory\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77734dc-4601-40a7-a9c9-90a10631e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DOMAINS COMMUNITY ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "domain_communities, domain_cliques, domain_comm_stats = analyze_network_communities(\n",
    "    domains_graph, \n",
    "    subject_main_dir, \n",
    "    \"domains\", \n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Visualize domain community statistics\n",
    "visualize_community_stats(domain_comm_stats, subject_main_dir, \"domains\")\n",
    "\n",
    "# Find domain bridge nodes\n",
    "domain_bridges = find_community_bridges(domains_graph, domain_communities, subject_main_dir, \"domains\")\n",
    "\n",
    "# Fields community analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FIELDS COMMUNITY ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "field_communities, field_cliques, field_comm_stats = analyze_network_communities(\n",
    "    fields_graph, \n",
    "    subject_main_dir, \n",
    "    \"fields\", \n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Visualize field community statistics\n",
    "visualize_community_stats(field_comm_stats, subject_main_dir, \"fields\")\n",
    "\n",
    "# Find field bridge nodes\n",
    "field_bridges = find_community_bridges(fields_graph, field_communities, subject_main_dir, \"fields\")\n",
    "\n",
    "# Print a summary of findings across all domains\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY OF COMMUNITY ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Concepts: {len(set(concept_communities.values()))} communities, {len(concept_cliques)} significant cliques\")\n",
    "print(f\"Domains: {len(set(domain_communities.values()))} communities, {len(domain_cliques)} significant cliques\")\n",
    "print(f\"Fields: {len(set(field_communities.values()))} communities, {len(field_cliques)} significant cliques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b0d562-8475-49d2-bbf5-0386c57ae01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEMPORAL ANALYSIS OF DOMAINS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "domain_temporal_results = analyze_temporal_networks(\n",
    "    df,\n",
    "    column_name='Domains',\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    decade_col='decade',\n",
    "    min_occurrence=2,  # Domains might be fewer, so lower threshold\n",
    "    top_n=None,        # Include all domains that meet min_occurrence\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Print some insights from the temporal analysis\n",
    "print(f\"\\nFound {len(domain_temporal_results['persistent_items'])} domains that persisted across multiple decades\")\n",
    "print(f\"Found {len(domain_temporal_results['persistent_connections'])} persistent domain connections\")\n",
    "\n",
    "# Temporal analysis of Fields\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEMPORAL ANALYSIS OF FIELDS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "field_temporal_results = analyze_temporal_networks(\n",
    "    df,\n",
    "    column_name='Fields',\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    decade_col='decade',\n",
    "    min_occurrence=2,\n",
    "    top_n=30\n",
    ")\n",
    "\n",
    "# Temporal analysis of Sub-fields\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEMPORAL ANALYSIS OF SUB-FIELDS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "subfield_temporal_results = analyze_temporal_networks(\n",
    "    df,\n",
    "    column_name='Sub-fields',\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    decade_col='decade',\n",
    "    min_occurrence=2,\n",
    "    top_n=40\n",
    ")\n",
    "\n",
    "# Temporal analysis of Topics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEMPORAL ANALYSIS OF TOPICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "topic_temporal_results = analyze_temporal_networks(\n",
    "    df,\n",
    "    column_name='Topics',\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    decade_col='decade',\n",
    "    min_occurrence=2,\n",
    "    top_n=40\n",
    ")\n",
    "\n",
    "# Print a comparative summary of temporal evolution across all subjects\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARATIVE SUMMARY OF TEMPORAL EVOLUTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "decades = df['decade'].unique()\n",
    "decades.sort()\n",
    "\n",
    "# Create a summary table structure for comparison\n",
    "summary_data = []\n",
    "\n",
    "for decade in decades:\n",
    "    row = {'Decade': f\"{decade}s\"}\n",
    "    \n",
    "    # Add concept stats if available\n",
    "    concept_idx = list(concept_temporal_results['metrics']['decades']).index(decade) if decade in concept_temporal_results['metrics']['decades'] else None\n",
    "    if concept_idx is not None and concept_idx < len(concept_temporal_results['metrics']['n_nodes']):\n",
    "        row['Concepts'] = concept_temporal_results['metrics']['n_nodes'][concept_idx]\n",
    "        row['Concept Communities'] = len(set(concept_temporal_results['metrics']['communities'][concept_idx].values())) if concept_temporal_results['metrics']['communities'][concept_idx] else 0\n",
    "    else:\n",
    "        row['Concepts'] = 'N/A'\n",
    "        row['Concept Communities'] = 'N/A'\n",
    "    \n",
    "    # Add domain stats if available\n",
    "    domain_idx = list(domain_temporal_results['metrics']['decades']).index(decade) if decade in domain_temporal_results['metrics']['decades'] else None\n",
    "    if domain_idx is not None and domain_idx < len(domain_temporal_results['metrics']['n_nodes']):\n",
    "        row['Domains'] = domain_temporal_results['metrics']['n_nodes'][domain_idx]\n",
    "        row['Domain Communities'] = len(set(domain_temporal_results['metrics']['communities'][domain_idx].values())) if domain_temporal_results['metrics']['communities'][domain_idx] else 0\n",
    "    else:\n",
    "        row['Domains'] = 'N/A'\n",
    "        row['Domain Communities'] = 'N/A'\n",
    "    \n",
    "    # Add field stats if available\n",
    "    field_idx = list(field_temporal_results['metrics']['decades']).index(decade) if decade in field_temporal_results['metrics']['decades'] else None\n",
    "    if field_idx is not None and field_idx < len(field_temporal_results['metrics']['n_nodes']):\n",
    "        row['Fields'] = field_temporal_results['metrics']['n_nodes'][field_idx]\n",
    "        row['Field Communities'] = len(set(field_temporal_results['metrics']['communities'][field_idx].values())) if field_temporal_results['metrics']['communities'][field_idx] else 0\n",
    "    else:\n",
    "        row['Fields'] = 'N/A'\n",
    "        row['Field Communities'] = 'N/A'\n",
    "    \n",
    "    summary_data.append(row)\n",
    "\n",
    "# Print the summary table\n",
    "print(\"\\nTemporal Evolution Summary by Decade:\")\n",
    "for row in summary_data:\n",
    "    print(f\"\\n{row['Decade']}:\")\n",
    "    print(f\"  Concepts: {row['Concepts']} (Communities: {row['Concept Communities']})\")\n",
    "    print(f\"  Domains: {row['Domains']} (Communities: {row['Domain Communities']})\")\n",
    "    print(f\"  Fields: {row['Fields']} (Communities: {row['Field Communities']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17535929-4c28-40a2-84bd-4410dd4cfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for analyzing the Authors field using the three main functions:\n",
    "# 1. create_network_visualization\n",
    "# 2. analyze_network_communities \n",
    "# 3. analyze_temporal_networks\n",
    "\n",
    "# ==========================================\n",
    "# 1. NETWORK VISUALIZATION FOR AUTHORS\n",
    "# ==========================================\n",
    "\n",
    "# Create co-authorship network visualization\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CO-AUTHORSHIP NETWORK VISUALIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Parameters tailored for author networks\n",
    "authors_graph = create_network_visualization(\n",
    "    df,\n",
    "    column_name='Authors',\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    # Increase min_occurrences to filter out authors with few papers\n",
    "    min_occurrences=3,\n",
    "    # Limit to top prolific authors for cleaner visualization\n",
    "    top_n=100\n",
    ")\n",
    "\n",
    "print(f\"Created co-authorship network with {authors_graph.number_of_nodes()} authors \" \n",
    "      f\"and {authors_graph.number_of_edges()} collaborations\")\n",
    "\n",
    "# Print basic network stats\n",
    "print_network_stats(authors_graph)\n",
    "\n",
    "# ==========================================\n",
    "# 2. COMMUNITY ANALYSIS FOR AUTHORS\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CO-AUTHORSHIP COMMUNITY ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze communities in the co-authorship network\n",
    "authors_communities, authors_cliques, authors_comm_stats = analyze_network_communities(\n",
    "    authors_graph,\n",
    "    subject_main_dir,\n",
    "    \"authors\",\n",
    "    # Cliques of 3+ authors represent close collaboration groups\n",
    "    min_clique_size=3,\n",
    "    # Show more top communities as author networks often have many small groups\n",
    "    top_communities=8,\n",
    "    # Show more top cliques as these represent complete collaboration groups\n",
    "    top_cliques=8\n",
    ")\n",
    "\n",
    "# Visualize author community statistics\n",
    "visualize_community_stats(authors_comm_stats, subject_main_dir, \"authors\")\n",
    "\n",
    "# Find and visualize bridge authors (who connect different communities)\n",
    "authors_bridges = find_community_bridges(\n",
    "    authors_graph, \n",
    "    authors_communities, \n",
    "    subject_main_dir, \n",
    "    \"authors\",\n",
    "    # Show more bridge authors as they're particularly important in research networks\n",
    "    top_n=15\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 3. TEMPORAL ANALYSIS FOR AUTHORS\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEMPORAL CO-AUTHORSHIP ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze how co-authorship networks evolve over time\n",
    "authors_temporal_results = analyze_temporal_networks(\n",
    "    df,\n",
    "    column_name='Authors',\n",
    "    subject_main_dir=subject_main_dir,\n",
    "    decade_col='decade',\n",
    "    # Lower minimum occurrence for authors as each decade may have fewer papers\n",
    "    min_occurrence=2,\n",
    "    # Limit to top authors per decade for cleaner visualization\n",
    "    top_n=75,\n",
    "    # Smaller cliques might be meaningful in author networks\n",
    "    min_clique_size=3\n",
    ")\n",
    "\n",
    "# Print some insights from the temporal analysis\n",
    "print(f\"\\nFound {len(authors_temporal_results['persistent_items'])} authors who published across multiple decades\")\n",
    "print(f\"Found {len(authors_temporal_results['persistent_connections'])} persistent co-authorship relationships\")\n",
    "\n",
    "# Get the most persistent authors (present in most decades)\n",
    "most_persistent_authors = sorted(\n",
    "    [(item, len(decades)) for item, decades in authors_temporal_results['persistent_items'].items()],\n",
    "    key=lambda x: x[1], \n",
    "    reverse=True\n",
    ")[:15]\n",
    "\n",
    "print(\"\\nMost persistent authors across decades:\")\n",
    "for author, num_decades in most_persistent_authors:\n",
    "    print(f\"  {author}: published in {num_decades} decades\")\n",
    "\n",
    "# Get the most persistent co-authorship connections\n",
    "most_persistent_collaborations = sorted(\n",
    "    [((author1, author2), count) for (author1, author2), count in authors_temporal_results['persistent_connections'].items()],\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:10]\n",
    "\n",
    "print(\"\\nMost persistent co-authorship relationships:\")\n",
    "for (author1, author2), decade_count in most_persistent_collaborations:\n",
    "    print(f\"  {author1} & {author2}: collaborated across {decade_count} decades\")\n",
    "\n",
    "# ==========================================\n",
    "# CUSTOM ANALYSIS SPECIFIC TO AUTHORS\n",
    "# ==========================================\n",
    "\n",
    "# Calculate and visualize author productivity over time\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AUTHOR PRODUCTIVITY ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Process data for productivity analysis\n",
    "def analyze_author_productivity(df, subject_main_dir):\n",
    "    # Get author counts by decade\n",
    "    author_publication_counts = {}\n",
    "    decades = sorted(df['decade'].unique())\n",
    "    \n",
    "    for decade in decades:\n",
    "        decade_df = df[df['decade'] == decade]\n",
    "        decade_authors = []\n",
    "        \n",
    "        # Collect all authors from this decade\n",
    "        for authors_list in preprocess_column(decade_df, 'Authors'):\n",
    "            decade_authors.extend(authors_list)\n",
    "        \n",
    "        # Count publications per author\n",
    "        author_counts = Counter(decade_authors)\n",
    "        author_publication_counts[decade] = author_counts\n",
    "    \n",
    "    # Find top authors across all decades\n",
    "    all_authors = set()\n",
    "    for decade_counts in author_publication_counts.values():\n",
    "        all_authors.update(decade_counts.keys())\n",
    "    \n",
    "    # Calculate total publications per author\n",
    "    total_pub_counts = Counter()\n",
    "    for decade_counts in author_publication_counts.values():\n",
    "        total_pub_counts.update(decade_counts)\n",
    "    \n",
    "    # Get top 20 most productive authors overall\n",
    "    top_authors = [author for author, _ in total_pub_counts.most_common(20)]\n",
    "    \n",
    "    # Create visualization of publication trends for top authors\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for author in top_authors[:10]:  # Limit to top 10 for clarity in the chart\n",
    "        pub_counts = [author_publication_counts[decade].get(author, 0) for decade in decades]\n",
    "        plt.plot(decades, pub_counts, 'o-', linewidth=2, label=author)\n",
    "    \n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel('Number of Publications', fontsize=12)\n",
    "    plt.title('Publication Trends of Top 10 Most Productive Authors', fontsize=16)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, \"author_productivity_trends.png\", subject_main_dir, \"authors\")\n",
    "    \n",
    "    # Create heatmap of top author productivity by decade\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Prepare heatmap data\n",
    "    heatmap_data = []\n",
    "    for author in top_authors:\n",
    "        row = [author_publication_counts[decade].get(author, 0) for decade in decades]\n",
    "        heatmap_data.append(row)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt=\"d\", \n",
    "               xticklabels=[f\"{d}s\" for d in decades],\n",
    "               yticklabels=top_authors,\n",
    "               cmap=\"YlGnBu\")\n",
    "    \n",
    "    plt.xlabel('Decade', fontsize=12)\n",
    "    plt.ylabel('Author', fontsize=12)\n",
    "    plt.title('Publication Count Heatmap of Top 20 Most Productive Authors', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, \"author_productivity_heatmap.png\", subject_main_dir, \"authors\")\n",
    "    \n",
    "    return {\n",
    "        'author_counts_by_decade': author_publication_counts,\n",
    "        'top_authors': top_authors,\n",
    "        'total_publication_counts': total_pub_counts\n",
    "    }\n",
    "\n",
    "# Run the custom author productivity analysis\n",
    "productivity_results = analyze_author_productivity(df, subject_main_dir)\n",
    "print(f\"Analyzed productivity for {len(productivity_results['total_publication_counts'])} unique authors\")\n",
    "print(f\"Top 5 most productive authors:\")\n",
    "for author, count in productivity_results['total_publication_counts'].most_common(5):\n",
    "    print(f\"  {author}: {count} publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267674d0-7f05-44ce-80f1-25817d8f5e37",
   "metadata": {},
   "source": [
    "# Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a175fbd-1580-4951-8581-aac99712c28c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kaleido  # Required for plotly.write_image\n",
    "\n",
    "# Utility functions for country network analysis\n",
    "def create_country_pairs(countries_list):\n",
    "    \"\"\"\n",
    "    Create all possible pairs of countries from a list of countries\n",
    "    \n",
    "    Parameters:\n",
    "    countries_list (list): List of country codes\n",
    "    \n",
    "    Returns:\n",
    "    list: List of country pairs (tuples)\n",
    "    \"\"\"\n",
    "    if not countries_list or len(countries_list) < 2:\n",
    "        return []\n",
    "    \n",
    "    # Create all possible pairs\n",
    "    return list(combinations(sorted(countries_list), 2))\n",
    "\n",
    "def count_country_pairs(df, country_col='Countries_flat'):\n",
    "    \"\"\"\n",
    "    Count co-appearances of countries\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame with country lists\n",
    "    country_col (str): Name of column containing country lists\n",
    "    \n",
    "    Returns:\n",
    "    Counter: Dictionary of country pairs and their counts\n",
    "    \"\"\"\n",
    "    pair_counts = Counter()\n",
    "    \n",
    "    # Process each row\n",
    "    for countries in df[country_col]:\n",
    "        if isinstance(countries, list) and len(countries) >= 2:\n",
    "            pairs = create_country_pairs(countries)\n",
    "            pair_counts.update(pairs)\n",
    "    \n",
    "    return pair_counts\n",
    "\n",
    "def get_country_centroids(world_gdf, iso_cols=['iso_a2', 'ISO_A2', 'ISO_A2_EH']):\n",
    "    \"\"\"\n",
    "    Get centroid coordinates for each country\n",
    "    \n",
    "    Parameters:\n",
    "    world_gdf (GeoDataFrame): World map GeoDataFrame\n",
    "    iso_cols (list): Possible column names for ISO country codes\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary mapping country codes to (longitude, latitude)\n",
    "    \"\"\"\n",
    "    country_positions = {}\n",
    "    \n",
    "    # Determine which ISO column exists in the dataframe\n",
    "    available_cols = [col for col in iso_cols if col in world_gdf.columns]\n",
    "    \n",
    "    if not available_cols:\n",
    "        print(f\"Warning: No ISO country code columns found in the dataframe. Available columns: {world_gdf.columns.tolist()}\")\n",
    "        # Try to use a different identifier as fallback\n",
    "        if 'ADMIN' in world_gdf.columns:\n",
    "            print(\"Using 'ADMIN' column as fallback. Note: This might not match your country codes.\")\n",
    "            iso_col = 'ADMIN'\n",
    "        elif 'NAME' in world_gdf.columns:\n",
    "            print(\"Using 'NAME' column as fallback. Note: This might not match your country codes.\")\n",
    "            iso_col = 'NAME'\n",
    "        else:\n",
    "            print(\"No suitable country identifier columns found.\")\n",
    "            return country_positions\n",
    "    else:\n",
    "        iso_col = available_cols[0]\n",
    "        print(f\"Using '{iso_col}' for country codes.\")\n",
    "    \n",
    "    # Print first few values to help with debugging\n",
    "    print(f\"Sample values from '{iso_col}' column: {world_gdf[iso_col].head().tolist()}\")\n",
    "    \n",
    "    for idx, row in world_gdf.iterrows():\n",
    "        if row[iso_col] not in ['-99', '-1', None, '']:  # Skip invalid codes\n",
    "            centroid = row.geometry.representative_point()\n",
    "            country_positions[row[iso_col]] = (centroid.y, centroid.x)  # lat, lon for folium\n",
    "    \n",
    "    return country_positions\n",
    "\n",
    "def get_top_countries(df, country_col='Countries_flat', top_n=20):\n",
    "    \"\"\"\n",
    "    Get top countries by appearance count\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame with country lists\n",
    "    country_col (str): Name of column containing country lists\n",
    "    top_n (int): Number of top countries to return\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary of country codes and their counts\n",
    "    \"\"\"\n",
    "    country_counts = Counter()\n",
    "    \n",
    "    for countries in df[country_col]:\n",
    "        if isinstance(countries, list):\n",
    "            country_counts.update(countries)\n",
    "    \n",
    "    return dict(country_counts.most_common(top_n))\n",
    "\n",
    "def analyze_country_coappearances_by_decade(df, country_col='Countries_flat', decade_col='decade'):\n",
    "    \"\"\"\n",
    "    Analyze country co-appearances by decade\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame with country lists and decade information\n",
    "    country_col (str): Name of column containing country lists\n",
    "    decade_col (str): Name of column containing decade information\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary of decades with country pair counts\n",
    "    \"\"\"\n",
    "    decades = df[decade_col].unique()\n",
    "    decade_pairs = {}\n",
    "    \n",
    "    for decade in decades:\n",
    "        decade_df = df[df[decade_col] == decade]\n",
    "        if not decade_df.empty:\n",
    "            decade_pairs[decade] = count_country_pairs(decade_df, country_col)\n",
    "    \n",
    "    return decade_pairs\n",
    "\n",
    "def plot_country_coappearance_network(df_flattened, subject_main_dir, min_weight=2):\n",
    "    \"\"\"\n",
    "    Create and save visualizations of country co-appearances\n",
    "    \n",
    "    Parameters:\n",
    "    df_flattened (DataFrame): DataFrame with flattened country lists\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    min_weight (int): Minimum weight to include in visualizations\n",
    "    \"\"\"\n",
    "    # Load world map data\n",
    "    # Handle the deprecated dataset by downloading directly\n",
    "    try:\n",
    "        # Try the old method first for backward compatibility\n",
    "        world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    except (AttributeError, ModuleNotFoundError):\n",
    "        # If that fails, download the data directly\n",
    "        world_url = \"https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\"\n",
    "        world = gpd.read_file(world_url)\n",
    "        \n",
    "    # Print available columns for debugging\n",
    "    print(f\"Available columns in world dataframe: {world.columns.tolist()}\")\n",
    "    \n",
    "    # Get country centroids\n",
    "    country_centroids = get_country_centroids(world)\n",
    "    \n",
    "    # Try to build a mapping of country codes to full names\n",
    "    country_names = {}\n",
    "    name_cols = ['NAME', 'ADMIN', 'name', 'admin', 'NAME_EN', 'name_en']\n",
    "    code_cols = ['ISO_A2', 'ISO_A3', 'iso_a2', 'iso_a3']\n",
    "    \n",
    "    # Find valid name and code columns\n",
    "    valid_name_col = next((col for col in name_cols if col in world.columns), None)\n",
    "    valid_code_col = next((col for col in code_cols if col in world.columns), None)\n",
    "    \n",
    "    if valid_name_col and valid_code_col:\n",
    "        for idx, row in world.iterrows():\n",
    "            if row[valid_code_col] not in ['-99', '-1', None, '']:\n",
    "                country_names[row[valid_code_col]] = row[valid_name_col]\n",
    "    \n",
    "    # Count country pairs\n",
    "    pair_counts = count_country_pairs(df_flattened)\n",
    "    \n",
    "    # Get top countries\n",
    "    top_countries = get_top_countries(df_flattened)\n",
    "    \n",
    "    # Create interactive folium map\n",
    "    map_center = [20, 0]  # Center of the map\n",
    "    m = folium.Map(location=map_center, zoom_start=2, tiles='CartoDB positron')\n",
    "    \n",
    "    # Add country markers\n",
    "    for country, count in top_countries.items():\n",
    "        if country in country_centroids:\n",
    "            lat, lon = country_centroids[country]\n",
    "            \n",
    "            # Get full country name if available\n",
    "            country_label = country_names.get(country, country)\n",
    "            \n",
    "            folium.CircleMarker(\n",
    "                location=[lat, lon],\n",
    "                radius=min(20, max(5, np.log1p(count))),  # Scale marker size by log of count\n",
    "                color='blue',\n",
    "                fill=True,\n",
    "                fill_color='blue',\n",
    "                fill_opacity=0.6,\n",
    "                popup=f\"{country_label}: {count} appearances\"\n",
    "            ).add_to(m)\n",
    "            \n",
    "            # Add country label\n",
    "            folium.Marker(\n",
    "                location=[lat, lon],\n",
    "                icon=folium.DivIcon(\n",
    "                    icon_size=(150, 36),\n",
    "                    icon_anchor=(75, 18),\n",
    "                    html=f'<div style=\"font-size: 10pt; color: black; font-weight: bold; text-align: center;\">{country_label}</div>'\n",
    "                )\n",
    "            ).add_to(m)\n",
    "    \n",
    "    # Add connections between countries\n",
    "    for (country1, country2), weight in pair_counts.items():\n",
    "        if weight >= min_weight and country1 in country_centroids and country2 in country_centroids:\n",
    "            lat1, lon1 = country_centroids[country1]\n",
    "            lat2, lon2 = country_centroids[country2]\n",
    "            \n",
    "            # Scale line weight\n",
    "            line_weight = min(10, max(1, np.log1p(weight) / 2))\n",
    "            \n",
    "            # Create line with lighter color (light blue)\n",
    "            folium.PolyLine(\n",
    "                locations=[[lat1, lon1], [lat2, lon2]],\n",
    "                weight=line_weight,\n",
    "                color='#ADD8E6',  # Light blue\n",
    "                opacity=min(0.8, max(0.2, weight / 100)),\n",
    "                popup=f\"{country1}-{country2}: {weight} co-appearances\"\n",
    "            ).add_to(m)\n",
    "    \n",
    "    # Save interactive map\n",
    "    map_path = Path(subject_main_dir) / 'countries' / 'country_coappearance_network.html'\n",
    "    map_dir = map_path.parent\n",
    "    map_dir.mkdir(parents=True, exist_ok=True)\n",
    "    m.save(str(map_path))\n",
    "    \n",
    "    # Analyze co-appearances by decade\n",
    "    decade_pairs = analyze_country_coappearances_by_decade(df_flattened)\n",
    "    \n",
    "    # Create visualizations for decade analysis\n",
    "    plot_decade_analysis(decade_pairs, top_countries, subject_main_dir)\n",
    "    \n",
    "    # Create plotly express choropleth map\n",
    "    create_plotly_choropleth(df_flattened, world, subject_main_dir)\n",
    "\n",
    "def plot_decade_analysis(decade_pairs, top_countries, subject_main_dir):\n",
    "    \"\"\"\n",
    "    Create and save visualizations for decade analysis\n",
    "    \n",
    "    Parameters:\n",
    "    decade_pairs (dict): Dictionary of decades with country pair counts\n",
    "    top_countries (dict): Dictionary of top countries\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    \"\"\"\n",
    "    # Prepare data for top co-appearances over decades\n",
    "    decades = sorted(decade_pairs.keys())\n",
    "    top_country_codes = list(top_countries.keys())[:10]  # Top 10 countries\n",
    "    \n",
    "    # Track persistent pairs\n",
    "    persistent_pairs = {}\n",
    "    for decade, pairs in decade_pairs.items():\n",
    "        for pair, count in pairs.most_common(50):  # Consider top 50 pairs\n",
    "            if pair not in persistent_pairs:\n",
    "                persistent_pairs[pair] = []\n",
    "            persistent_pairs[pair].append((decade, count))\n",
    "    \n",
    "    # Filter for pairs that appear in multiple decades\n",
    "    multi_decade_pairs = {pair: decades for pair, decades in persistent_pairs.items() \n",
    "                        if len(decades) > 1}\n",
    "    \n",
    "    # Create heatmap of co-appearances over decades\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    heatmap_data = []\n",
    "    pair_labels = []\n",
    "    \n",
    "    # Take top 20 most persistent pairs\n",
    "    sorted_persistent = sorted(multi_decade_pairs.items(), \n",
    "                             key=lambda x: len(x[1]), reverse=True)[:20]\n",
    "    \n",
    "    for pair, decade_counts in sorted_persistent:\n",
    "        row = []\n",
    "        pair_labels.append(f\"{pair[0]}-{pair[1]}\")\n",
    "        \n",
    "        # Create row with counts for each decade\n",
    "        decade_dict = {d: c for d, c in decade_counts}\n",
    "        for decade in decades:\n",
    "            row.append(decade_dict.get(decade, 0))\n",
    "        \n",
    "        heatmap_data.append(row)\n",
    "    \n",
    "    # Create and save heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(heatmap_data, cmap=\"YlOrRd\", annot=True, fmt=\"d\", \n",
    "                xticklabels=decades, yticklabels=pair_labels)\n",
    "    plt.title(\"Persistent Country Co-appearances by Decade\")\n",
    "    plt.xlabel(\"Decade\")\n",
    "    plt.ylabel(\"Country Pair\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, \"persistent_coappearances_by_decade.png\", subject_main_dir, \"countries\")\n",
    "    \n",
    "    # Create bar chart of top countries by decade\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Count country appearances by decade\n",
    "    decade_country_counts = {}\n",
    "    for decade in decades:\n",
    "        decade_df = df_flattened[df_flattened['decade'] == decade]\n",
    "        decade_country_counts[decade] = get_top_countries(decade_df, top_n=10)\n",
    "    \n",
    "    # Plot top 5 countries for each decade\n",
    "    for i, decade in enumerate(decades):\n",
    "        counts = decade_country_counts[decade]\n",
    "        countries = list(counts.keys())[:5]  # Top 5\n",
    "        values = [counts[c] for c in countries]\n",
    "        \n",
    "        plt.subplot(1, len(decades), i+1)\n",
    "        plt.barh(countries, values, color='skyblue')\n",
    "        plt.title(f\"Decade: {decade}\")\n",
    "        plt.xlabel(\"Count\")\n",
    "        if i == 0:\n",
    "            plt.ylabel(\"Country\")\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, \"top_countries_by_decade.png\", subject_main_dir, \"countries\")\n",
    "\n",
    "def create_plotly_choropleth(df_flattened, world, subject_main_dir):\n",
    "    \"\"\"\n",
    "    Create a Plotly Express choropleth map for country appearances\n",
    "    \n",
    "    Parameters:\n",
    "    df_flattened (DataFrame): DataFrame with flattened country lists\n",
    "    world (GeoDataFrame): World map GeoDataFrame\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    \"\"\"\n",
    "    # Count country appearances\n",
    "    country_counts = get_top_countries(df_flattened, top_n=None)  # Get all countries\n",
    "    \n",
    "    # Prepare data for plotly\n",
    "    choropleth_data = []\n",
    "    for country, count in country_counts.items():\n",
    "        choropleth_data.append({\n",
    "            'country_code': country,\n",
    "            'count': count\n",
    "        })\n",
    "    \n",
    "    choropleth_df = pd.DataFrame(choropleth_data)\n",
    "    \n",
    "    # Check if we need to convert from ISO-2 to ISO-3\n",
    "    # Plotly only supports ISO-3, country names, USA-states, or geojson-id\n",
    "    locationmode = 'ISO-3'\n",
    "    \n",
    "    # Create choropleth map\n",
    "    fig = px.choropleth(\n",
    "        choropleth_df,\n",
    "        locations='country_code',\n",
    "        color='count',\n",
    "        hover_name='country_code',\n",
    "        color_continuous_scale=px.colors.sequential.Plasma,\n",
    "        title='Global Country Appearances Count',\n",
    "        locationmode=locationmode,\n",
    "        projection='natural earth'\n",
    "    )\n",
    "    \n",
    "    # Improve layout\n",
    "    fig.update_layout(\n",
    "        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
    "        coloraxis_colorbar=dict(\n",
    "            title=\"Appearances\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save as HTML\n",
    "    plotly_path = Path(subject_main_dir) / 'countries' / 'country_appearances_choropleth.html'\n",
    "    plotly_dir = plotly_path.parent\n",
    "    plotly_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fig.write_html(str(plotly_path))\n",
    "    \n",
    "    # Save as image (PNG)\n",
    "    img_path = plotly_path.with_suffix('.png')\n",
    "    fig.write_image(str(img_path), width=1200, height=800)\n",
    "    \n",
    "    print(f\"Saved choropleth visualization to {plotly_path} and {img_path}\")\n",
    "    \n",
    "    # Improve layout\n",
    "    fig.update_layout(\n",
    "        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
    "        coloraxis_colorbar=dict(\n",
    "            title=\"Appearances\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save as HTML\n",
    "    plotly_path = Path(subject_main_dir) / 'countries' / 'country_appearances_choropleth.html'\n",
    "    plotly_dir = plotly_path.parent\n",
    "    plotly_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fig.write_html(str(plotly_path))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming df_flattened is already loaded with columns:\n",
    "    # - Countries_flat: lists of country codes\n",
    "    # - decade: decade information\n",
    "    \n",
    "    # Example data structure (comment out when using real data)\n",
    "    # df_flattened = pd.DataFrame({\n",
    "    #     'Countries_flat': [\n",
    "    #         ['US', 'CA', 'MX'], \n",
    "    #         ['FR', 'DE', 'IT'],\n",
    "    #         ['US', 'UK', 'FR'],\n",
    "    #         ['CN', 'JP', 'KR']\n",
    "    #     ],\n",
    "    #     'decade': ['1990s', '1990s', '2000s', '2010s']\n",
    "    # })\n",
    "    \n",
    "    # Define main directory\n",
    "    \n",
    "    # Create and save visualizations\n",
    "    plot_country_coappearance_network(df_flattened, subject_main_dir, min_weight=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb8ff0-74cb-4e2a-8b7e-7e59154b5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kaleido  # Required for plotly.write_image\n",
    "import io\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# Utility functions for country network analysis\n",
    "def create_country_pairs(countries_list):\n",
    "    \"\"\"\n",
    "    Create all possible pairs of countries from a list of countries\n",
    "    \n",
    "    Parameters:\n",
    "    countries_list (list): List of country codes\n",
    "    \n",
    "    Returns:\n",
    "    list: List of country pairs (tuples)\n",
    "    \"\"\"\n",
    "    if not countries_list or len(countries_list) < 2:\n",
    "        return []\n",
    "    \n",
    "    # Create all possible pairs\n",
    "    return list(combinations(sorted(countries_list), 2))\n",
    "\n",
    "def count_country_pairs(df, country_col='Countries_flat'):\n",
    "    \"\"\"\n",
    "    Count co-appearances of countries\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame with country lists\n",
    "    country_col (str): Name of column containing country lists\n",
    "    \n",
    "    Returns:\n",
    "    Counter: Dictionary of country pairs and their counts\n",
    "    \"\"\"\n",
    "    pair_counts = Counter()\n",
    "    \n",
    "    # Process each row\n",
    "    for countries in df[country_col]:\n",
    "        if isinstance(countries, list) and len(countries) >= 2:\n",
    "            pairs = create_country_pairs(countries)\n",
    "            pair_counts.update(pairs)\n",
    "    \n",
    "    return pair_counts\n",
    "\n",
    "def get_country_centroids(world_gdf, iso_cols=['iso_a2', 'ISO_A2', 'ISO_A2_EH']):\n",
    "    \"\"\"\n",
    "    Get centroid coordinates for each country\n",
    "    \n",
    "    Parameters:\n",
    "    world_gdf (GeoDataFrame): World map GeoDataFrame\n",
    "    iso_cols (list): Possible column names for ISO country codes\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary mapping country codes to (longitude, latitude)\n",
    "    \"\"\"\n",
    "    country_positions = {}\n",
    "    \n",
    "    # Determine which ISO column exists in the dataframe\n",
    "    available_cols = [col for col in iso_cols if col in world_gdf.columns]\n",
    "    \n",
    "    if not available_cols:\n",
    "        print(f\"Warning: No ISO country code columns found in the dataframe. Available columns: {world_gdf.columns.tolist()}\")\n",
    "        # Try to use a different identifier as fallback\n",
    "        if 'ADMIN' in world_gdf.columns:\n",
    "            print(\"Using 'ADMIN' column as fallback. Note: This might not match your country codes.\")\n",
    "            iso_col = 'ADMIN'\n",
    "        elif 'NAME' in world_gdf.columns:\n",
    "            print(\"Using 'NAME' column as fallback. Note: This might not match your country codes.\")\n",
    "            iso_col = 'NAME'\n",
    "        else:\n",
    "            print(\"No suitable country identifier columns found.\")\n",
    "            return country_positions\n",
    "    else:\n",
    "        iso_col = available_cols[0]\n",
    "        print(f\"Using '{iso_col}' for country codes.\")\n",
    "    \n",
    "    # Print first few values to help with debugging\n",
    "    print(f\"Sample values from '{iso_col}' column: {world_gdf[iso_col].head().tolist()}\")\n",
    "    \n",
    "    for idx, row in world_gdf.iterrows():\n",
    "        if row[iso_col] not in ['-99', '-1', None, '']:  # Skip invalid codes\n",
    "            centroid = row.geometry.representative_point()\n",
    "            country_positions[row[iso_col]] = (centroid.y, centroid.x)  # lat, lon for folium\n",
    "    \n",
    "    return country_positions\n",
    "\n",
    "def get_top_countries(df, country_col='Countries_flat', top_n=20):\n",
    "    \"\"\"\n",
    "    Get top countries by appearance count\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame with country lists\n",
    "    country_col (str): Name of column containing country lists\n",
    "    top_n (int): Number of top countries to return\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary of country codes and their counts\n",
    "    \"\"\"\n",
    "    country_counts = Counter()\n",
    "    \n",
    "    for countries in df[country_col]:\n",
    "        if isinstance(countries, list):\n",
    "            country_counts.update(countries)\n",
    "    \n",
    "    return dict(country_counts.most_common(top_n))\n",
    "\n",
    "def analyze_country_coappearances_by_decade(df, country_col='Countries_flat', decade_col='decade'):\n",
    "    \"\"\"\n",
    "    Analyze country co-appearances by decade\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame with country lists and decade information\n",
    "    country_col (str): Name of column containing country lists\n",
    "    decade_col (str): Name of column containing decade information\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary of decades with country pair counts\n",
    "    \"\"\"\n",
    "    decades = df[decade_col].unique()\n",
    "    decade_pairs = {}\n",
    "    \n",
    "    for decade in decades:\n",
    "        decade_df = df[df[decade_col] == decade]\n",
    "        if not decade_df.empty:\n",
    "            decade_pairs[decade] = count_country_pairs(decade_df, country_col)\n",
    "    \n",
    "    return decade_pairs\n",
    "\n",
    "def save_folium_as_png(map_obj, output_path, width=1200, height=800):\n",
    "    \"\"\"\n",
    "    Save a folium map as a PNG file\n",
    "    \n",
    "    Parameters:\n",
    "    map_obj (folium.Map): Folium map object\n",
    "    output_path (str/Path): Path to save the PNG file\n",
    "    width (int): Width of the output image\n",
    "    height (int): Height of the output image\n",
    "    \"\"\"\n",
    "    # Save the map as an HTML file temporarily\n",
    "    temp_html = str(output_path).replace('.png', '_temp.html')\n",
    "    map_obj.save(temp_html)\n",
    "    \n",
    "    try:\n",
    "        # Set up Chrome options for headless rendering\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(f\"--window-size={width},{height}\")\n",
    "        \n",
    "        # Initialize a browser\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Load the HTML file\n",
    "        driver.get(f\"file://{Path(temp_html).absolute()}\")\n",
    "        \n",
    "        # Wait for the map to fully load\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Take a screenshot\n",
    "        driver.save_screenshot(str(output_path))\n",
    "        \n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "        \n",
    "        print(f\"Saved map as PNG: {output_path}\")\n",
    "        \n",
    "        # Optionally clean up the temporary HTML file\n",
    "        Path(temp_html).unlink()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving map as PNG: {e}\")\n",
    "        print(\"Make sure you have installed Chrome and chromedriver\")\n",
    "        print(\"Falling back to just HTML output\")\n",
    "        \n",
    "def plot_country_coappearance_network(df_flattened, subject_main_dir, min_weight=2):\n",
    "    \"\"\"\n",
    "    Create and save visualizations of country co-appearances\n",
    "    \n",
    "    Parameters:\n",
    "    df_flattened (DataFrame): DataFrame with flattened country lists\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    min_weight (int): Minimum weight to include in visualizations\n",
    "    \"\"\"\n",
    "    # Load world map data\n",
    "    # Handle the deprecated dataset by downloading directly\n",
    "    try:\n",
    "        # Try the old method first for backward compatibility\n",
    "        world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    except (AttributeError, ModuleNotFoundError):\n",
    "        # If that fails, download the data directly\n",
    "        world_url = \"https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\"\n",
    "        world = gpd.read_file(world_url)\n",
    "        \n",
    "    # Print available columns for debugging\n",
    "    print(f\"Available columns in world dataframe: {world.columns.tolist()}\")\n",
    "    \n",
    "    # Get country centroids\n",
    "    country_centroids = get_country_centroids(world)\n",
    "    \n",
    "    # Try to build a mapping of country codes to full names\n",
    "    country_names = {}\n",
    "    name_cols = ['NAME', 'ADMIN', 'name', 'admin', 'NAME_EN', 'name_en']\n",
    "    code_cols = ['ISO_A2', 'ISO_A3', 'iso_a2', 'iso_a3']\n",
    "    \n",
    "    # Find valid name and code columns\n",
    "    valid_name_col = next((col for col in name_cols if col in world.columns), None)\n",
    "    valid_code_col = next((col for col in code_cols if col in world.columns), None)\n",
    "    \n",
    "    if valid_name_col and valid_code_col:\n",
    "        for idx, row in world.iterrows():\n",
    "            if row[valid_code_col] not in ['-99', '-1', None, '']:\n",
    "                country_names[row[valid_code_col]] = row[valid_name_col]\n",
    "    \n",
    "    # Count country pairs\n",
    "    pair_counts = count_country_pairs(df_flattened)\n",
    "    \n",
    "    # Get top countries\n",
    "    top_countries = get_top_countries(df_flattened)\n",
    "    \n",
    "    # Create interactive folium map\n",
    "    map_center = [20, 0]  # Center of the map\n",
    "    m = folium.Map(location=map_center, zoom_start=2, tiles='CartoDB positron')\n",
    "    \n",
    "    # Add country markers\n",
    "    for country, count in top_countries.items():\n",
    "        if country in country_centroids:\n",
    "            lat, lon = country_centroids[country]\n",
    "            \n",
    "            # Get full country name if available\n",
    "            country_label = country_names.get(country, country)\n",
    "            \n",
    "            folium.CircleMarker(\n",
    "                location=[lat, lon],\n",
    "                radius=min(20, max(5, np.log1p(count))),  # Scale marker size by log of count\n",
    "                color='blue',\n",
    "                fill=True,\n",
    "                fill_color='blue',\n",
    "                fill_opacity=0.6,\n",
    "                popup=f\"{country_label}: {count} appearances\"\n",
    "            ).add_to(m)\n",
    "            \n",
    "            # Add country label\n",
    "            folium.Marker(\n",
    "                location=[lat, lon],\n",
    "                icon=folium.DivIcon(\n",
    "                    icon_size=(150, 36),\n",
    "                    icon_anchor=(75, 18),\n",
    "                    html=f'<div style=\"font-size: 10pt; color: black; font-weight: bold; text-align: center;\">{country_label}</div>'\n",
    "                )\n",
    "            ).add_to(m)\n",
    "    \n",
    "    # Add connections between countries\n",
    "    for (country1, country2), weight in pair_counts.items():\n",
    "        if weight >= min_weight and country1 in country_centroids and country2 in country_centroids:\n",
    "            lat1, lon1 = country_centroids[country1]\n",
    "            lat2, lon2 = country_centroids[country2]\n",
    "            \n",
    "            # Scale line weight\n",
    "            line_weight = min(10, max(1, np.log1p(weight) / 2))\n",
    "            \n",
    "            # Create line with lighter color (light blue)\n",
    "            folium.PolyLine(\n",
    "                locations=[[lat1, lon1], [lat2, lon2]],\n",
    "                weight=line_weight,\n",
    "                color='#ADD8E6',  # Light blue\n",
    "                opacity=min(0.8, max(0.2, weight / 100)),\n",
    "                popup=f\"{country1}-{country2}: {weight} co-appearances\"\n",
    "            ).add_to(m)\n",
    "    \n",
    "    # Save interactive map\n",
    "    map_path = Path(subject_main_dir) / 'countries' / 'country_coappearance_network.html'\n",
    "    map_dir = map_path.parent\n",
    "    map_dir.mkdir(parents=True, exist_ok=True)\n",
    "    m.save(str(map_path))\n",
    "    \n",
    "    # Save as PNG as well\n",
    "    png_path = map_path.with_suffix('.png')\n",
    "    try:\n",
    "        save_folium_as_png(m, png_path, width=1600, height=1000)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save map as PNG. Error: {e}\")\n",
    "        print(\"Make sure you have installed the required packages: selenium, pillow, and have Chrome/chromedriver available.\")\n",
    "    \n",
    "    # Analyze co-appearances by decade\n",
    "    decade_pairs = analyze_country_coappearances_by_decade(df_flattened)\n",
    "    \n",
    "    # Create visualizations for decade analysis\n",
    "    plot_decade_analysis(decade_pairs, top_countries, subject_main_dir)\n",
    "    \n",
    "    # Create plotly express choropleth map\n",
    "    create_plotly_choropleth(df_flattened, world, subject_main_dir)\n",
    "\n",
    "def plot_decade_analysis(decade_pairs, top_countries, subject_main_dir):\n",
    "    \"\"\"\n",
    "    Create and save visualizations for decade analysis\n",
    "    \n",
    "    Parameters:\n",
    "    decade_pairs (dict): Dictionary of decades with country pair counts\n",
    "    top_countries (dict): Dictionary of top countries\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    \"\"\"\n",
    "    # Prepare data for top co-appearances over decades\n",
    "    decades = sorted(decade_pairs.keys())\n",
    "    top_country_codes = list(top_countries.keys())[:10]  # Top 10 countries\n",
    "    \n",
    "    # Track persistent pairs\n",
    "    persistent_pairs = {}\n",
    "    for decade, pairs in decade_pairs.items():\n",
    "        for pair, count in pairs.most_common(50):  # Consider top 50 pairs\n",
    "            if pair not in persistent_pairs:\n",
    "                persistent_pairs[pair] = []\n",
    "            persistent_pairs[pair].append((decade, count))\n",
    "    \n",
    "    # Filter for pairs that appear in multiple decades\n",
    "    multi_decade_pairs = {pair: decades for pair, decades in persistent_pairs.items() \n",
    "                        if len(decades) > 1}\n",
    "    \n",
    "    # Create heatmap of co-appearances over decades\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    heatmap_data = []\n",
    "    pair_labels = []\n",
    "    \n",
    "    # Take top 20 most persistent pairs\n",
    "    sorted_persistent = sorted(multi_decade_pairs.items(), \n",
    "                             key=lambda x: len(x[1]), reverse=True)[:20]\n",
    "    \n",
    "    for pair, decade_counts in sorted_persistent:\n",
    "        row = []\n",
    "        pair_labels.append(f\"{pair[0]}-{pair[1]}\")\n",
    "        \n",
    "        # Create row with counts for each decade\n",
    "        decade_dict = {d: c for d, c in decade_counts}\n",
    "        for decade in decades:\n",
    "            row.append(decade_dict.get(decade, 0))\n",
    "        \n",
    "        heatmap_data.append(row)\n",
    "    \n",
    "    # Create and save heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(heatmap_data, cmap=\"YlOrRd\", annot=True, fmt=\"d\", \n",
    "                xticklabels=decades, yticklabels=pair_labels)\n",
    "    plt.title(\"Persistent Country Co-appearances by Decade\")\n",
    "    plt.xlabel(\"Decade\")\n",
    "    plt.ylabel(\"Country Pair\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, \"persistent_coappearances_by_decade.png\", subject_main_dir, \"countries\")\n",
    "    \n",
    "    # Create bar chart of top countries by decade\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Count country appearances by decade\n",
    "    decade_country_counts = {}\n",
    "    for decade in decades:\n",
    "        decade_df = df_flattened[df_flattened['decade'] == decade]\n",
    "        decade_country_counts[decade] = get_top_countries(decade_df, top_n=10)\n",
    "    \n",
    "    # Plot top 5 countries for each decade\n",
    "    for i, decade in enumerate(decades):\n",
    "        counts = decade_country_counts[decade]\n",
    "        countries = list(counts.keys())[:5]  # Top 5\n",
    "        values = [counts[c] for c in countries]\n",
    "        \n",
    "        plt.subplot(1, len(decades), i+1)\n",
    "        plt.barh(countries, values, color='skyblue')\n",
    "        plt.title(f\"Decade: {decade}\")\n",
    "        plt.xlabel(\"Count\")\n",
    "        if i == 0:\n",
    "            plt.ylabel(\"Country\")\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, \"top_countries_by_decade.png\", subject_main_dir, \"countries\")\n",
    "\n",
    "def create_plotly_choropleth(df_flattened, world, subject_main_dir):\n",
    "    \"\"\"\n",
    "    Create a Plotly Express choropleth map for country appearances\n",
    "    \n",
    "    Parameters:\n",
    "    df_flattened (DataFrame): DataFrame with flattened country lists\n",
    "    world (GeoDataFrame): World map GeoDataFrame\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    \"\"\"\n",
    "    # Count country appearances\n",
    "    country_counts = get_top_countries(df_flattened, top_n=None)  # Get all countries\n",
    "    \n",
    "    # Prepare data for plotly\n",
    "    choropleth_data = []\n",
    "    for country, count in country_counts.items():\n",
    "        choropleth_data.append({\n",
    "            'country_code': country,\n",
    "            'count': count\n",
    "        })\n",
    "    \n",
    "    choropleth_df = pd.DataFrame(choropleth_data)\n",
    "    \n",
    "    # Check if we need to convert from ISO-2 to ISO-3\n",
    "    # Plotly only supports ISO-3, country names, USA-states, or geojson-id\n",
    "    locationmode = 'ISO-3'\n",
    "    \n",
    "    # Create choropleth map\n",
    "    fig = px.choropleth(\n",
    "        choropleth_df,\n",
    "        locations='country_code',\n",
    "        color='count',\n",
    "        hover_name='country_code',\n",
    "        color_continuous_scale=px.colors.sequential.Plasma,\n",
    "        title='Global Country Appearances Count',\n",
    "        locationmode=locationmode,\n",
    "        projection='natural earth'\n",
    "    )\n",
    "    \n",
    "    # Improve layout\n",
    "    fig.update_layout(\n",
    "        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
    "        coloraxis_colorbar=dict(\n",
    "            title=\"Appearances\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save as HTML\n",
    "    plotly_path = Path(subject_main_dir) / 'countries' / 'country_appearances_choropleth.html'\n",
    "    plotly_dir = plotly_path.parent\n",
    "    plotly_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fig.write_html(str(plotly_path))\n",
    "    \n",
    "    # Save as image (PNG)\n",
    "    img_path = plotly_path.with_suffix('.png')\n",
    "    fig.write_image(str(img_path), width=1200, height=800)\n",
    "    \n",
    "    print(f\"Saved choropleth visualization to {plotly_path} and {img_path}\")\n",
    "    \n",
    "    # Improve layout\n",
    "    fig.update_layout(\n",
    "        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
    "        coloraxis_colorbar=dict(\n",
    "            title=\"Appearances\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save as HTML\n",
    "    plotly_path = Path(subject_main_dir) / 'countries' / 'country_appearances_choropleth.html'\n",
    "    plotly_dir = plotly_path.parent\n",
    "    plotly_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fig.write_html(str(plotly_path))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming df_flattened is already loaded with columns:\n",
    "    # - Countries_flat: lists of country codes\n",
    "    # - decade: decade information\n",
    "    \n",
    "    # Example data structure (comment out when using real data)\n",
    "    # df_flattened = pd.DataFrame({\n",
    "    #     'Countries_flat': [\n",
    "    #         ['US', 'CA', 'MX'], \n",
    "    #         ['FR', 'DE', 'IT'],\n",
    "    #         ['US', 'UK', 'FR'],\n",
    "    #         ['CN', 'JP', 'KR']\n",
    "    #     ],\n",
    "    #     'decade': ['1990s', '1990s', '2000s', '2010s']\n",
    "    # })\n",
    "    \n",
    " \n",
    "    \n",
    "    # Create and save visualizations\n",
    "    plot_country_coappearance_network(df_flattened, subject_main_dir, min_weight=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb39148-c1e3-4408-9d83-2c781b12c741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kaleido  # Required for plotly.write_image\n",
    "import io\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# Utility functions for country network analysis\n",
    "def create_country_pairs(countries_list):\n",
    "    \"\"\"\n",
    "    Create all possible pairs of countries from a list of countries\n",
    "    \n",
    "    Parameters:\n",
    "    countries_list (list): List of country codes\n",
    "    \n",
    "    Returns:\n",
    "    list: List of country pairs (tuples)\n",
    "    \"\"\"\n",
    "    if not countries_list or len(countries_list) < 2:\n",
    "        return []\n",
    "    \n",
    "    # Create all possible pairs\n",
    "    return list(combinations(sorted(countries_list), 2))\n",
    "\n",
    "def count_country_pairs(df, country_col='Countries_flat'):\n",
    "    \"\"\"\n",
    "    Count co-appearances of countries\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame with country lists\n",
    "    country_col (str): Name of column containing country lists\n",
    "    \n",
    "    Returns:\n",
    "    Counter: Dictionary of country pairs and their counts\n",
    "    \"\"\"\n",
    "    pair_counts = Counter()\n",
    "    \n",
    "    # Process each row\n",
    "    for countries in df[country_col]:\n",
    "        if isinstance(countries, list) and len(countries) >= 2:\n",
    "            pairs = create_country_pairs(countries)\n",
    "            pair_counts.update(pairs)\n",
    "    \n",
    "    return pair_counts\n",
    "\n",
    "def get_country_centroids(world_gdf, iso_cols=['iso_a2', 'ISO_A2', 'ISO_A2_EH']):\n",
    "    \"\"\"\n",
    "    Get centroid coordinates for each country\n",
    "    \n",
    "    Parameters:\n",
    "    world_gdf (GeoDataFrame): World map GeoDataFrame\n",
    "    iso_cols (list): Possible column names for ISO country codes\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary mapping country codes to (longitude, latitude)\n",
    "    \"\"\"\n",
    "    country_positions = {}\n",
    "    \n",
    "    # Determine which ISO column exists in the dataframe\n",
    "    available_cols = [col for col in iso_cols if col in world_gdf.columns]\n",
    "    \n",
    "    if not available_cols:\n",
    "        print(f\"Warning: No ISO country code columns found in the dataframe. Available columns: {world_gdf.columns.tolist()}\")\n",
    "        # Try to use a different identifier as fallback\n",
    "        if 'ADMIN' in world_gdf.columns:\n",
    "            print(\"Using 'ADMIN' column as fallback. Note: This might not match your country codes.\")\n",
    "            iso_col = 'ADMIN'\n",
    "        elif 'NAME' in world_gdf.columns:\n",
    "            print(\"Using 'NAME' column as fallback. Note: This might not match your country codes.\")\n",
    "            iso_col = 'NAME'\n",
    "        else:\n",
    "            print(\"No suitable country identifier columns found.\")\n",
    "            return country_positions\n",
    "    else:\n",
    "        iso_col = available_cols[0]\n",
    "        print(f\"Using '{iso_col}' for country codes.\")\n",
    "    \n",
    "    # Print first few values to help with debugging\n",
    "    print(f\"Sample values from '{iso_col}' column: {world_gdf[iso_col].head().tolist()}\")\n",
    "    \n",
    "    for idx, row in world_gdf.iterrows():\n",
    "        if row[iso_col] not in ['-99', '-1', None, '']:  # Skip invalid codes\n",
    "            centroid = row.geometry.representative_point()\n",
    "            country_positions[row[iso_col]] = (centroid.y, centroid.x)  # lat, lon for folium\n",
    "    \n",
    "    return country_positions\n",
    "\n",
    "def get_top_countries(df, country_col='Countries_flat', top_n=20):\n",
    "    \"\"\"\n",
    "    Get top countries by appearance count\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame with country lists\n",
    "    country_col (str): Name of column containing country lists\n",
    "    top_n (int): Number of top countries to return\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary of country codes and their counts\n",
    "    \"\"\"\n",
    "    country_counts = Counter()\n",
    "    \n",
    "    for countries in df[country_col]:\n",
    "        if isinstance(countries, list):\n",
    "            country_counts.update(countries)\n",
    "    \n",
    "    return dict(country_counts.most_common(top_n))\n",
    "\n",
    "def analyze_country_coappearances_by_decade(df, country_col='Countries_flat', decade_col='decade'):\n",
    "    \"\"\"\n",
    "    Analyze country co-appearances by decade\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame with country lists and decade information\n",
    "    country_col (str): Name of column containing country lists\n",
    "    decade_col (str): Name of column containing decade information\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary of decades with country pair counts\n",
    "    \"\"\"\n",
    "    decades = df[decade_col].unique()\n",
    "    decade_pairs = {}\n",
    "    \n",
    "    for decade in decades:\n",
    "        decade_df = df[df[decade_col] == decade]\n",
    "        if not decade_df.empty:\n",
    "            decade_pairs[decade] = count_country_pairs(decade_df, country_col)\n",
    "    \n",
    "    return decade_pairs\n",
    "\n",
    "def create_static_map(df_flattened, world, country_centroids, pair_counts, top_countries, country_names, min_weight=2):\n",
    "    \"\"\"\n",
    "    Create a static matplotlib version of the country network map\n",
    "    \n",
    "    Parameters:\n",
    "    df_flattened (DataFrame): DataFrame with flattened country lists\n",
    "    world (GeoDataFrame): World map GeoDataFrame\n",
    "    country_centroids (dict): Dictionary of country centroids\n",
    "    pair_counts (Counter): Country pair counts\n",
    "    top_countries (dict): Top countries by count\n",
    "    country_names (dict): Country code to name mapping\n",
    "    min_weight (int): Minimum weight to include connections\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: Figure with the map\n",
    "    \"\"\"\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(16, 10), dpi=150)\n",
    "    \n",
    "    # Plot world map background\n",
    "    world.boundary.plot(ax=ax, linewidth=0.5, color='gray')\n",
    "    \n",
    "    # Plot connections\n",
    "    for (country1, country2), weight in pair_counts.items():\n",
    "        if weight >= min_weight and country1 in country_centroids and country2 in country_centroids:\n",
    "            # Get coordinates\n",
    "            y1, x1 = country_centroids[country1]\n",
    "            y2, x2 = country_centroids[country2]\n",
    "            \n",
    "            # Scale line width by weight\n",
    "            line_width = min(3, max(0.5, np.log1p(weight) / 3))\n",
    "            \n",
    "            # Plot line\n",
    "            ax.plot([x1, x2], [y1, y2], color='lightblue', linewidth=line_width, \n",
    "                   alpha=min(0.8, max(0.2, weight / 100)), zorder=1)\n",
    "    \n",
    "    # Plot country nodes\n",
    "    for country, count in top_countries.items():\n",
    "        if country in country_centroids:\n",
    "            y, x = country_centroids[country]\n",
    "            \n",
    "            # Scale marker size by count\n",
    "            marker_size = min(150, max(30, np.log1p(count) * 20))\n",
    "            \n",
    "            # Get country name if available\n",
    "            country_label = country_names.get(country, country)\n",
    "            \n",
    "            # Plot node\n",
    "            ax.scatter(x, y, s=marker_size, color='blue', alpha=0.6, zorder=2)\n",
    "            \n",
    "            # Add country label\n",
    "            ax.text(x, y, country_label, fontsize=8, ha='center', va='center', \n",
    "                   color='black', fontweight='bold', zorder=3)\n",
    "    \n",
    "    # Add title and adjust layout\n",
    "    ax.set_title('Country Co-appearance Network', fontsize=14)\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_country_coappearance_network(df_flattened, subject_main_dir, min_weight=2):\n",
    "    \"\"\"\n",
    "    Create and save visualizations of country co-appearances\n",
    "    \n",
    "    Parameters:\n",
    "    df_flattened (DataFrame): DataFrame with flattened country lists\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    min_weight (int): Minimum weight to include in visualizations\n",
    "    \"\"\"\n",
    "    # Load world map data\n",
    "    # Handle the deprecated dataset by downloading directly\n",
    "    try:\n",
    "        # Try the old method first for backward compatibility\n",
    "        world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    except (AttributeError, ModuleNotFoundError):\n",
    "        # If that fails, download the data directly\n",
    "        world_url = \"https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\"\n",
    "        world = gpd.read_file(world_url)\n",
    "        \n",
    "    # Print available columns for debugging\n",
    "    print(f\"Available columns in world dataframe: {world.columns.tolist()}\")\n",
    "    \n",
    "    # Get country centroids\n",
    "    country_centroids = get_country_centroids(world)\n",
    "    \n",
    "    # Try to build a mapping of country codes to full names\n",
    "    country_names = {}\n",
    "    name_cols = ['NAME', 'ADMIN', 'name', 'admin', 'NAME_EN', 'name_en']\n",
    "    code_cols = ['ISO_A2', 'ISO_A3', 'iso_a2', 'iso_a3']\n",
    "    \n",
    "    # Find valid name and code columns\n",
    "    valid_name_col = next((col for col in name_cols if col in world.columns), None)\n",
    "    valid_code_col = next((col for col in code_cols if col in world.columns), None)\n",
    "    \n",
    "    if valid_name_col and valid_code_col:\n",
    "        for idx, row in world.iterrows():\n",
    "            if row[valid_code_col] not in ['-99', '-1', None, '']:\n",
    "                country_names[row[valid_code_col]] = row[valid_name_col]\n",
    "    \n",
    "    # Count country pairs\n",
    "    pair_counts = count_country_pairs(df_flattened)\n",
    "    \n",
    "    # Get top countries\n",
    "    top_countries = get_top_countries(df_flattened)\n",
    "    \n",
    "    # Create interactive folium map\n",
    "    map_center = [20, 0]  # Center of the map\n",
    "    m = folium.Map(location=map_center, zoom_start=2, tiles='CartoDB positron')\n",
    "    \n",
    "    # Add country markers\n",
    "    for country, count in top_countries.items():\n",
    "        if country in country_centroids:\n",
    "            lat, lon = country_centroids[country]\n",
    "            \n",
    "            # Get full country name if available\n",
    "            country_label = country_names.get(country, country)\n",
    "            \n",
    "            folium.CircleMarker(\n",
    "                location=[lat, lon],\n",
    "                radius=min(20, max(5, np.log1p(count))),  # Scale marker size by log of count\n",
    "                color='blue',\n",
    "                fill=True,\n",
    "                fill_color='blue',\n",
    "                fill_opacity=0.6,\n",
    "                popup=f\"{country_label}: {count} appearances\"\n",
    "            ).add_to(m)\n",
    "            \n",
    "            # Add country label\n",
    "            folium.Marker(\n",
    "                location=[lat, lon],\n",
    "                icon=folium.DivIcon(\n",
    "                    icon_size=(150, 36),\n",
    "                    icon_anchor=(75, 18),\n",
    "                    html=f'<div style=\"font-size: 10pt; color: black; font-weight: bold; text-align: center;\">{country_label}</div>'\n",
    "                )\n",
    "            ).add_to(m)\n",
    "    \n",
    "    # Add connections between countries\n",
    "    for (country1, country2), weight in pair_counts.items():\n",
    "        if weight >= min_weight and country1 in country_centroids and country2 in country_centroids:\n",
    "            lat1, lon1 = country_centroids[country1]\n",
    "            lat2, lon2 = country_centroids[country2]\n",
    "            \n",
    "            # Scale line weight\n",
    "            line_weight = min(10, max(1, np.log1p(weight) / 2))\n",
    "            \n",
    "            # Create line with lighter color (light blue)\n",
    "            folium.PolyLine(\n",
    "                locations=[[lat1, lon1], [lat2, lon2]],\n",
    "                weight=line_weight,\n",
    "                color='#ADD8E6',  # Light blue\n",
    "                opacity=min(0.8, max(0.2, weight / 100)),\n",
    "                popup=f\"{country1}-{country2}: {weight} co-appearances\"\n",
    "            ).add_to(m)\n",
    "    \n",
    "    # Save interactive map\n",
    "    map_path = Path(subject_main_dir) / 'countries' / 'country_coappearance_network.html'\n",
    "    map_dir = map_path.parent\n",
    "    map_dir.mkdir(parents=True, exist_ok=True)\n",
    "    m.save(str(map_path))\n",
    "    \n",
    "    # Create and save static matplotlib version of the map\n",
    "    print(\"Creating static map image...\")\n",
    "    static_fig = create_static_map(\n",
    "        df_flattened, world, country_centroids, \n",
    "        pair_counts, top_countries, country_names, min_weight\n",
    "    )\n",
    "    \n",
    "    # Save static map as PNG\n",
    "    static_map_path = Path(subject_main_dir) / 'countries' / 'country_coappearance_network_static.png'\n",
    "    static_fig.savefig(static_map_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(static_fig)\n",
    "    print(f\"Saved static map as: {static_map_path}\")\n",
    "    \n",
    "    # Analyze co-appearances by decade\n",
    "    decade_pairs = analyze_country_coappearances_by_decade(df_flattened)\n",
    "    \n",
    "    # Create visualizations for decade analysis\n",
    "    plot_decade_analysis(decade_pairs, top_countries, subject_main_dir)\n",
    "    \n",
    "    # Create plotly express choropleth map\n",
    "    create_plotly_choropleth(df_flattened, world, subject_main_dir)\n",
    "\n",
    "def plot_decade_analysis(decade_pairs, top_countries, subject_main_dir):\n",
    "    \"\"\"\n",
    "    Create and save visualizations for decade analysis\n",
    "    \n",
    "    Parameters:\n",
    "    decade_pairs (dict): Dictionary of decades with country pair counts\n",
    "    top_countries (dict): Dictionary of top countries\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    \"\"\"\n",
    "    # Prepare data for top co-appearances over decades\n",
    "    decades = sorted(decade_pairs.keys())\n",
    "    top_country_codes = list(top_countries.keys())[:10]  # Top 10 countries\n",
    "    \n",
    "    # Track persistent pairs\n",
    "    persistent_pairs = {}\n",
    "    for decade, pairs in decade_pairs.items():\n",
    "        for pair, count in pairs.most_common(50):  # Consider top 50 pairs\n",
    "            if pair not in persistent_pairs:\n",
    "                persistent_pairs[pair] = []\n",
    "            persistent_pairs[pair].append((decade, count))\n",
    "    \n",
    "    # Filter for pairs that appear in multiple decades\n",
    "    multi_decade_pairs = {pair: decades for pair, decades in persistent_pairs.items() \n",
    "                        if len(decades) > 1}\n",
    "    \n",
    "    # Create heatmap of co-appearances over decades\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    heatmap_data = []\n",
    "    pair_labels = []\n",
    "    \n",
    "    # Take top 20 most persistent pairs\n",
    "    sorted_persistent = sorted(multi_decade_pairs.items(), \n",
    "                             key=lambda x: len(x[1]), reverse=True)[:20]\n",
    "    \n",
    "    for pair, decade_counts in sorted_persistent:\n",
    "        row = []\n",
    "        pair_labels.append(f\"{pair[0]}-{pair[1]}\")\n",
    "        \n",
    "        # Create row with counts for each decade\n",
    "        decade_dict = {d: c for d, c in decade_counts}\n",
    "        for decade in decades:\n",
    "            row.append(decade_dict.get(decade, 0))\n",
    "        \n",
    "        heatmap_data.append(row)\n",
    "    \n",
    "    # Create and save heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(heatmap_data, cmap=\"YlOrRd\", annot=True, fmt=\"d\", \n",
    "                xticklabels=decades, yticklabels=pair_labels)\n",
    "    plt.title(\"Persistent Country Co-appearances by Decade\")\n",
    "    plt.xlabel(\"Decade\")\n",
    "    plt.ylabel(\"Country Pair\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, \"persistent_coappearances_by_decade.png\", subject_main_dir, \"countries\")\n",
    "    \n",
    "    # Create bar chart of top countries by decade\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Count country appearances by decade\n",
    "    decade_country_counts = {}\n",
    "    for decade in decades:\n",
    "        decade_df = df_flattened[df_flattened['decade'] == decade]\n",
    "        decade_country_counts[decade] = get_top_countries(decade_df, top_n=10)\n",
    "    \n",
    "    # Plot top 5 countries for each decade\n",
    "    for i, decade in enumerate(decades):\n",
    "        counts = decade_country_counts[decade]\n",
    "        countries = list(counts.keys())[:5]  # Top 5\n",
    "        values = [counts[c] for c in countries]\n",
    "        \n",
    "        plt.subplot(1, len(decades), i+1)\n",
    "        plt.barh(countries, values, color='skyblue')\n",
    "        plt.title(f\"Decade: {decade}\")\n",
    "        plt.xlabel(\"Count\")\n",
    "        if i == 0:\n",
    "            plt.ylabel(\"Country\")\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    save_plot(plt, \"top_countries_by_decade.png\", subject_main_dir, \"countries\")\n",
    "\n",
    "def create_plotly_choropleth(df_flattened, world, subject_main_dir):\n",
    "    \"\"\"\n",
    "    Create a Plotly Express choropleth map for country appearances\n",
    "    \n",
    "    Parameters:\n",
    "    df_flattened (DataFrame): DataFrame with flattened country lists\n",
    "    world (GeoDataFrame): World map GeoDataFrame\n",
    "    subject_main_dir (str/Path): Main directory for saving visualizations\n",
    "    \"\"\"\n",
    "    # Count country appearances\n",
    "    country_counts = get_top_countries(df_flattened, top_n=None)  # Get all countries\n",
    "    \n",
    "    # Prepare data for plotly\n",
    "    choropleth_data = []\n",
    "    for country, count in country_counts.items():\n",
    "        choropleth_data.append({\n",
    "            'country_code': country,\n",
    "            'count': count\n",
    "        })\n",
    "    \n",
    "    choropleth_df = pd.DataFrame(choropleth_data)\n",
    "    \n",
    "    # Check if we need to convert from ISO-2 to ISO-3\n",
    "    # Plotly only supports ISO-3, country names, USA-states, or geojson-id\n",
    "    locationmode = 'ISO-3'\n",
    "    \n",
    "    # Create choropleth map\n",
    "    fig = px.choropleth(\n",
    "        choropleth_df,\n",
    "        locations='country_code',\n",
    "        color='count',\n",
    "        hover_name='country_code',\n",
    "        color_continuous_scale=px.colors.sequential.Plasma,\n",
    "        title='Global Country Appearances Count',\n",
    "        locationmode=locationmode,\n",
    "        projection='natural earth'\n",
    "    )\n",
    "    \n",
    "    # Improve layout\n",
    "    fig.update_layout(\n",
    "        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
    "        coloraxis_colorbar=dict(\n",
    "            title=\"Appearances\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save as HTML\n",
    "    plotly_path = Path(subject_main_dir) / 'countries' / 'country_appearances_choropleth.html'\n",
    "    plotly_dir = plotly_path.parent\n",
    "    plotly_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fig.write_html(str(plotly_path))\n",
    "    \n",
    "    # Save as image (PNG)\n",
    "    img_path = plotly_path.with_suffix('.png')\n",
    "    fig.write_image(str(img_path), width=1200, height=800)\n",
    "    \n",
    "    print(f\"Saved choropleth visualization to {plotly_path} and {img_path}\")\n",
    "    \n",
    "    # Improve layout\n",
    "    fig.update_layout(\n",
    "        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
    "        coloraxis_colorbar=dict(\n",
    "            title=\"Appearances\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save as HTML\n",
    "    plotly_path = Path(subject_main_dir) / 'countries' / 'country_appearances_choropleth.html'\n",
    "    plotly_dir = plotly_path.parent\n",
    "    plotly_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fig.write_html(str(plotly_path))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming df_flattened is already loaded with columns:\n",
    "    # - Countries_flat: lists of country codes\n",
    "    # - decade: decade information\n",
    "    \n",
    "    # Example data structure (comment out when using real data)\n",
    "    # df_flattened = pd.DataFrame({\n",
    "    #     'Countries_flat': [\n",
    "    #         ['US', 'CA', 'MX'], \n",
    "    #         ['FR', 'DE', 'IT'],\n",
    "    #         ['US', 'UK', 'FR'],\n",
    "    #         ['CN', 'JP', 'KR']\n",
    "    #     ],\n",
    "    #     'decade': ['1990s', '1990s', '2000s', '2010s']\n",
    "    # })\n",
    "    \n",
    "    # Define main directory\n",
    "    \n",
    "    # Create and save visualizations\n",
    "    plot_country_coappearance_network(df_flattened, subject_main_dir, min_weight=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea7046-a85a-46ac-b71d-a3512401ee76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miki_project",
   "language": "python",
   "name": "miki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
